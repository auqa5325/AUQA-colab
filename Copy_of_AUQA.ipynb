{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnR2E8RWcz++3c/Rlnzpc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auqa5325/AUQA-colab/blob/main/Copy_of_AUQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-uDEQ4a38AH",
        "outputId": "0dd9ea0e-b415-4b68-88c3-c24e64d2276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install boto3 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY_ID     = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "#AWS_SESSION_TOKEN     = userdata.get(\"AWS_SESSION_TOKEN\")  # may be None\n",
        "AWS_REGION = userdata.get(\"AWS_REGION\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
        "\n",
        "print(\"‚úÖ Credentials set. Region:\", AWS_REGION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8aVrSE4ZiZ",
        "outputId": "bc768233-e5be-4b5c-e292-b3ca100c78e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Credentials set. Region: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]\n",
        "session = boto3.Session(region_name=REGION)\n",
        "bedrock         = session.client(\"bedrock\")\n",
        "bedrock_runtime = session.client(\"bedrock-runtime\")\n",
        "print(\"‚úÖ boto3 session initialized in:\", session.region_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqqWGkX4-QA",
        "outputId": "97938a45-4fdd-4934-fd4b-0442e7f0e2b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ boto3 session initialized in: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "s = boto3.Session(region_name=\"ap-south-1\")\n",
        "print(s.client(\"sts\").get_caller_identity())   # must print Account, Arn\n",
        "creds = s.get_credentials().get_frozen_credentials()\n",
        "print(\"AccessKey:\", creds.access_key[:4], \"HasToken:\", bool(creds.token))\n"
      ],
      "metadata": {
        "id": "UT2f-KtPS8C0",
        "outputId": "50ec2908-7828-47b9-9e96-004a73232aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY1NDg4MzQxNDA6Ujp3Y0FQd1kzUA==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Sat, 30 Aug 2025 10:13:54 GMT'}, 'RetryAttempts': 0}}\n",
            "AccessKey: AKIA HasToken: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output dict structure :\n",
        "* id ‚Üí unique identifier for this response (good for\n",
        "logging/debugging).\n",
        "\n",
        "* type ‚Üí \"message\" ‚Üí tells you this is a message object.\n",
        "\n",
        "* role ‚Üí \"assistant\" ‚Üí the speaker role (assistant vs. user).\n",
        "\n",
        "* model ‚Üí which model gave this reply (claude-3-sonnet-20240229).\n",
        "\n",
        "* content ‚Üí a list of parts that make up the response.\n",
        "\n",
        "* stop_reason ‚Üí why the model stopped (e.g., end_turn, max_tokens).\n",
        "\n",
        "* stop_sequence ‚Üí custom sequence that stopped generation (here it‚Äôs None).\n",
        "\n",
        "* usage ‚Üí token usage info (handy for cost + rate limits)."
      ],
      "metadata": {
        "id": "wbqD05a1_0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "prompt = \"explain tcp ip?\"\n",
        "\n",
        "body = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 100,\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "print(output[\"outputs\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "xKuzvoVdBegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "session = boto3.Session(region_name=\"ap-south-1\")\n",
        "sts = session.client(\"sts\")\n",
        "print(sts.get_caller_identity())  # should return Account, Arn, UserId\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5nKJfpQRoqH",
        "outputId": "6c4b9df1-8ef1-42e2-acc7-d3e67a45b3d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY0NTkzNTE3NDk6UjpJd1BrYlNOVg==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Fri, 29 Aug 2025 09:22:31 GMT'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'outputs': [{'text': '\\n\\nTCP/IP is a suite of protocols that defines the Internet. Originally designed for the UNIX operating system. The TCP/IP protocol suite is a four-layer model that provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed, and received.\\n\\nThe four layers of the TCP/IP model are:\\n\\n1. The Application Layer: This is the topmost layer of the TCP/IP', 'stop_reason': 'length'}]}"
      ],
      "metadata": {
        "id": "zlZJ3onqB_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opensearch-py requests-aws4auth\n",
        "\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "from requests_aws4auth import AWS4Auth\n",
        "import boto3\n",
        "\n",
        "# --- CONFIG ---\n",
        "host   = \"https://search-test1-annauniv-pcx3f52wxykhpd4md6v4bjeqdy.ap-south-1.es.amazonaws.com\"  # OpenSearch domain endpoint\n",
        "region = \"ap-south-1\"\n",
        "service = \"es\"\n",
        "\n",
        "# --- AWS SigV4 Auth ---\n",
        "session = boto3.Session()\n",
        "credentials = session.get_credentials()\n",
        "awsauth = AWS4Auth(\n",
        "    credentials.access_key,\n",
        "    credentials.secret_key,\n",
        "    region,\n",
        "    service,\n",
        "    session_token=credentials.token\n",
        ")\n",
        "\n",
        "# --- OpenSearch Client ---\n",
        "client = OpenSearch(\n",
        "    hosts=[host],\n",
        "    http_auth=awsauth,\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    connection_class=RequestsHttpConnection\n",
        ")\n",
        "\n",
        "# --- TEST: Get cluster info ---\n",
        "print(client.info())\n"
      ],
      "metadata": {
        "id": "l5z2sIp87q1F",
        "outputId": "fd28194c-422f-4221-b070-f9d0bb23b28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/371.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m368.6/371.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m371.5/371.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h{'name': 'd69eb8cacc6e2cf42750a5f4788db327', 'cluster_name': '850146468080:test1-annauniv', 'cluster_uuid': 'qhkHo9X9Suuw8REB9uInGQ', 'version': {'distribution': 'opensearch', 'number': '2.19.0', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-07-24T06:15:41.026838036Z', 'build_snapshot': False, 'lucene_version': '9.12.1', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "index_body = {\n",
        "    \"settings\": {\n",
        "        \"index\": {\n",
        "            \"knn\": True,\n",
        "            \"knn.algo_param.ef_search\": 512,\n",
        "            \"number_of_shards\": 5,\n",
        "            \"number_of_replicas\": 1\n",
        "        }\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            # main chunk text\n",
        "            \"chunk_text\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # ids & names\n",
        "            \"course_id\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"filename\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # page number\n",
        "            \"page_no\": { \"type\": \"long\" },\n",
        "\n",
        "\n",
        "\n",
        "            # vector index used for ANN similarity search\n",
        "            \"vector_field\": {\n",
        "                \"type\": \"knn_vector\",\n",
        "                \"dimension\": 1024,\n",
        "                \"method\": {\n",
        "                    \"name\": \"hnsw\",\n",
        "                    \"space_type\": \"cosinesimil\",\n",
        "                    \"engine\": \"nmslib\",\n",
        "                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# recreate index\n",
        "if client.indices.exists(index=index_name):\n",
        "    print(f\"Index '{index_name}' already exists. Deleting and recreating...\")\n",
        "    client.indices.delete(index=index_name)\n",
        "\n",
        "client.indices.create(index=index_name, body=index_body)\n",
        "print(f\"‚úÖ Index '{index_name}' created successfully!\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UcTIfVU4RS02",
        "outputId": "7b3b813f-69be-4de4-ef61-e6b822f22dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-2645953989.py, line 64)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2645953989.py\"\u001b[0;36m, line \u001b[0;32m64\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'test-annauniv': {'aliases': {}, 'mappings': {'properties': {'course_id': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'embedding': {'type': 'float'}, 'metadata': {'properties': {'page': {'type': 'long'}, 'source': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'page_no': {'type': 'long'}, 'text': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'vector_field': {'type': 'knn_vector', 'dimension': 1024, 'method': {'engine': 'nmslib', 'space_type': 'cosinesimil', 'name': 'hnsw', 'parameters': {'ef_construction': 512, 'm': 16}}}}}, 'settings': {'index': {'replication': {'type': 'DOCUMENT'}, 'refresh_interval': '1s', 'number_of_shards': '5', 'knn.algo_param': {'ef_search': '512'}, 'provided_name': 'test-annauniv', 'knn': 'true', 'creation_date': '1756144596121', 'number_of_replicas': '1', 'uuid': 'rumasDZaQC6xBy4G48v1KQ', 'version': {'created': '136407827'}}}}}\n",
        "\n"
      ],
      "metadata": {
        "id": "n-xX1sT3yxqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, time, re\n",
        "\n",
        "BUCKET = \"anna-univ-qna\"\n",
        "DOCUMENT = \"Textbooks/SPM.pdf\"\n",
        "REGION = \"ap-south-1\"\n",
        "COURSE_ID = \"CS6022\"   # <-- set this per textbook\n",
        "FILENAME = \"SPM.pdf\"\n",
        "\n",
        "textract = boto3.client(\"textract\", region_name=REGION)\n",
        "\n",
        "# --- Start async Textract job ---\n",
        "start = time.time()\n",
        "response = textract.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": BUCKET, \"Name\": DOCUMENT}}\n",
        ")\n",
        "print(\"FILENAME =\",FILENAME)\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"‚úÖ Started Textract JobId: {job_id}\")\n",
        "\n",
        "# --- Wait for completion ---\n",
        "while True:\n",
        "    result = textract.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"‚ùå Textract job failed!\")\n",
        "\n",
        "pages = result[\"DocumentMetadata\"][\"Pages\"]\n",
        "print(f\"üìÑ Pages: {pages}\")\n",
        "print(f\"‚è±Ô∏è Time taken: {elapsed:.2f} sec\")\n",
        "print(f\"üí∞ Estimated cost: ${(pages/1000)*1.5:.4f}\")\n",
        "\n",
        "# --- Collect page-wise text into docs with metadata ---\n",
        "page_texts = {}\n",
        "next_token = None\n",
        "\n",
        "while True:\n",
        "    if next_token:\n",
        "        result = textract.get_document_text_detection(JobId=job_id, NextToken=next_token)\n",
        "    else:\n",
        "        result = textract.get_document_text_detection(JobId=job_id)\n",
        "\n",
        "    for block in result[\"Blocks\"]:\n",
        "        if block[\"BlockType\"] == \"LINE\":\n",
        "            page_no = block[\"Page\"]\n",
        "            page_texts.setdefault(page_no, []).append(block[\"Text\"])\n",
        "\n",
        "    next_token = result.get(\"NextToken\")\n",
        "    if not next_token:\n",
        "        break\n",
        "\n",
        "# Build document objects (ready to send to Titan for embeddings)\n",
        "docs = []\n",
        "for page_no, lines in page_texts.items():\n",
        "    chunk_text = \"\\n\".join(lines)\n",
        "    doc = {\n",
        "        \"chunk_text\": chunk_text,\n",
        "        \"course_id\": COURSE_ID,\n",
        "        \"filename\": FILENAME,\n",
        "        \"page_no\": page_no\n",
        "    }\n",
        "    docs.append(doc)\n",
        "\n",
        "# --- Preview first 5 pages ---\n",
        "print(\"\\nüìë First 5 pages extracted:\")\n",
        "for d in docs[:5]:\n",
        "    print(f\"\\nPage {d['page_no']} | Course: {d['course_id']} | File: {d['filename']}\")\n",
        "    print(d[\"chunk_text\"][:400], \"...\")\n"
      ],
      "metadata": {
        "id": "DVaNNUjgqWN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "806b9018-75df-421c-b8ef-aeea73a65c2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FILENAME = SPM.pdf\n",
            "‚úÖ Started Textract JobId: ce41d655412d458abda1771970d243aee8b1fcb47f07c25ecdfed15620863448\n",
            "üìÑ Pages: 396\n",
            "‚è±Ô∏è Time taken: 231.58 sec\n",
            "üí∞ Estimated cost: $0.5940\n",
            "\n",
            "üìë First 5 pages extracted:\n",
            "\n",
            "Page 1 | Course: CS6022 | File: SPM.pdf\n",
            "BoB HUGHES AND MIKE COTTERELL\n",
            "Software\n",
            "Project\n",
            "Management Second Edition\n",
            "Tistimating\n",
            "www.mcgraw-hill.co.uk/hughes ...\n",
            "\n",
            "Page 2 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition) ...\n",
            "\n",
            "Page 3 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition)\n",
            "Bob Hughes and Mike Cotterell,\n",
            "School of Information Management, University of Brighton\n",
            "The McGraw-Hill Companies\n",
            "London\n",
            "Burr Ridge, IL\n",
            "New York\n",
            "St Louis\n",
            "San Francisco\n",
            "Auckland\n",
            "Bogot√° Caracas\n",
            "Lisbon\n",
            "Madrid\n",
            "Mexico\n",
            "Milan\n",
            "Montreal\n",
            "New Delhi\n",
            "Panama\n",
            "Paris\n",
            "San Juan\n",
            "S√£o Paulo\n",
            "Singapore\n",
            "Tokyo\n",
            "Toronto ...\n",
            "\n",
            "Page 4 | Course: CS6022 | File: SPM.pdf\n",
            "Published by\n",
            "McGraw-Hill Publishing Company\n",
            "SHOPPENHANGERS ROAD, MAIDENHEAD, BERKSHIRE, SL6 2QL, ENGLAND\n",
            "Telephone: +44(o) 1628 502500\n",
            "Fax: +44(o) 1628 770224\n",
            "Web site: http://www.megraw-hill.co.uk\n",
            "British Library Cataloguing in Publication Data\n",
            "A catalogue record for this book is available from the British Library\n",
            "ISBN 007 709505 7\n",
            "Library of Congress cataloguing in publication data\n",
            "The LOC data  ...\n",
            "\n",
            "Page 5 | Course: CS6022 | File: SPM.pdf\n",
            "The road to hell is paved with works-in-progress.\n",
            "Philip Roth ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, json, time\n",
        "\n",
        "# --- Config ---\n",
        "REGION = \"ap-south-1\"\n",
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "PRICE_PER_1K = 0.000024  # USD per 1k tokens\n",
        "\n",
        "# Assume docs is already built from Textract step\n",
        "# docs = [\n",
        "#   {\"chunk_text\": \"...\", \"course_id\":\"CS101\", \"filename\":\"SPM.pdf\", \"page_no\":1},\n",
        "#   ...\n",
        "# ]\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "all_embeddings = []\n",
        "total_tokens = 0\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for d in docs:\n",
        "    body = json.dumps({\n",
        "        \"inputText\": d[\"chunk_text\"]\n",
        "    })\n",
        "\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "\n",
        "    # Titan Embedding returns: { \"embedding\": [...], \"inputTextTokenCount\": N }\n",
        "    vector = result[\"embedding\"]\n",
        "    tokens = result.get(\"inputTextTokenCount\", len(d[\"chunk_text\"].split()))  # fallback\n",
        "\n",
        "    d[\"vector_field\"] = vector\n",
        "    d[\"tokens\"] = tokens\n",
        "\n",
        "    all_embeddings.append(d)\n",
        "\n",
        "    total_tokens += tokens\n",
        "    print(f\"üìÑ Page {d['page_no']}: {tokens} tokens\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "cost = (total_tokens / 1000) * PRICE_PER_1K\n",
        "\n",
        "print(\"\\n‚è±Ô∏è Total time:\", round(elapsed, 2), \"seconds\")\n",
        "print(\"üìä Total tokens:\", total_tokens)\n",
        "print(f\"üí∞ Estimated embedding cost: ${cost:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f_xnQuxafnwr",
        "outputId": "419eb821-d9c8-46a0-e861-eadaf9e012da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Page 1: 37 tokens\n",
            "üìÑ Page 2: 9 tokens\n",
            "üìÑ Page 3: 96 tokens\n",
            "üìÑ Page 4: 345 tokens\n",
            "üìÑ Page 5: 14 tokens\n",
            "üìÑ Page 6: 393 tokens\n",
            "üìÑ Page 7: 424 tokens\n",
            "üìÑ Page 8: 393 tokens\n",
            "üìÑ Page 9: 393 tokens\n",
            "üìÑ Page 10: 253 tokens\n",
            "üìÑ Page 11: 517 tokens\n",
            "üìÑ Page 12: 482 tokens\n",
            "üìÑ Page 13: 235 tokens\n",
            "üìÑ Page 14: 460 tokens\n",
            "üìÑ Page 15: 551 tokens\n",
            "üìÑ Page 16: 373 tokens\n",
            "üìÑ Page 17: 575 tokens\n",
            "üìÑ Page 18: 463 tokens\n",
            "üìÑ Page 19: 400 tokens\n",
            "üìÑ Page 20: 529 tokens\n",
            "üìÑ Page 21: 457 tokens\n",
            "üìÑ Page 22: 410 tokens\n",
            "üìÑ Page 23: 645 tokens\n",
            "üìÑ Page 24: 322 tokens\n",
            "üìÑ Page 25: 558 tokens\n",
            "üìÑ Page 26: 550 tokens\n",
            "üìÑ Page 27: 411 tokens\n",
            "üìÑ Page 28: 588 tokens\n",
            "üìÑ Page 29: 457 tokens\n",
            "üìÑ Page 30: 558 tokens\n",
            "üìÑ Page 31: 365 tokens\n",
            "üìÑ Page 32: 603 tokens\n",
            "üìÑ Page 33: 403 tokens\n",
            "üìÑ Page 34: 273 tokens\n",
            "üìÑ Page 35: 515 tokens\n",
            "üìÑ Page 36: 542 tokens\n",
            "üìÑ Page 37: 588 tokens\n",
            "üìÑ Page 38: 553 tokens\n",
            "üìÑ Page 39: 520 tokens\n",
            "üìÑ Page 40: 540 tokens\n",
            "üìÑ Page 41: 276 tokens\n",
            "üìÑ Page 42: 443 tokens\n",
            "üìÑ Page 43: 369 tokens\n",
            "üìÑ Page 44: 600 tokens\n",
            "üìÑ Page 45: 532 tokens\n",
            "üìÑ Page 46: 529 tokens\n",
            "üìÑ Page 47: 435 tokens\n",
            "üìÑ Page 48: 530 tokens\n",
            "üìÑ Page 49: 328 tokens\n",
            "üìÑ Page 50: 339 tokens\n",
            "üìÑ Page 51: 507 tokens\n",
            "üìÑ Page 52: 577 tokens\n",
            "üìÑ Page 53: 568 tokens\n",
            "üìÑ Page 54: 619 tokens\n",
            "üìÑ Page 55: 590 tokens\n",
            "üìÑ Page 56: 425 tokens\n",
            "üìÑ Page 57: 731 tokens\n",
            "üìÑ Page 58: 750 tokens\n",
            "üìÑ Page 59: 579 tokens\n",
            "üìÑ Page 60: 701 tokens\n",
            "üìÑ Page 61: 712 tokens\n",
            "üìÑ Page 62: 622 tokens\n",
            "üìÑ Page 63: 553 tokens\n",
            "üìÑ Page 64: 617 tokens\n",
            "üìÑ Page 65: 578 tokens\n",
            "üìÑ Page 66: 315 tokens\n",
            "üìÑ Page 67: 350 tokens\n",
            "üìÑ Page 69: 319 tokens\n",
            "üìÑ Page 70: 269 tokens\n",
            "üìÑ Page 71: 518 tokens\n",
            "üìÑ Page 72: 529 tokens\n",
            "üìÑ Page 73: 579 tokens\n",
            "üìÑ Page 74: 597 tokens\n",
            "üìÑ Page 75: 402 tokens\n",
            "üìÑ Page 76: 598 tokens\n",
            "üìÑ Page 77: 372 tokens\n",
            "üìÑ Page 78: 422 tokens\n",
            "üìÑ Page 79: 559 tokens\n",
            "üìÑ Page 80: 334 tokens\n",
            "üìÑ Page 81: 513 tokens\n",
            "üìÑ Page 82: 515 tokens\n",
            "üìÑ Page 83: 481 tokens\n",
            "üìÑ Page 84: 535 tokens\n",
            "üìÑ Page 85: 329 tokens\n",
            "üìÑ Page 86: 468 tokens\n",
            "üìÑ Page 87: 527 tokens\n",
            "üìÑ Page 88: 506 tokens\n",
            "üìÑ Page 89: 470 tokens\n",
            "üìÑ Page 90: 108 tokens\n",
            "üìÑ Page 91: 328 tokens\n",
            "üìÑ Page 92: 729 tokens\n",
            "üìÑ Page 93: 627 tokens\n",
            "üìÑ Page 94: 708 tokens\n",
            "üìÑ Page 95: 415 tokens\n",
            "üìÑ Page 96: 700 tokens\n",
            "üìÑ Page 97: 565 tokens\n",
            "üìÑ Page 98: 604 tokens\n",
            "üìÑ Page 99: 607 tokens\n",
            "üìÑ Page 100: 625 tokens\n",
            "üìÑ Page 101: 664 tokens\n",
            "üìÑ Page 102: 488 tokens\n",
            "üìÑ Page 103: 546 tokens\n",
            "üìÑ Page 104: 550 tokens\n",
            "üìÑ Page 105: 379 tokens\n",
            "üìÑ Page 106: 525 tokens\n",
            "üìÑ Page 107: 451 tokens\n",
            "üìÑ Page 108: 482 tokens\n",
            "üìÑ Page 109: 624 tokens\n",
            "üìÑ Page 110: 560 tokens\n",
            "üìÑ Page 111: 430 tokens\n",
            "üìÑ Page 112: 625 tokens\n",
            "üìÑ Page 113: 557 tokens\n",
            "üìÑ Page 114: 501 tokens\n",
            "üìÑ Page 115: 273 tokens\n",
            "üìÑ Page 116: 426 tokens\n",
            "üìÑ Page 117: 80 tokens\n",
            "üìÑ Page 119: 301 tokens\n",
            "üìÑ Page 120: 571 tokens\n",
            "üìÑ Page 121: 676 tokens\n",
            "üìÑ Page 122: 312 tokens\n",
            "üìÑ Page 123: 568 tokens\n",
            "üìÑ Page 124: 507 tokens\n",
            "üìÑ Page 125: 654 tokens\n",
            "üìÑ Page 126: 488 tokens\n",
            "üìÑ Page 127: 475 tokens\n",
            "üìÑ Page 128: 604 tokens\n",
            "üìÑ Page 129: 410 tokens\n",
            "üìÑ Page 130: 603 tokens\n",
            "üìÑ Page 131: 498 tokens\n",
            "üìÑ Page 132: 186 tokens\n",
            "üìÑ Page 133: 392 tokens\n",
            "üìÑ Page 134: 445 tokens\n",
            "üìÑ Page 135: 490 tokens\n",
            "üìÑ Page 136: 544 tokens\n",
            "üìÑ Page 137: 436 tokens\n",
            "üìÑ Page 138: 500 tokens\n",
            "üìÑ Page 139: 518 tokens\n",
            "üìÑ Page 140: 552 tokens\n",
            "üìÑ Page 141: 600 tokens\n",
            "üìÑ Page 142: 661 tokens\n",
            "üìÑ Page 143: 425 tokens\n",
            "üìÑ Page 144: 166 tokens\n",
            "üìÑ Page 145: 334 tokens\n",
            "üìÑ Page 146: 282 tokens\n",
            "üìÑ Page 147: 546 tokens\n",
            "üìÑ Page 148: 495 tokens\n",
            "üìÑ Page 149: 624 tokens\n",
            "üìÑ Page 150: 580 tokens\n",
            "üìÑ Page 151: 642 tokens\n",
            "üìÑ Page 152: 399 tokens\n",
            "üìÑ Page 153: 674 tokens\n",
            "üìÑ Page 154: 487 tokens\n",
            "üìÑ Page 155: 482 tokens\n",
            "üìÑ Page 156: 538 tokens\n",
            "üìÑ Page 157: 402 tokens\n",
            "üìÑ Page 158: 513 tokens\n",
            "üìÑ Page 159: 541 tokens\n",
            "üìÑ Page 160: 477 tokens\n",
            "üìÑ Page 161: 423 tokens\n",
            "üìÑ Page 162: 340 tokens\n",
            "üìÑ Page 163: 354 tokens\n",
            "üìÑ Page 164: 298 tokens\n",
            "üìÑ Page 165: 533 tokens\n",
            "üìÑ Page 166: 789 tokens\n",
            "üìÑ Page 167: 567 tokens\n",
            "üìÑ Page 168: 331 tokens\n",
            "üìÑ Page 169: 460 tokens\n",
            "üìÑ Page 170: 564 tokens\n",
            "üìÑ Page 171: 858 tokens\n",
            "üìÑ Page 172: 271 tokens\n",
            "üìÑ Page 173: 553 tokens\n",
            "üìÑ Page 174: 591 tokens\n",
            "üìÑ Page 175: 141 tokens\n",
            "üìÑ Page 176: 777 tokens\n",
            "üìÑ Page 177: 472 tokens\n",
            "üìÑ Page 178: 320 tokens\n",
            "üìÑ Page 179: 257 tokens\n",
            "üìÑ Page 180: 237 tokens\n",
            "üìÑ Page 181: 260 tokens\n",
            "üìÑ Page 182: 304 tokens\n",
            "üìÑ Page 183: 425 tokens\n",
            "üìÑ Page 184: 429 tokens\n",
            "üìÑ Page 185: 559 tokens\n",
            "üìÑ Page 186: 471 tokens\n",
            "üìÑ Page 187: 512 tokens\n",
            "üìÑ Page 188: 510 tokens\n",
            "üìÑ Page 189: 559 tokens\n",
            "üìÑ Page 190: 525 tokens\n",
            "üìÑ Page 191: 370 tokens\n",
            "üìÑ Page 192: 456 tokens\n",
            "üìÑ Page 193: 372 tokens\n",
            "üìÑ Page 194: 484 tokens\n",
            "üìÑ Page 195: 249 tokens\n",
            "üìÑ Page 196: 485 tokens\n",
            "üìÑ Page 197: 474 tokens\n",
            "üìÑ Page 198: 602 tokens\n",
            "üìÑ Page 199: 679 tokens\n",
            "üìÑ Page 200: 468 tokens\n",
            "üìÑ Page 201: 382 tokens\n",
            "üìÑ Page 202: 465 tokens\n",
            "üìÑ Page 203: 299 tokens\n",
            "üìÑ Page 204: 788 tokens\n",
            "üìÑ Page 205: 526 tokens\n",
            "üìÑ Page 206: 485 tokens\n",
            "üìÑ Page 207: 614 tokens\n",
            "üìÑ Page 208: 561 tokens\n",
            "üìÑ Page 209: 601 tokens\n",
            "üìÑ Page 210: 582 tokens\n",
            "üìÑ Page 211: 526 tokens\n",
            "üìÑ Page 212: 728 tokens\n",
            "üìÑ Page 213: 662 tokens\n",
            "üìÑ Page 214: 567 tokens\n",
            "üìÑ Page 215: 478 tokens\n",
            "üìÑ Page 216: 447 tokens\n",
            "üìÑ Page 217: 537 tokens\n",
            "üìÑ Page 218: 594 tokens\n",
            "üìÑ Page 219: 671 tokens\n",
            "üìÑ Page 220: 479 tokens\n",
            "üìÑ Page 221: 347 tokens\n",
            "üìÑ Page 223: 242 tokens\n",
            "üìÑ Page 224: 263 tokens\n",
            "üìÑ Page 225: 621 tokens\n",
            "üìÑ Page 226: 572 tokens\n",
            "üìÑ Page 227: 804 tokens\n",
            "üìÑ Page 228: 558 tokens\n",
            "üìÑ Page 229: 578 tokens\n",
            "üìÑ Page 230: 654 tokens\n",
            "üìÑ Page 231: 464 tokens\n",
            "üìÑ Page 232: 562 tokens\n",
            "üìÑ Page 233: 490 tokens\n",
            "üìÑ Page 234: 622 tokens\n",
            "üìÑ Page 235: 513 tokens\n",
            "üìÑ Page 236: 589 tokens\n",
            "üìÑ Page 237: 607 tokens\n",
            "üìÑ Page 238: 440 tokens\n",
            "üìÑ Page 239: 446 tokens\n",
            "üìÑ Page 240: 579 tokens\n",
            "üìÑ Page 241: 470 tokens\n",
            "üìÑ Page 242: 607 tokens\n",
            "üìÑ Page 243: 648 tokens\n",
            "üìÑ Page 244: 423 tokens\n",
            "üìÑ Page 245: 296 tokens\n",
            "üìÑ Page 247: 331 tokens\n",
            "üìÑ Page 248: 266 tokens\n",
            "üìÑ Page 249: 552 tokens\n",
            "üìÑ Page 250: 496 tokens\n",
            "üìÑ Page 251: 385 tokens\n",
            "üìÑ Page 252: 681 tokens\n",
            "üìÑ Page 253: 443 tokens\n",
            "üìÑ Page 254: 420 tokens\n",
            "üìÑ Page 255: 464 tokens\n",
            "üìÑ Page 256: 593 tokens\n",
            "üìÑ Page 257: 493 tokens\n",
            "üìÑ Page 258: 554 tokens\n",
            "üìÑ Page 259: 215 tokens\n",
            "üìÑ Page 260: 630 tokens\n",
            "üìÑ Page 261: 415 tokens\n",
            "üìÑ Page 262: 389 tokens\n",
            "üìÑ Page 263: 697 tokens\n",
            "üìÑ Page 264: 517 tokens\n",
            "üìÑ Page 265: 630 tokens\n",
            "üìÑ Page 266: 557 tokens\n",
            "üìÑ Page 267: 604 tokens\n",
            "üìÑ Page 268: 446 tokens\n",
            "üìÑ Page 269: 678 tokens\n",
            "üìÑ Page 270: 389 tokens\n",
            "üìÑ Page 271: 136 tokens\n",
            "üìÑ Page 273: 364 tokens\n",
            "üìÑ Page 274: 587 tokens\n",
            "üìÑ Page 275: 602 tokens\n",
            "üìÑ Page 276: 504 tokens\n",
            "üìÑ Page 277: 511 tokens\n",
            "üìÑ Page 278: 517 tokens\n",
            "üìÑ Page 279: 385 tokens\n",
            "üìÑ Page 281: 479 tokens\n",
            "üìÑ Page 282: 410 tokens\n",
            "üìÑ Page 283: 364 tokens\n",
            "üìÑ Page 284: 733 tokens\n",
            "üìÑ Page 285: 642 tokens\n",
            "üìÑ Page 286: 272 tokens\n",
            "üìÑ Page 287: 593 tokens\n",
            "üìÑ Page 288: 512 tokens\n",
            "üìÑ Page 289: 569 tokens\n",
            "üìÑ Page 290: 609 tokens\n",
            "üìÑ Page 291: 567 tokens\n",
            "üìÑ Page 292: 375 tokens\n",
            "üìÑ Page 293: 606 tokens\n",
            "üìÑ Page 294: 587 tokens\n",
            "üìÑ Page 295: 426 tokens\n",
            "üìÑ Page 296: 252 tokens\n",
            "üìÑ Page 297: 374 tokens\n",
            "üìÑ Page 298: 350 tokens\n",
            "üìÑ Page 299: 595 tokens\n",
            "üìÑ Page 300: 204 tokens\n",
            "üìÑ Page 301: 555 tokens\n",
            "üìÑ Page 302: 260 tokens\n",
            "üìÑ Page 303: 294 tokens\n",
            "üìÑ Page 304: 483 tokens\n",
            "üìÑ Page 305: 343 tokens\n",
            "üìÑ Page 306: 335 tokens\n",
            "üìÑ Page 307: 488 tokens\n",
            "üìÑ Page 308: 206 tokens\n",
            "üìÑ Page 309: 626 tokens\n",
            "üìÑ Page 310: 339 tokens\n",
            "üìÑ Page 311: 418 tokens\n",
            "üìÑ Page 312: 540 tokens\n",
            "üìÑ Page 313: 544 tokens\n",
            "üìÑ Page 315: 454 tokens\n",
            "üìÑ Page 316: 363 tokens\n",
            "üìÑ Page 317: 374 tokens\n",
            "üìÑ Page 318: 579 tokens\n",
            "üìÑ Page 319: 136 tokens\n",
            "üìÑ Page 320: 588 tokens\n",
            "üìÑ Page 321: 388 tokens\n",
            "üìÑ Page 322: 407 tokens\n",
            "üìÑ Page 323: 240 tokens\n",
            "üìÑ Page 324: 181 tokens\n",
            "üìÑ Page 325: 270 tokens\n",
            "üìÑ Page 327: 460 tokens\n",
            "üìÑ Page 328: 685 tokens\n",
            "üìÑ Page 329: 190 tokens\n",
            "üìÑ Page 330: 635 tokens\n",
            "üìÑ Page 331: 470 tokens\n",
            "üìÑ Page 332: 564 tokens\n",
            "üìÑ Page 333: 306 tokens\n",
            "üìÑ Page 334: 520 tokens\n",
            "üìÑ Page 335: 381 tokens\n",
            "üìÑ Page 336: 516 tokens\n",
            "üìÑ Page 337: 232 tokens\n",
            "üìÑ Page 339: 428 tokens\n",
            "üìÑ Page 340: 522 tokens\n",
            "üìÑ Page 341: 263 tokens\n",
            "üìÑ Page 342: 195 tokens\n",
            "üìÑ Page 343: 345 tokens\n",
            "üìÑ Page 344: 348 tokens\n",
            "üìÑ Page 345: 239 tokens\n",
            "üìÑ Page 346: 179 tokens\n",
            "üìÑ Page 347: 387 tokens\n",
            "üìÑ Page 348: 559 tokens\n",
            "üìÑ Page 349: 545 tokens\n",
            "üìÑ Page 350: 380 tokens\n",
            "üìÑ Page 351: 376 tokens\n",
            "üìÑ Page 352: 337 tokens\n",
            "üìÑ Page 353: 316 tokens\n",
            "üìÑ Page 354: 341 tokens\n",
            "üìÑ Page 355: 416 tokens\n",
            "üìÑ Page 356: 319 tokens\n",
            "üìÑ Page 357: 370 tokens\n",
            "üìÑ Page 358: 417 tokens\n",
            "üìÑ Page 359: 551 tokens\n",
            "üìÑ Page 360: 303 tokens\n",
            "üìÑ Page 361: 469 tokens\n",
            "üìÑ Page 362: 542 tokens\n",
            "üìÑ Page 363: 418 tokens\n",
            "üìÑ Page 364: 477 tokens\n",
            "üìÑ Page 365: 703 tokens\n",
            "üìÑ Page 366: 369 tokens\n",
            "üìÑ Page 367: 346 tokens\n",
            "üìÑ Page 368: 226 tokens\n",
            "üìÑ Page 369: 485 tokens\n",
            "üìÑ Page 370: 307 tokens\n",
            "üìÑ Page 371: 405 tokens\n",
            "üìÑ Page 372: 315 tokens\n",
            "üìÑ Page 373: 460 tokens\n",
            "üìÑ Page 374: 523 tokens\n",
            "üìÑ Page 375: 317 tokens\n",
            "üìÑ Page 376: 413 tokens\n",
            "üìÑ Page 377: 479 tokens\n",
            "üìÑ Page 379: 450 tokens\n",
            "üìÑ Page 380: 449 tokens\n",
            "üìÑ Page 381: 435 tokens\n",
            "üìÑ Page 382: 324 tokens\n",
            "üìÑ Page 383: 537 tokens\n",
            "üìÑ Page 384: 737 tokens\n",
            "üìÑ Page 385: 646 tokens\n",
            "üìÑ Page 386: 751 tokens\n",
            "üìÑ Page 387: 751 tokens\n",
            "üìÑ Page 388: 674 tokens\n",
            "üìÑ Page 389: 693 tokens\n",
            "üìÑ Page 390: 646 tokens\n",
            "üìÑ Page 391: 705 tokens\n",
            "üìÑ Page 392: 646 tokens\n",
            "üìÑ Page 393: 662 tokens\n",
            "üìÑ Page 394: 821 tokens\n",
            "üìÑ Page 395: 656 tokens\n",
            "üìÑ Page 396: 304 tokens\n",
            "\n",
            "‚è±Ô∏è Total time: 118.84 seconds\n",
            "üìä Total tokens: 182925\n",
            "üí∞ Estimated embedding cost: $0.004390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib, re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Normalize text to avoid minor whitespace/case changes causing new IDs.\"\"\"\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower()\n",
        "\n",
        "def make_chunk_id(course_id: str, chunk_text: str) -> str:\n",
        "    \"\"\"Deterministic ID based on course_id + chunk_text.\"\"\"\n",
        "    norm = normalize_text(chunk_text)\n",
        "    raw = f\"{course_id}|{norm}\"\n",
        "    return hashlib.blake2b(raw.encode(\"utf-8\"), digest_size=16).hexdigest()\n"
      ],
      "metadata": {
        "id": "rUwMCTl4sSMI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opensearchpy.helpers import parallel_bulk\n",
        "index_name = \"test-auqa\"\n",
        "def doc_to_action(doc):\n",
        "    doc_id = make_chunk_id(doc[\"course_id\"], doc[\"chunk_text\"])\n",
        "    return {\n",
        "        \"_op_type\": \"index\",          # overwrite if exists\n",
        "        \"_index\": index_name,\n",
        "        \"_id\": doc_id,\n",
        "        \"_source\": {\n",
        "            \"chunk_text\": doc[\"chunk_text\"],\n",
        "            \"course_id\": doc[\"course_id\"],\n",
        "            \"filename\": doc[\"filename\"],\n",
        "            \"page_no\": doc[\"page_no\"],\n",
        "            \"vector_field\": doc[\"vector_field\"]  # Titan embedding (1024 floats)\n",
        "        }\n",
        "    }\n",
        "\n",
        "actions = (doc_to_action(d) for d in all_embeddings)\n",
        "\n",
        "for ok, result in parallel_bulk(client, actions, thread_count=1, chunk_size=100):\n",
        "    if not ok:\n",
        "        print(\"‚ùå Failed:\", result)\n",
        "print(\"‚úÖ Bulk upsert finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwKHeDGFsVgo",
        "outputId": "b591f4dc-8ca5-42e2-d1b3-33e59566dd79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Bulk upsert finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "def get_titan_embedding(text: str):\n",
        "    body = json.dumps({\"inputText\": text})\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "    return result[\"embedding\"], result.get(\"inputTextTokenCount\", None)\n",
        "\n",
        "query_text = \"Cost-benefit evaluation techniques\"\n",
        "query_vector, token_count = get_titan_embedding(query_text)\n",
        "\n",
        "print(f\"‚úÖ Query embedded: {len(query_vector)}-dim vector | Tokens: {token_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWNe5hit_6U",
        "outputId": "ce299fb8-8e7d-4bf5-9489-5dc2b9bd099e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Query embedded: 1024-dim vector | Tokens: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 20\n",
        "BM25_WEIGHT = 0.3\n",
        "VECTOR_WEIGHT = 0.7\n",
        "\n",
        "# --- Run BM25 ---\n",
        "bm25_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\"size\": TOP_N, \"query\": {\"match\": {\"chunk_text\": query_text}}}\n",
        ")\n",
        "bm25_hits = bm25_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Run Vector ---\n",
        "vector_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\n",
        "        \"size\": TOP_N,\n",
        "        \"query\": {\n",
        "            \"knn\": {\"vector_field\": {\"vector\": query_vector, \"k\": TOP_N}}\n",
        "        }\n",
        "    }\n",
        ")\n",
        "vector_hits = vector_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Normalize scores ---\n",
        "def normalize_scores(hits):\n",
        "    scores = [h[\"_score\"] for h in hits]\n",
        "    if not scores:\n",
        "        return {}\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if min_s == max_s:\n",
        "        return {h[\"_id\"]: 1.0 for h in hits}\n",
        "    return {h[\"_id\"]: (h[\"_score\"] - min_s) / (max_s - min_s) for h in hits}\n",
        "\n",
        "bm25_norm = normalize_scores(bm25_hits)\n",
        "vector_norm = normalize_scores(vector_hits)\n",
        "\n",
        "# --- Combine ---\n",
        "combined = {}\n",
        "for h in bm25_hits + vector_hits:\n",
        "    _id = h[\"_id\"]\n",
        "    src = h[\"_source\"]\n",
        "    bm25_s = bm25_norm.get(_id, 0.0)\n",
        "    vec_s = vector_norm.get(_id, 0.0)\n",
        "    hybrid_score = BM25_WEIGHT * bm25_s + VECTOR_WEIGHT * vec_s\n",
        "    combined[_id] = {\n",
        "        \"hybrid_score\": hybrid_score,\n",
        "        \"bm25_score\": bm25_s,\n",
        "        \"vector_score\": vec_s,\n",
        "        \"source\": src\n",
        "    }\n",
        "\n",
        "results = sorted(combined.values(), key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
        "\n",
        "# --- Print top 5 results ---\n",
        "print(\"\\nüîé Manual Hybrid Results:\")\n",
        "for r in results[:]:\n",
        "    src = r[\"source\"]\n",
        "    print(f\"Hybrid={r['hybrid_score']:.3f} | BM25={r['bm25_score']:.3f} | Vec={r['vector_score']:.3f}\")\n",
        "    print(f\"Page={src['page_no']} | Course={src['course_id']} | File={src['filename']}\")\n",
        "    print(src[\"chunk_text\"][:10], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hecRPJZwHnp",
        "outputId": "7973c1e8-1f1e-4bf0-9d9f-22ecfe792298"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Manual Hybrid Results:\n",
            "Hybrid=0.919 | BM25=0.729 | Vec=1.000\n",
            "Page=55 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.734 | BM25=0.580 | Vec=0.801\n",
            "Page=53 | Course=CS6022 | File=SPM.pdf\n",
            "3.4 COST-B ...\n",
            "\n",
            "Hybrid=0.557 | BM25=0.312 | Vec=0.663\n",
            "Page=57 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.522 | BM25=0.681 | Vec=0.454\n",
            "Page=61 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.467 | BM25=0.267 | Vec=0.552\n",
            "Page=59 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.466 | BM25=1.000 | Vec=0.237\n",
            "Page=7 | Course=CS6022 | File=SPM.pdf\n",
            "viii\n",
            "CONTE ...\n",
            "\n",
            "Hybrid=0.442 | BM25=0.529 | Vec=0.405\n",
            "Page=52 | Course=CS6022 | File=SPM.pdf\n",
            "40\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.410 | BM25=0.521 | Vec=0.363\n",
            "Page=63 | Course=CS6022 | File=SPM.pdf\n",
            "3.7 RISK E ...\n",
            "\n",
            "Hybrid=0.347 | BM25=0.803 | Vec=0.152\n",
            "Page=49 | Course=CS6022 | File=SPM.pdf\n",
            "Chapter 3\n",
            " ...\n",
            "\n",
            "Hybrid=0.277 | BM25=0.923 | Vec=0.000\n",
            "Page=155 | Course=CS6022 | File=SPM.pdf\n",
            "7.7 EVALUA ...\n",
            "\n",
            "Hybrid=0.274 | BM25=0.505 | Vec=0.175\n",
            "Page=67 | Course=CS6022 | File=SPM.pdf\n",
            "3.9 FURTHE ...\n",
            "\n",
            "Hybrid=0.215 | BM25=0.000 | Vec=0.306\n",
            "Page=56 | Course=CS6022 | File=SPM.pdf\n",
            "44\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.188 | BM25=0.627 | Vec=0.000\n",
            "Page=385 | Course=CS6022 | File=SPM.pdf\n",
            "INDEX\n",
            "373\n",
            " ...\n",
            "\n",
            "Hybrid=0.174 | BM25=0.581 | Vec=0.000\n",
            "Page=148 | Course=CS6022 | File=SPM.pdf\n",
            "136\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.163 | BM25=0.279 | Vec=0.113\n",
            "Page=62 | Course=CS6022 | File=SPM.pdf\n",
            "50\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.145 | BM25=0.000 | Vec=0.208\n",
            "Page=212 | Course=CS6022 | File=SPM.pdf\n",
            "200\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.144 | BM25=0.000 | Vec=0.206\n",
            "Page=64 | Course=CS6022 | File=SPM.pdf\n",
            "52\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.092 | BM25=0.306 | Vec=0.000\n",
            "Page=393 | Course=CS6022 | File=SPM.pdf\n",
            "INDEX\n",
            "381\n",
            " ...\n",
            "\n",
            "Hybrid=0.088 | BM25=0.295 | Vec=0.000\n",
            "Page=349 | Course=CS6022 | File=SPM.pdf\n",
            "CHAPTER 3\n",
            " ...\n",
            "\n",
            "Hybrid=0.081 | BM25=0.270 | Vec=0.000\n",
            "Page=347 | Course=CS6022 | File=SPM.pdf\n",
            "CHAPTER 3\n",
            " ...\n",
            "\n",
            "Hybrid=0.062 | BM25=0.000 | Vec=0.089\n",
            "Page=54 | Course=CS6022 | File=SPM.pdf\n",
            "42\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.058 | BM25=0.000 | Vec=0.083\n",
            "Page=214 | Course=CS6022 | File=SPM.pdf\n",
            "202\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.057 | BM25=0.000 | Vec=0.081\n",
            "Page=192 | Course=CS6022 | File=SPM.pdf\n",
            "180\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.051 | BM25=0.000 | Vec=0.073\n",
            "Page=193 | Course=CS6022 | File=SPM.pdf\n",
            "9.6 EARNED ...\n",
            "\n",
            "Hybrid=0.023 | BM25=0.076 | Vec=0.000\n",
            "Page=151 | Course=CS6022 | File=SPM.pdf\n",
            "7.5 RISK A ...\n",
            "\n",
            "Hybrid=0.014 | BM25=0.045 | Vec=0.000\n",
            "Page=305 | Course=CS6022 | File=SPM.pdf\n",
            "C.5 ACQUIS ...\n",
            "\n",
            "Hybrid=0.003 | BM25=0.000 | Vec=0.004\n",
            "Page=58 | Course=CS6022 | File=SPM.pdf\n",
            "46\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.000 | BM25=0.000 | Vec=0.000\n",
            "Page=84 | Course=CS6022 | File=SPM.pdf\n",
            "72\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.000 | BM25=0.000 | Vec=0.000\n",
            "Page=66 | Course=CS6022 | File=SPM.pdf\n",
            "54\n",
            "CHAPTER ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect top 5 page chunks into one string\n",
        "context_text = \"\\n\\n\".join([r[\"source\"][\"chunk_text\"] for r in results[:]])\n",
        "ques_query = query_text   # the query you used\n",
        "no = 20\n"
      ],
      "metadata": {
        "id": "PiJhquiAyjO2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an AI assistant specialized in **question generation and analysis**.\n",
        "\n",
        "Your goals:\n",
        "1. Generate insightful and generic questions based on the given context and user query.\n",
        "   - Do not copy sentences from the context.\n",
        "   - Make questions broad and meaningful, not text-specific.\n",
        "   - Generate at least **{no} questions**.\n",
        "2. For each generated question, classify it by:\n",
        "   - **Difficulty level**:\n",
        "       * Easy ‚Üí simple recall/basic understanding (1-sentence answers).\n",
        "       * Medium ‚Üí requires moderate understanding, application, or 2‚Äì3 steps of reasoning.\n",
        "       * Hard ‚Üí requires deep understanding, critical thinking, multi-step reasoning.\n",
        "   - **Bloom's taxonomy level**:\n",
        "       * Choose one of: Remember, Understand, Apply, Analyze, Evaluate, Create.\n",
        "\n",
        "### Input\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "User Query (Focus Topic):\n",
        "{ques_query}\n",
        "\n",
        "### Output\n",
        "Return the results in **valid JSON** format, as a list of objects.\n",
        "Each object must look like this:\n",
        "{{\n",
        "  \"question\": \"...\",\n",
        "  \"difficulty\": \"...\",\n",
        "  \"blooms_level\": \"...\"\n",
        "}}\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8M7L4Eiryjio"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, json, time\n",
        "\n",
        "REGION = \"ap-south-1\"\n",
        "PROFILE_ARN = \"arn:aws:bedrock:ap-south-1:850146468080:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "body = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 500,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "start = time.time()\n",
        "resp = bedrock_rt.invoke_model(modelId=PROFILE_ARN, body=json.dumps(body))\n",
        "elapsed = time.time() - start\n",
        "\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "\n",
        "reply = output[\"content\"][0][\"text\"]\n",
        "\n",
        "# Usage metadata (if present)\n",
        "usage = output.get(\"usage\", {})\n",
        "input_tokens = usage.get(\"input_tokens\", \"N/A\")\n",
        "output_tokens = usage.get(\"output_tokens\", \"N/A\")\n",
        "\n",
        "print(\"‚úÖ Generated Output:\\n\")\n",
        "print(reply)\n",
        "\n",
        "print(\"\\nüìä Stats:\")\n",
        "print(\"  Input tokens :\", input_tokens)\n",
        "print(\"  Output tokens:\", output_tokens)\n",
        "print(f\"  Time taken   : {elapsed:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PimWoLJNyjlt",
        "outputId": "ad6b073d-9d6e-428b-fb30-d9b1f9f06beb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated Output:\n",
            "\n",
            "{\n",
            "  \"questions\": [\n",
            "    {\n",
            "      \"question\": \"What are the key differences between net profit and net present value as project evaluation methods?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\" \n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How does the timing of cash flows impact the financial evaluation of a project?\",\n",
            "      \"difficulty\": \"Medium\", \n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the advantages and disadvantages of using return on investment (ROI) as a project evaluation technique?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Evaluate\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why is it important to consider both the size of investment and timing of returns when evaluating projects?\",\n",
            "      \"difficulty\": \"Easy\",\n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can risk factors be incorporated into the financial evaluation of a project?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Apply\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What role does the discount rate play in net present value calculations and how is it determined?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why might projects with the same net profit have different net present values?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the key limitations of using payback period as a project selection criterion?\",\n",
            "      \"difficulty\": \"Easy\",\n",
            "      \"blooms_level\": \"Evaluate\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can decision trees help in evaluating projects with multiple possible outcomes?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Apply\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What factors should be considered when choosing between competing project proposals?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Evaluate\"\n",
            "    },\n",
            "\n",
            "üìä Stats:\n",
            "  Input tokens : 16627\n",
            "  Output tokens: 500\n",
            "  Time taken   : 12.40 seconds\n"
          ]
        }
      ]
    }
  ]
}