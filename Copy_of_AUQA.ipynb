{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnR2E8RWcz++3c/Rlnzpc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auqa5325/AUQA-colab/blob/main/Copy_of_AUQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-uDEQ4a38AH",
        "outputId": "0dd9ea0e-b415-4b68-88c3-c24e64d2276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install boto3 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY_ID     = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "#AWS_SESSION_TOKEN     = userdata.get(\"AWS_SESSION_TOKEN\")  # may be None\n",
        "AWS_REGION = userdata.get(\"AWS_REGION\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
        "\n",
        "print(\"✅ Credentials set. Region:\", AWS_REGION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8aVrSE4ZiZ",
        "outputId": "bc768233-e5be-4b5c-e292-b3ca100c78e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Credentials set. Region: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]\n",
        "session = boto3.Session(region_name=REGION)\n",
        "bedrock         = session.client(\"bedrock\")\n",
        "bedrock_runtime = session.client(\"bedrock-runtime\")\n",
        "print(\"✅ boto3 session initialized in:\", session.region_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqqWGkX4-QA",
        "outputId": "97938a45-4fdd-4934-fd4b-0442e7f0e2b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ boto3 session initialized in: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "s = boto3.Session(region_name=\"ap-south-1\")\n",
        "print(s.client(\"sts\").get_caller_identity())   # must print Account, Arn\n",
        "creds = s.get_credentials().get_frozen_credentials()\n",
        "print(\"AccessKey:\", creds.access_key[:4], \"HasToken:\", bool(creds.token))\n"
      ],
      "metadata": {
        "id": "UT2f-KtPS8C0",
        "outputId": "50ec2908-7828-47b9-9e96-004a73232aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY1NDg4MzQxNDA6Ujp3Y0FQd1kzUA==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Sat, 30 Aug 2025 10:13:54 GMT'}, 'RetryAttempts': 0}}\n",
            "AccessKey: AKIA HasToken: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output dict structure :\n",
        "* id → unique identifier for this response (good for\n",
        "logging/debugging).\n",
        "\n",
        "* type → \"message\" → tells you this is a message object.\n",
        "\n",
        "* role → \"assistant\" → the speaker role (assistant vs. user).\n",
        "\n",
        "* model → which model gave this reply (claude-3-sonnet-20240229).\n",
        "\n",
        "* content → a list of parts that make up the response.\n",
        "\n",
        "* stop_reason → why the model stopped (e.g., end_turn, max_tokens).\n",
        "\n",
        "* stop_sequence → custom sequence that stopped generation (here it’s None).\n",
        "\n",
        "* usage → token usage info (handy for cost + rate limits)."
      ],
      "metadata": {
        "id": "wbqD05a1_0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "prompt = \"explain tcp ip?\"\n",
        "\n",
        "body = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 100,\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "print(output[\"outputs\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "xKuzvoVdBegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "session = boto3.Session(region_name=\"ap-south-1\")\n",
        "sts = session.client(\"sts\")\n",
        "print(sts.get_caller_identity())  # should return Account, Arn, UserId\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5nKJfpQRoqH",
        "outputId": "6c4b9df1-8ef1-42e2-acc7-d3e67a45b3d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY0NTkzNTE3NDk6UjpJd1BrYlNOVg==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Fri, 29 Aug 2025 09:22:31 GMT'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'outputs': [{'text': '\\n\\nTCP/IP is a suite of protocols that defines the Internet. Originally designed for the UNIX operating system. The TCP/IP protocol suite is a four-layer model that provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed, and received.\\n\\nThe four layers of the TCP/IP model are:\\n\\n1. The Application Layer: This is the topmost layer of the TCP/IP', 'stop_reason': 'length'}]}"
      ],
      "metadata": {
        "id": "zlZJ3onqB_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opensearch-py requests-aws4auth\n",
        "\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "from requests_aws4auth import AWS4Auth\n",
        "import boto3\n",
        "\n",
        "# --- CONFIG ---\n",
        "host   = \"https://search-test1-annauniv-pcx3f52wxykhpd4md6v4bjeqdy.ap-south-1.es.amazonaws.com\"  # OpenSearch domain endpoint\n",
        "region = \"ap-south-1\"\n",
        "service = \"es\"\n",
        "\n",
        "# --- AWS SigV4 Auth ---\n",
        "session = boto3.Session()\n",
        "credentials = session.get_credentials()\n",
        "awsauth = AWS4Auth(\n",
        "    credentials.access_key,\n",
        "    credentials.secret_key,\n",
        "    region,\n",
        "    service,\n",
        "    session_token=credentials.token\n",
        ")\n",
        "\n",
        "# --- OpenSearch Client ---\n",
        "client = OpenSearch(\n",
        "    hosts=[host],\n",
        "    http_auth=awsauth,\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    connection_class=RequestsHttpConnection\n",
        ")\n",
        "\n",
        "# --- TEST: Get cluster info ---\n",
        "print(client.info())\n"
      ],
      "metadata": {
        "id": "l5z2sIp87q1F",
        "outputId": "fd28194c-422f-4221-b070-f9d0bb23b28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/371.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m368.6/371.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.5/371.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h{'name': 'd69eb8cacc6e2cf42750a5f4788db327', 'cluster_name': '850146468080:test1-annauniv', 'cluster_uuid': 'qhkHo9X9Suuw8REB9uInGQ', 'version': {'distribution': 'opensearch', 'number': '2.19.0', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-07-24T06:15:41.026838036Z', 'build_snapshot': False, 'lucene_version': '9.12.1', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "index_body = {\n",
        "    \"settings\": {\n",
        "        \"index\": {\n",
        "            \"knn\": True,\n",
        "            \"knn.algo_param.ef_search\": 512,\n",
        "            \"number_of_shards\": 5,\n",
        "            \"number_of_replicas\": 1\n",
        "        }\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            # main chunk text\n",
        "            \"chunk_text\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # ids & names\n",
        "            \"course_id\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"filename\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # page number\n",
        "            \"page_no\": { \"type\": \"long\" },\n",
        "\n",
        "\n",
        "\n",
        "            # vector index used for ANN similarity search\n",
        "            \"vector_field\": {\n",
        "                \"type\": \"knn_vector\",\n",
        "                \"dimension\": 1024,\n",
        "                \"method\": {\n",
        "                    \"name\": \"hnsw\",\n",
        "                    \"space_type\": \"cosinesimil\",\n",
        "                    \"engine\": \"nmslib\",\n",
        "                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# recreate index\n",
        "if client.indices.exists(index=index_name):\n",
        "    print(f\"Index '{index_name}' already exists. Deleting and recreating...\")\n",
        "    client.indices.delete(index=index_name)\n",
        "\n",
        "client.indices.create(index=index_name, body=index_body)\n",
        "print(f\"✅ Index '{index_name}' created successfully!\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UcTIfVU4RS02",
        "outputId": "7b3b813f-69be-4de4-ef61-e6b822f22dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-2645953989.py, line 64)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2645953989.py\"\u001b[0;36m, line \u001b[0;32m64\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'test-annauniv': {'aliases': {}, 'mappings': {'properties': {'course_id': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'embedding': {'type': 'float'}, 'metadata': {'properties': {'page': {'type': 'long'}, 'source': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'page_no': {'type': 'long'}, 'text': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'vector_field': {'type': 'knn_vector', 'dimension': 1024, 'method': {'engine': 'nmslib', 'space_type': 'cosinesimil', 'name': 'hnsw', 'parameters': {'ef_construction': 512, 'm': 16}}}}}, 'settings': {'index': {'replication': {'type': 'DOCUMENT'}, 'refresh_interval': '1s', 'number_of_shards': '5', 'knn.algo_param': {'ef_search': '512'}, 'provided_name': 'test-annauniv', 'knn': 'true', 'creation_date': '1756144596121', 'number_of_replicas': '1', 'uuid': 'rumasDZaQC6xBy4G48v1KQ', 'version': {'created': '136407827'}}}}}\n",
        "\n"
      ],
      "metadata": {
        "id": "n-xX1sT3yxqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, time, re\n",
        "\n",
        "BUCKET = \"anna-univ-qna\"\n",
        "DOCUMENT = \"Textbooks/SPM.pdf\"\n",
        "REGION = \"ap-south-1\"\n",
        "COURSE_ID = \"CS6022\"   # <-- set this per textbook\n",
        "FILENAME = \"SPM.pdf\"\n",
        "\n",
        "textract = boto3.client(\"textract\", region_name=REGION)\n",
        "\n",
        "# --- Start async Textract job ---\n",
        "start = time.time()\n",
        "response = textract.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": BUCKET, \"Name\": DOCUMENT}}\n",
        ")\n",
        "print(\"FILENAME =\",FILENAME)\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"✅ Started Textract JobId: {job_id}\")\n",
        "\n",
        "# --- Wait for completion ---\n",
        "while True:\n",
        "    result = textract.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"❌ Textract job failed!\")\n",
        "\n",
        "pages = result[\"DocumentMetadata\"][\"Pages\"]\n",
        "print(f\"📄 Pages: {pages}\")\n",
        "print(f\"⏱️ Time taken: {elapsed:.2f} sec\")\n",
        "print(f\"💰 Estimated cost: ${(pages/1000)*1.5:.4f}\")\n",
        "\n",
        "# --- Collect page-wise text into docs with metadata ---\n",
        "page_texts = {}\n",
        "next_token = None\n",
        "\n",
        "while True:\n",
        "    if next_token:\n",
        "        result = textract.get_document_text_detection(JobId=job_id, NextToken=next_token)\n",
        "    else:\n",
        "        result = textract.get_document_text_detection(JobId=job_id)\n",
        "\n",
        "    for block in result[\"Blocks\"]:\n",
        "        if block[\"BlockType\"] == \"LINE\":\n",
        "            page_no = block[\"Page\"]\n",
        "            page_texts.setdefault(page_no, []).append(block[\"Text\"])\n",
        "\n",
        "    next_token = result.get(\"NextToken\")\n",
        "    if not next_token:\n",
        "        break\n",
        "\n",
        "# Build document objects (ready to send to Titan for embeddings)\n",
        "docs = []\n",
        "for page_no, lines in page_texts.items():\n",
        "    chunk_text = \"\\n\".join(lines)\n",
        "    doc = {\n",
        "        \"chunk_text\": chunk_text,\n",
        "        \"course_id\": COURSE_ID,\n",
        "        \"filename\": FILENAME,\n",
        "        \"page_no\": page_no\n",
        "    }\n",
        "    docs.append(doc)\n",
        "\n",
        "# --- Preview first 5 pages ---\n",
        "print(\"\\n📑 First 5 pages extracted:\")\n",
        "for d in docs[:5]:\n",
        "    print(f\"\\nPage {d['page_no']} | Course: {d['course_id']} | File: {d['filename']}\")\n",
        "    print(d[\"chunk_text\"][:400], \"...\")\n"
      ],
      "metadata": {
        "id": "DVaNNUjgqWN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "806b9018-75df-421c-b8ef-aeea73a65c2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FILENAME = SPM.pdf\n",
            "✅ Started Textract JobId: ce41d655412d458abda1771970d243aee8b1fcb47f07c25ecdfed15620863448\n",
            "📄 Pages: 396\n",
            "⏱️ Time taken: 231.58 sec\n",
            "💰 Estimated cost: $0.5940\n",
            "\n",
            "📑 First 5 pages extracted:\n",
            "\n",
            "Page 1 | Course: CS6022 | File: SPM.pdf\n",
            "BoB HUGHES AND MIKE COTTERELL\n",
            "Software\n",
            "Project\n",
            "Management Second Edition\n",
            "Tistimating\n",
            "www.mcgraw-hill.co.uk/hughes ...\n",
            "\n",
            "Page 2 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition) ...\n",
            "\n",
            "Page 3 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition)\n",
            "Bob Hughes and Mike Cotterell,\n",
            "School of Information Management, University of Brighton\n",
            "The McGraw-Hill Companies\n",
            "London\n",
            "Burr Ridge, IL\n",
            "New York\n",
            "St Louis\n",
            "San Francisco\n",
            "Auckland\n",
            "Bogotá Caracas\n",
            "Lisbon\n",
            "Madrid\n",
            "Mexico\n",
            "Milan\n",
            "Montreal\n",
            "New Delhi\n",
            "Panama\n",
            "Paris\n",
            "San Juan\n",
            "São Paulo\n",
            "Singapore\n",
            "Tokyo\n",
            "Toronto ...\n",
            "\n",
            "Page 4 | Course: CS6022 | File: SPM.pdf\n",
            "Published by\n",
            "McGraw-Hill Publishing Company\n",
            "SHOPPENHANGERS ROAD, MAIDENHEAD, BERKSHIRE, SL6 2QL, ENGLAND\n",
            "Telephone: +44(o) 1628 502500\n",
            "Fax: +44(o) 1628 770224\n",
            "Web site: http://www.megraw-hill.co.uk\n",
            "British Library Cataloguing in Publication Data\n",
            "A catalogue record for this book is available from the British Library\n",
            "ISBN 007 709505 7\n",
            "Library of Congress cataloguing in publication data\n",
            "The LOC data  ...\n",
            "\n",
            "Page 5 | Course: CS6022 | File: SPM.pdf\n",
            "The road to hell is paved with works-in-progress.\n",
            "Philip Roth ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, json, time\n",
        "\n",
        "# --- Config ---\n",
        "REGION = \"ap-south-1\"\n",
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "PRICE_PER_1K = 0.000024  # USD per 1k tokens\n",
        "\n",
        "# Assume docs is already built from Textract step\n",
        "# docs = [\n",
        "#   {\"chunk_text\": \"...\", \"course_id\":\"CS101\", \"filename\":\"SPM.pdf\", \"page_no\":1},\n",
        "#   ...\n",
        "# ]\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "all_embeddings = []\n",
        "total_tokens = 0\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for d in docs:\n",
        "    body = json.dumps({\n",
        "        \"inputText\": d[\"chunk_text\"]\n",
        "    })\n",
        "\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "\n",
        "    # Titan Embedding returns: { \"embedding\": [...], \"inputTextTokenCount\": N }\n",
        "    vector = result[\"embedding\"]\n",
        "    tokens = result.get(\"inputTextTokenCount\", len(d[\"chunk_text\"].split()))  # fallback\n",
        "\n",
        "    d[\"vector_field\"] = vector\n",
        "    d[\"tokens\"] = tokens\n",
        "\n",
        "    all_embeddings.append(d)\n",
        "\n",
        "    total_tokens += tokens\n",
        "    print(f\"📄 Page {d['page_no']}: {tokens} tokens\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "cost = (total_tokens / 1000) * PRICE_PER_1K\n",
        "\n",
        "print(\"\\n⏱️ Total time:\", round(elapsed, 2), \"seconds\")\n",
        "print(\"📊 Total tokens:\", total_tokens)\n",
        "print(f\"💰 Estimated embedding cost: ${cost:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f_xnQuxafnwr",
        "outputId": "419eb821-d9c8-46a0-e861-eadaf9e012da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Page 1: 37 tokens\n",
            "📄 Page 2: 9 tokens\n",
            "📄 Page 3: 96 tokens\n",
            "📄 Page 4: 345 tokens\n",
            "📄 Page 5: 14 tokens\n",
            "📄 Page 6: 393 tokens\n",
            "📄 Page 7: 424 tokens\n",
            "📄 Page 8: 393 tokens\n",
            "📄 Page 9: 393 tokens\n",
            "📄 Page 10: 253 tokens\n",
            "📄 Page 11: 517 tokens\n",
            "📄 Page 12: 482 tokens\n",
            "📄 Page 13: 235 tokens\n",
            "📄 Page 14: 460 tokens\n",
            "📄 Page 15: 551 tokens\n",
            "📄 Page 16: 373 tokens\n",
            "📄 Page 17: 575 tokens\n",
            "📄 Page 18: 463 tokens\n",
            "📄 Page 19: 400 tokens\n",
            "📄 Page 20: 529 tokens\n",
            "📄 Page 21: 457 tokens\n",
            "📄 Page 22: 410 tokens\n",
            "📄 Page 23: 645 tokens\n",
            "📄 Page 24: 322 tokens\n",
            "📄 Page 25: 558 tokens\n",
            "📄 Page 26: 550 tokens\n",
            "📄 Page 27: 411 tokens\n",
            "📄 Page 28: 588 tokens\n",
            "📄 Page 29: 457 tokens\n",
            "📄 Page 30: 558 tokens\n",
            "📄 Page 31: 365 tokens\n",
            "📄 Page 32: 603 tokens\n",
            "📄 Page 33: 403 tokens\n",
            "📄 Page 34: 273 tokens\n",
            "📄 Page 35: 515 tokens\n",
            "📄 Page 36: 542 tokens\n",
            "📄 Page 37: 588 tokens\n",
            "📄 Page 38: 553 tokens\n",
            "📄 Page 39: 520 tokens\n",
            "📄 Page 40: 540 tokens\n",
            "📄 Page 41: 276 tokens\n",
            "📄 Page 42: 443 tokens\n",
            "📄 Page 43: 369 tokens\n",
            "📄 Page 44: 600 tokens\n",
            "📄 Page 45: 532 tokens\n",
            "📄 Page 46: 529 tokens\n",
            "📄 Page 47: 435 tokens\n",
            "📄 Page 48: 530 tokens\n",
            "📄 Page 49: 328 tokens\n",
            "📄 Page 50: 339 tokens\n",
            "📄 Page 51: 507 tokens\n",
            "📄 Page 52: 577 tokens\n",
            "📄 Page 53: 568 tokens\n",
            "📄 Page 54: 619 tokens\n",
            "📄 Page 55: 590 tokens\n",
            "📄 Page 56: 425 tokens\n",
            "📄 Page 57: 731 tokens\n",
            "📄 Page 58: 750 tokens\n",
            "📄 Page 59: 579 tokens\n",
            "📄 Page 60: 701 tokens\n",
            "📄 Page 61: 712 tokens\n",
            "📄 Page 62: 622 tokens\n",
            "📄 Page 63: 553 tokens\n",
            "📄 Page 64: 617 tokens\n",
            "📄 Page 65: 578 tokens\n",
            "📄 Page 66: 315 tokens\n",
            "📄 Page 67: 350 tokens\n",
            "📄 Page 69: 319 tokens\n",
            "📄 Page 70: 269 tokens\n",
            "📄 Page 71: 518 tokens\n",
            "📄 Page 72: 529 tokens\n",
            "📄 Page 73: 579 tokens\n",
            "📄 Page 74: 597 tokens\n",
            "📄 Page 75: 402 tokens\n",
            "📄 Page 76: 598 tokens\n",
            "📄 Page 77: 372 tokens\n",
            "📄 Page 78: 422 tokens\n",
            "📄 Page 79: 559 tokens\n",
            "📄 Page 80: 334 tokens\n",
            "📄 Page 81: 513 tokens\n",
            "📄 Page 82: 515 tokens\n",
            "📄 Page 83: 481 tokens\n",
            "📄 Page 84: 535 tokens\n",
            "📄 Page 85: 329 tokens\n",
            "📄 Page 86: 468 tokens\n",
            "📄 Page 87: 527 tokens\n",
            "📄 Page 88: 506 tokens\n",
            "📄 Page 89: 470 tokens\n",
            "📄 Page 90: 108 tokens\n",
            "📄 Page 91: 328 tokens\n",
            "📄 Page 92: 729 tokens\n",
            "📄 Page 93: 627 tokens\n",
            "📄 Page 94: 708 tokens\n",
            "📄 Page 95: 415 tokens\n",
            "📄 Page 96: 700 tokens\n",
            "📄 Page 97: 565 tokens\n",
            "📄 Page 98: 604 tokens\n",
            "📄 Page 99: 607 tokens\n",
            "📄 Page 100: 625 tokens\n",
            "📄 Page 101: 664 tokens\n",
            "📄 Page 102: 488 tokens\n",
            "📄 Page 103: 546 tokens\n",
            "📄 Page 104: 550 tokens\n",
            "📄 Page 105: 379 tokens\n",
            "📄 Page 106: 525 tokens\n",
            "📄 Page 107: 451 tokens\n",
            "📄 Page 108: 482 tokens\n",
            "📄 Page 109: 624 tokens\n",
            "📄 Page 110: 560 tokens\n",
            "📄 Page 111: 430 tokens\n",
            "📄 Page 112: 625 tokens\n",
            "📄 Page 113: 557 tokens\n",
            "📄 Page 114: 501 tokens\n",
            "📄 Page 115: 273 tokens\n",
            "📄 Page 116: 426 tokens\n",
            "📄 Page 117: 80 tokens\n",
            "📄 Page 119: 301 tokens\n",
            "📄 Page 120: 571 tokens\n",
            "📄 Page 121: 676 tokens\n",
            "📄 Page 122: 312 tokens\n",
            "📄 Page 123: 568 tokens\n",
            "📄 Page 124: 507 tokens\n",
            "📄 Page 125: 654 tokens\n",
            "📄 Page 126: 488 tokens\n",
            "📄 Page 127: 475 tokens\n",
            "📄 Page 128: 604 tokens\n",
            "📄 Page 129: 410 tokens\n",
            "📄 Page 130: 603 tokens\n",
            "📄 Page 131: 498 tokens\n",
            "📄 Page 132: 186 tokens\n",
            "📄 Page 133: 392 tokens\n",
            "📄 Page 134: 445 tokens\n",
            "📄 Page 135: 490 tokens\n",
            "📄 Page 136: 544 tokens\n",
            "📄 Page 137: 436 tokens\n",
            "📄 Page 138: 500 tokens\n",
            "📄 Page 139: 518 tokens\n",
            "📄 Page 140: 552 tokens\n",
            "📄 Page 141: 600 tokens\n",
            "📄 Page 142: 661 tokens\n",
            "📄 Page 143: 425 tokens\n",
            "📄 Page 144: 166 tokens\n",
            "📄 Page 145: 334 tokens\n",
            "📄 Page 146: 282 tokens\n",
            "📄 Page 147: 546 tokens\n",
            "📄 Page 148: 495 tokens\n",
            "📄 Page 149: 624 tokens\n",
            "📄 Page 150: 580 tokens\n",
            "📄 Page 151: 642 tokens\n",
            "📄 Page 152: 399 tokens\n",
            "📄 Page 153: 674 tokens\n",
            "📄 Page 154: 487 tokens\n",
            "📄 Page 155: 482 tokens\n",
            "📄 Page 156: 538 tokens\n",
            "📄 Page 157: 402 tokens\n",
            "📄 Page 158: 513 tokens\n",
            "📄 Page 159: 541 tokens\n",
            "📄 Page 160: 477 tokens\n",
            "📄 Page 161: 423 tokens\n",
            "📄 Page 162: 340 tokens\n",
            "📄 Page 163: 354 tokens\n",
            "📄 Page 164: 298 tokens\n",
            "📄 Page 165: 533 tokens\n",
            "📄 Page 166: 789 tokens\n",
            "📄 Page 167: 567 tokens\n",
            "📄 Page 168: 331 tokens\n",
            "📄 Page 169: 460 tokens\n",
            "📄 Page 170: 564 tokens\n",
            "📄 Page 171: 858 tokens\n",
            "📄 Page 172: 271 tokens\n",
            "📄 Page 173: 553 tokens\n",
            "📄 Page 174: 591 tokens\n",
            "📄 Page 175: 141 tokens\n",
            "📄 Page 176: 777 tokens\n",
            "📄 Page 177: 472 tokens\n",
            "📄 Page 178: 320 tokens\n",
            "📄 Page 179: 257 tokens\n",
            "📄 Page 180: 237 tokens\n",
            "📄 Page 181: 260 tokens\n",
            "📄 Page 182: 304 tokens\n",
            "📄 Page 183: 425 tokens\n",
            "📄 Page 184: 429 tokens\n",
            "📄 Page 185: 559 tokens\n",
            "📄 Page 186: 471 tokens\n",
            "📄 Page 187: 512 tokens\n",
            "📄 Page 188: 510 tokens\n",
            "📄 Page 189: 559 tokens\n",
            "📄 Page 190: 525 tokens\n",
            "📄 Page 191: 370 tokens\n",
            "📄 Page 192: 456 tokens\n",
            "📄 Page 193: 372 tokens\n",
            "📄 Page 194: 484 tokens\n",
            "📄 Page 195: 249 tokens\n",
            "📄 Page 196: 485 tokens\n",
            "📄 Page 197: 474 tokens\n",
            "📄 Page 198: 602 tokens\n",
            "📄 Page 199: 679 tokens\n",
            "📄 Page 200: 468 tokens\n",
            "📄 Page 201: 382 tokens\n",
            "📄 Page 202: 465 tokens\n",
            "📄 Page 203: 299 tokens\n",
            "📄 Page 204: 788 tokens\n",
            "📄 Page 205: 526 tokens\n",
            "📄 Page 206: 485 tokens\n",
            "📄 Page 207: 614 tokens\n",
            "📄 Page 208: 561 tokens\n",
            "📄 Page 209: 601 tokens\n",
            "📄 Page 210: 582 tokens\n",
            "📄 Page 211: 526 tokens\n",
            "📄 Page 212: 728 tokens\n",
            "📄 Page 213: 662 tokens\n",
            "📄 Page 214: 567 tokens\n",
            "📄 Page 215: 478 tokens\n",
            "📄 Page 216: 447 tokens\n",
            "📄 Page 217: 537 tokens\n",
            "📄 Page 218: 594 tokens\n",
            "📄 Page 219: 671 tokens\n",
            "📄 Page 220: 479 tokens\n",
            "📄 Page 221: 347 tokens\n",
            "📄 Page 223: 242 tokens\n",
            "📄 Page 224: 263 tokens\n",
            "📄 Page 225: 621 tokens\n",
            "📄 Page 226: 572 tokens\n",
            "📄 Page 227: 804 tokens\n",
            "📄 Page 228: 558 tokens\n",
            "📄 Page 229: 578 tokens\n",
            "📄 Page 230: 654 tokens\n",
            "📄 Page 231: 464 tokens\n",
            "📄 Page 232: 562 tokens\n",
            "📄 Page 233: 490 tokens\n",
            "📄 Page 234: 622 tokens\n",
            "📄 Page 235: 513 tokens\n",
            "📄 Page 236: 589 tokens\n",
            "📄 Page 237: 607 tokens\n",
            "📄 Page 238: 440 tokens\n",
            "📄 Page 239: 446 tokens\n",
            "📄 Page 240: 579 tokens\n",
            "📄 Page 241: 470 tokens\n",
            "📄 Page 242: 607 tokens\n",
            "📄 Page 243: 648 tokens\n",
            "📄 Page 244: 423 tokens\n",
            "📄 Page 245: 296 tokens\n",
            "📄 Page 247: 331 tokens\n",
            "📄 Page 248: 266 tokens\n",
            "📄 Page 249: 552 tokens\n",
            "📄 Page 250: 496 tokens\n",
            "📄 Page 251: 385 tokens\n",
            "📄 Page 252: 681 tokens\n",
            "📄 Page 253: 443 tokens\n",
            "📄 Page 254: 420 tokens\n",
            "📄 Page 255: 464 tokens\n",
            "📄 Page 256: 593 tokens\n",
            "📄 Page 257: 493 tokens\n",
            "📄 Page 258: 554 tokens\n",
            "📄 Page 259: 215 tokens\n",
            "📄 Page 260: 630 tokens\n",
            "📄 Page 261: 415 tokens\n",
            "📄 Page 262: 389 tokens\n",
            "📄 Page 263: 697 tokens\n",
            "📄 Page 264: 517 tokens\n",
            "📄 Page 265: 630 tokens\n",
            "📄 Page 266: 557 tokens\n",
            "📄 Page 267: 604 tokens\n",
            "📄 Page 268: 446 tokens\n",
            "📄 Page 269: 678 tokens\n",
            "📄 Page 270: 389 tokens\n",
            "📄 Page 271: 136 tokens\n",
            "📄 Page 273: 364 tokens\n",
            "📄 Page 274: 587 tokens\n",
            "📄 Page 275: 602 tokens\n",
            "📄 Page 276: 504 tokens\n",
            "📄 Page 277: 511 tokens\n",
            "📄 Page 278: 517 tokens\n",
            "📄 Page 279: 385 tokens\n",
            "📄 Page 281: 479 tokens\n",
            "📄 Page 282: 410 tokens\n",
            "📄 Page 283: 364 tokens\n",
            "📄 Page 284: 733 tokens\n",
            "📄 Page 285: 642 tokens\n",
            "📄 Page 286: 272 tokens\n",
            "📄 Page 287: 593 tokens\n",
            "📄 Page 288: 512 tokens\n",
            "📄 Page 289: 569 tokens\n",
            "📄 Page 290: 609 tokens\n",
            "📄 Page 291: 567 tokens\n",
            "📄 Page 292: 375 tokens\n",
            "📄 Page 293: 606 tokens\n",
            "📄 Page 294: 587 tokens\n",
            "📄 Page 295: 426 tokens\n",
            "📄 Page 296: 252 tokens\n",
            "📄 Page 297: 374 tokens\n",
            "📄 Page 298: 350 tokens\n",
            "📄 Page 299: 595 tokens\n",
            "📄 Page 300: 204 tokens\n",
            "📄 Page 301: 555 tokens\n",
            "📄 Page 302: 260 tokens\n",
            "📄 Page 303: 294 tokens\n",
            "📄 Page 304: 483 tokens\n",
            "📄 Page 305: 343 tokens\n",
            "📄 Page 306: 335 tokens\n",
            "📄 Page 307: 488 tokens\n",
            "📄 Page 308: 206 tokens\n",
            "📄 Page 309: 626 tokens\n",
            "📄 Page 310: 339 tokens\n",
            "📄 Page 311: 418 tokens\n",
            "📄 Page 312: 540 tokens\n",
            "📄 Page 313: 544 tokens\n",
            "📄 Page 315: 454 tokens\n",
            "📄 Page 316: 363 tokens\n",
            "📄 Page 317: 374 tokens\n",
            "📄 Page 318: 579 tokens\n",
            "📄 Page 319: 136 tokens\n",
            "📄 Page 320: 588 tokens\n",
            "📄 Page 321: 388 tokens\n",
            "📄 Page 322: 407 tokens\n",
            "📄 Page 323: 240 tokens\n",
            "📄 Page 324: 181 tokens\n",
            "📄 Page 325: 270 tokens\n",
            "📄 Page 327: 460 tokens\n",
            "📄 Page 328: 685 tokens\n",
            "📄 Page 329: 190 tokens\n",
            "📄 Page 330: 635 tokens\n",
            "📄 Page 331: 470 tokens\n",
            "📄 Page 332: 564 tokens\n",
            "📄 Page 333: 306 tokens\n",
            "📄 Page 334: 520 tokens\n",
            "📄 Page 335: 381 tokens\n",
            "📄 Page 336: 516 tokens\n",
            "📄 Page 337: 232 tokens\n",
            "📄 Page 339: 428 tokens\n",
            "📄 Page 340: 522 tokens\n",
            "📄 Page 341: 263 tokens\n",
            "📄 Page 342: 195 tokens\n",
            "📄 Page 343: 345 tokens\n",
            "📄 Page 344: 348 tokens\n",
            "📄 Page 345: 239 tokens\n",
            "📄 Page 346: 179 tokens\n",
            "📄 Page 347: 387 tokens\n",
            "📄 Page 348: 559 tokens\n",
            "📄 Page 349: 545 tokens\n",
            "📄 Page 350: 380 tokens\n",
            "📄 Page 351: 376 tokens\n",
            "📄 Page 352: 337 tokens\n",
            "📄 Page 353: 316 tokens\n",
            "📄 Page 354: 341 tokens\n",
            "📄 Page 355: 416 tokens\n",
            "📄 Page 356: 319 tokens\n",
            "📄 Page 357: 370 tokens\n",
            "📄 Page 358: 417 tokens\n",
            "📄 Page 359: 551 tokens\n",
            "📄 Page 360: 303 tokens\n",
            "📄 Page 361: 469 tokens\n",
            "📄 Page 362: 542 tokens\n",
            "📄 Page 363: 418 tokens\n",
            "📄 Page 364: 477 tokens\n",
            "📄 Page 365: 703 tokens\n",
            "📄 Page 366: 369 tokens\n",
            "📄 Page 367: 346 tokens\n",
            "📄 Page 368: 226 tokens\n",
            "📄 Page 369: 485 tokens\n",
            "📄 Page 370: 307 tokens\n",
            "📄 Page 371: 405 tokens\n",
            "📄 Page 372: 315 tokens\n",
            "📄 Page 373: 460 tokens\n",
            "📄 Page 374: 523 tokens\n",
            "📄 Page 375: 317 tokens\n",
            "📄 Page 376: 413 tokens\n",
            "📄 Page 377: 479 tokens\n",
            "📄 Page 379: 450 tokens\n",
            "📄 Page 380: 449 tokens\n",
            "📄 Page 381: 435 tokens\n",
            "📄 Page 382: 324 tokens\n",
            "📄 Page 383: 537 tokens\n",
            "📄 Page 384: 737 tokens\n",
            "📄 Page 385: 646 tokens\n",
            "📄 Page 386: 751 tokens\n",
            "📄 Page 387: 751 tokens\n",
            "📄 Page 388: 674 tokens\n",
            "📄 Page 389: 693 tokens\n",
            "📄 Page 390: 646 tokens\n",
            "📄 Page 391: 705 tokens\n",
            "📄 Page 392: 646 tokens\n",
            "📄 Page 393: 662 tokens\n",
            "📄 Page 394: 821 tokens\n",
            "📄 Page 395: 656 tokens\n",
            "📄 Page 396: 304 tokens\n",
            "\n",
            "⏱️ Total time: 118.84 seconds\n",
            "📊 Total tokens: 182925\n",
            "💰 Estimated embedding cost: $0.004390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib, re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Normalize text to avoid minor whitespace/case changes causing new IDs.\"\"\"\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower()\n",
        "\n",
        "def make_chunk_id(course_id: str, chunk_text: str) -> str:\n",
        "    \"\"\"Deterministic ID based on course_id + chunk_text.\"\"\"\n",
        "    norm = normalize_text(chunk_text)\n",
        "    raw = f\"{course_id}|{norm}\"\n",
        "    return hashlib.blake2b(raw.encode(\"utf-8\"), digest_size=16).hexdigest()\n"
      ],
      "metadata": {
        "id": "rUwMCTl4sSMI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opensearchpy.helpers import parallel_bulk\n",
        "index_name = \"test-auqa\"\n",
        "def doc_to_action(doc):\n",
        "    doc_id = make_chunk_id(doc[\"course_id\"], doc[\"chunk_text\"])\n",
        "    return {\n",
        "        \"_op_type\": \"index\",          # overwrite if exists\n",
        "        \"_index\": index_name,\n",
        "        \"_id\": doc_id,\n",
        "        \"_source\": {\n",
        "            \"chunk_text\": doc[\"chunk_text\"],\n",
        "            \"course_id\": doc[\"course_id\"],\n",
        "            \"filename\": doc[\"filename\"],\n",
        "            \"page_no\": doc[\"page_no\"],\n",
        "            \"vector_field\": doc[\"vector_field\"]  # Titan embedding (1024 floats)\n",
        "        }\n",
        "    }\n",
        "\n",
        "actions = (doc_to_action(d) for d in all_embeddings)\n",
        "\n",
        "for ok, result in parallel_bulk(client, actions, thread_count=1, chunk_size=100):\n",
        "    if not ok:\n",
        "        print(\"❌ Failed:\", result)\n",
        "print(\"✅ Bulk upsert finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwKHeDGFsVgo",
        "outputId": "b591f4dc-8ca5-42e2-d1b3-33e59566dd79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bulk upsert finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "def get_titan_embedding(text: str):\n",
        "    body = json.dumps({\"inputText\": text})\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "    return result[\"embedding\"], result.get(\"inputTextTokenCount\", None)\n",
        "\n",
        "query_text = \"Cost-benefit evaluation techniques\"\n",
        "query_vector, token_count = get_titan_embedding(query_text)\n",
        "\n",
        "print(f\"✅ Query embedded: {len(query_vector)}-dim vector | Tokens: {token_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWNe5hit_6U",
        "outputId": "ce299fb8-8e7d-4bf5-9489-5dc2b9bd099e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Query embedded: 1024-dim vector | Tokens: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 20\n",
        "BM25_WEIGHT = 0.3\n",
        "VECTOR_WEIGHT = 0.7\n",
        "\n",
        "# --- Run BM25 ---\n",
        "bm25_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\"size\": TOP_N, \"query\": {\"match\": {\"chunk_text\": query_text}}}\n",
        ")\n",
        "bm25_hits = bm25_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Run Vector ---\n",
        "vector_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\n",
        "        \"size\": TOP_N,\n",
        "        \"query\": {\n",
        "            \"knn\": {\"vector_field\": {\"vector\": query_vector, \"k\": TOP_N}}\n",
        "        }\n",
        "    }\n",
        ")\n",
        "vector_hits = vector_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Normalize scores ---\n",
        "def normalize_scores(hits):\n",
        "    scores = [h[\"_score\"] for h in hits]\n",
        "    if not scores:\n",
        "        return {}\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if min_s == max_s:\n",
        "        return {h[\"_id\"]: 1.0 for h in hits}\n",
        "    return {h[\"_id\"]: (h[\"_score\"] - min_s) / (max_s - min_s) for h in hits}\n",
        "\n",
        "bm25_norm = normalize_scores(bm25_hits)\n",
        "vector_norm = normalize_scores(vector_hits)\n",
        "\n",
        "# --- Combine ---\n",
        "combined = {}\n",
        "for h in bm25_hits + vector_hits:\n",
        "    _id = h[\"_id\"]\n",
        "    src = h[\"_source\"]\n",
        "    bm25_s = bm25_norm.get(_id, 0.0)\n",
        "    vec_s = vector_norm.get(_id, 0.0)\n",
        "    hybrid_score = BM25_WEIGHT * bm25_s + VECTOR_WEIGHT * vec_s\n",
        "    combined[_id] = {\n",
        "        \"hybrid_score\": hybrid_score,\n",
        "        \"bm25_score\": bm25_s,\n",
        "        \"vector_score\": vec_s,\n",
        "        \"source\": src\n",
        "    }\n",
        "\n",
        "results = sorted(combined.values(), key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
        "\n",
        "# --- Print top 5 results ---\n",
        "print(\"\\n🔎 Manual Hybrid Results:\")\n",
        "for r in results[:]:\n",
        "    src = r[\"source\"]\n",
        "    print(f\"Hybrid={r['hybrid_score']:.3f} | BM25={r['bm25_score']:.3f} | Vec={r['vector_score']:.3f}\")\n",
        "    print(f\"Page={src['page_no']} | Course={src['course_id']} | File={src['filename']}\")\n",
        "    print(src[\"chunk_text\"][:10], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hecRPJZwHnp",
        "outputId": "7973c1e8-1f1e-4bf0-9d9f-22ecfe792298"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Manual Hybrid Results:\n",
            "Hybrid=0.919 | BM25=0.729 | Vec=1.000\n",
            "Page=55 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.734 | BM25=0.580 | Vec=0.801\n",
            "Page=53 | Course=CS6022 | File=SPM.pdf\n",
            "3.4 COST-B ...\n",
            "\n",
            "Hybrid=0.557 | BM25=0.312 | Vec=0.663\n",
            "Page=57 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.522 | BM25=0.681 | Vec=0.454\n",
            "Page=61 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.467 | BM25=0.267 | Vec=0.552\n",
            "Page=59 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.466 | BM25=1.000 | Vec=0.237\n",
            "Page=7 | Course=CS6022 | File=SPM.pdf\n",
            "viii\n",
            "CONTE ...\n",
            "\n",
            "Hybrid=0.442 | BM25=0.529 | Vec=0.405\n",
            "Page=52 | Course=CS6022 | File=SPM.pdf\n",
            "40\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.410 | BM25=0.521 | Vec=0.363\n",
            "Page=63 | Course=CS6022 | File=SPM.pdf\n",
            "3.7 RISK E ...\n",
            "\n",
            "Hybrid=0.347 | BM25=0.803 | Vec=0.152\n",
            "Page=49 | Course=CS6022 | File=SPM.pdf\n",
            "Chapter 3\n",
            " ...\n",
            "\n",
            "Hybrid=0.277 | BM25=0.923 | Vec=0.000\n",
            "Page=155 | Course=CS6022 | File=SPM.pdf\n",
            "7.7 EVALUA ...\n",
            "\n",
            "Hybrid=0.274 | BM25=0.505 | Vec=0.175\n",
            "Page=67 | Course=CS6022 | File=SPM.pdf\n",
            "3.9 FURTHE ...\n",
            "\n",
            "Hybrid=0.215 | BM25=0.000 | Vec=0.306\n",
            "Page=56 | Course=CS6022 | File=SPM.pdf\n",
            "44\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.188 | BM25=0.627 | Vec=0.000\n",
            "Page=385 | Course=CS6022 | File=SPM.pdf\n",
            "INDEX\n",
            "373\n",
            " ...\n",
            "\n",
            "Hybrid=0.174 | BM25=0.581 | Vec=0.000\n",
            "Page=148 | Course=CS6022 | File=SPM.pdf\n",
            "136\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.163 | BM25=0.279 | Vec=0.113\n",
            "Page=62 | Course=CS6022 | File=SPM.pdf\n",
            "50\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.145 | BM25=0.000 | Vec=0.208\n",
            "Page=212 | Course=CS6022 | File=SPM.pdf\n",
            "200\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.144 | BM25=0.000 | Vec=0.206\n",
            "Page=64 | Course=CS6022 | File=SPM.pdf\n",
            "52\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.092 | BM25=0.306 | Vec=0.000\n",
            "Page=393 | Course=CS6022 | File=SPM.pdf\n",
            "INDEX\n",
            "381\n",
            " ...\n",
            "\n",
            "Hybrid=0.088 | BM25=0.295 | Vec=0.000\n",
            "Page=349 | Course=CS6022 | File=SPM.pdf\n",
            "CHAPTER 3\n",
            " ...\n",
            "\n",
            "Hybrid=0.081 | BM25=0.270 | Vec=0.000\n",
            "Page=347 | Course=CS6022 | File=SPM.pdf\n",
            "CHAPTER 3\n",
            " ...\n",
            "\n",
            "Hybrid=0.062 | BM25=0.000 | Vec=0.089\n",
            "Page=54 | Course=CS6022 | File=SPM.pdf\n",
            "42\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.058 | BM25=0.000 | Vec=0.083\n",
            "Page=214 | Course=CS6022 | File=SPM.pdf\n",
            "202\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.057 | BM25=0.000 | Vec=0.081\n",
            "Page=192 | Course=CS6022 | File=SPM.pdf\n",
            "180\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.051 | BM25=0.000 | Vec=0.073\n",
            "Page=193 | Course=CS6022 | File=SPM.pdf\n",
            "9.6 EARNED ...\n",
            "\n",
            "Hybrid=0.023 | BM25=0.076 | Vec=0.000\n",
            "Page=151 | Course=CS6022 | File=SPM.pdf\n",
            "7.5 RISK A ...\n",
            "\n",
            "Hybrid=0.014 | BM25=0.045 | Vec=0.000\n",
            "Page=305 | Course=CS6022 | File=SPM.pdf\n",
            "C.5 ACQUIS ...\n",
            "\n",
            "Hybrid=0.003 | BM25=0.000 | Vec=0.004\n",
            "Page=58 | Course=CS6022 | File=SPM.pdf\n",
            "46\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.000 | BM25=0.000 | Vec=0.000\n",
            "Page=84 | Course=CS6022 | File=SPM.pdf\n",
            "72\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.000 | BM25=0.000 | Vec=0.000\n",
            "Page=66 | Course=CS6022 | File=SPM.pdf\n",
            "54\n",
            "CHAPTER ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect top 5 page chunks into one string\n",
        "context_text = \"\\n\\n\".join([r[\"source\"][\"chunk_text\"] for r in results[:]])\n",
        "ques_query = query_text   # the query you used\n",
        "no = 20\n"
      ],
      "metadata": {
        "id": "PiJhquiAyjO2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an AI assistant specialized in **question generation and analysis**.\n",
        "\n",
        "Your goals:\n",
        "1. Generate insightful and generic questions based on the given context and user query.\n",
        "   - Do not copy sentences from the context.\n",
        "   - Make questions broad and meaningful, not text-specific.\n",
        "   - Generate at least **{no} questions**.\n",
        "2. For each generated question, classify it by:\n",
        "   - **Difficulty level**:\n",
        "       * Easy → simple recall/basic understanding (1-sentence answers).\n",
        "       * Medium → requires moderate understanding, application, or 2–3 steps of reasoning.\n",
        "       * Hard → requires deep understanding, critical thinking, multi-step reasoning.\n",
        "   - **Bloom's taxonomy level**:\n",
        "       * Choose one of: Remember, Understand, Apply, Analyze, Evaluate, Create.\n",
        "\n",
        "### Input\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "User Query (Focus Topic):\n",
        "{ques_query}\n",
        "\n",
        "### Output\n",
        "Return the results in **valid JSON** format, as a list of objects.\n",
        "Each object must look like this:\n",
        "{{\n",
        "  \"question\": \"...\",\n",
        "  \"difficulty\": \"...\",\n",
        "  \"blooms_level\": \"...\"\n",
        "}}\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8M7L4Eiryjio"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, json, time\n",
        "\n",
        "REGION = \"ap-south-1\"\n",
        "PROFILE_ARN = \"arn:aws:bedrock:ap-south-1:850146468080:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "body = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 500,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "start = time.time()\n",
        "resp = bedrock_rt.invoke_model(modelId=PROFILE_ARN, body=json.dumps(body))\n",
        "elapsed = time.time() - start\n",
        "\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "\n",
        "reply = output[\"content\"][0][\"text\"]\n",
        "\n",
        "# Usage metadata (if present)\n",
        "usage = output.get(\"usage\", {})\n",
        "input_tokens = usage.get(\"input_tokens\", \"N/A\")\n",
        "output_tokens = usage.get(\"output_tokens\", \"N/A\")\n",
        "\n",
        "print(\"✅ Generated Output:\\n\")\n",
        "print(reply)\n",
        "\n",
        "print(\"\\n📊 Stats:\")\n",
        "print(\"  Input tokens :\", input_tokens)\n",
        "print(\"  Output tokens:\", output_tokens)\n",
        "print(f\"  Time taken   : {elapsed:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PimWoLJNyjlt",
        "outputId": "ad6b073d-9d6e-428b-fb30-d9b1f9f06beb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generated Output:\n",
            "\n",
            "{\n",
            "  \"questions\": [\n",
            "    {\n",
            "      \"question\": \"What are the key differences between net profit and net present value as project evaluation methods?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\" \n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How does the timing of cash flows impact the financial evaluation of a project?\",\n",
            "      \"difficulty\": \"Medium\", \n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the advantages and disadvantages of using return on investment (ROI) as a project evaluation technique?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Evaluate\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why is it important to consider both the size of investment and timing of returns when evaluating projects?\",\n",
            "      \"difficulty\": \"Easy\",\n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can risk factors be incorporated into the financial evaluation of a project?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Apply\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What role does the discount rate play in net present value calculations and how is it determined?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why might projects with the same net profit have different net present values?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the key limitations of using payback period as a project selection criterion?\",\n",
            "      \"difficulty\": \"Easy\",\n",
            "      \"blooms_level\": \"Evaluate\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can decision trees help in evaluating projects with multiple possible outcomes?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Apply\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What factors should be considered when choosing between competing project proposals?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Evaluate\"\n",
            "    },\n",
            "\n",
            "📊 Stats:\n",
            "  Input tokens : 16627\n",
            "  Output tokens: 500\n",
            "  Time taken   : 12.40 seconds\n"
          ]
        }
      ]
    }
  ]
}