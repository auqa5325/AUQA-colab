{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoU7CkAijvFRK+0f+fWCE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auqa5325/AUQA-colab/blob/main/Copy_of_AUQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-uDEQ4a38AH",
        "outputId": "0dd9ea0e-b415-4b68-88c3-c24e64d2276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install boto3 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY_ID     = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "#AWS_SESSION_TOKEN     = userdata.get(\"AWS_SESSION_TOKEN\")  # may be None\n",
        "AWS_REGION = userdata.get(\"AWS_REGION\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
        "\n",
        "print(\"✅ Credentials set. Region:\", AWS_REGION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8aVrSE4ZiZ",
        "outputId": "bc768233-e5be-4b5c-e292-b3ca100c78e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Credentials set. Region: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]\n",
        "session = boto3.Session(region_name=REGION)\n",
        "bedrock         = session.client(\"bedrock\")\n",
        "bedrock_runtime = session.client(\"bedrock-runtime\")\n",
        "print(\"✅ boto3 session initialized in:\", session.region_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqqWGkX4-QA",
        "outputId": "97938a45-4fdd-4934-fd4b-0442e7f0e2b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ boto3 session initialized in: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "s = boto3.Session(region_name=\"ap-south-1\")\n",
        "print(s.client(\"sts\").get_caller_identity())   # must print Account, Arn\n",
        "creds = s.get_credentials().get_frozen_credentials()\n",
        "print(\"AccessKey:\", creds.access_key[:4], \"HasToken:\", bool(creds.token))\n"
      ],
      "metadata": {
        "id": "UT2f-KtPS8C0",
        "outputId": "50ec2908-7828-47b9-9e96-004a73232aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY1NDg4MzQxNDA6Ujp3Y0FQd1kzUA==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Sat, 30 Aug 2025 10:13:54 GMT'}, 'RetryAttempts': 0}}\n",
            "AccessKey: AKIA HasToken: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output dict structure :\n",
        "* id → unique identifier for this response (good for\n",
        "logging/debugging).\n",
        "\n",
        "* type → \"message\" → tells you this is a message object.\n",
        "\n",
        "* role → \"assistant\" → the speaker role (assistant vs. user).\n",
        "\n",
        "* model → which model gave this reply (claude-3-sonnet-20240229).\n",
        "\n",
        "* content → a list of parts that make up the response.\n",
        "\n",
        "* stop_reason → why the model stopped (e.g., end_turn, max_tokens).\n",
        "\n",
        "* stop_sequence → custom sequence that stopped generation (here it’s None).\n",
        "\n",
        "* usage → token usage info (handy for cost + rate limits)."
      ],
      "metadata": {
        "id": "wbqD05a1_0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "prompt = \"explain tcp ip?\"\n",
        "\n",
        "body = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 100,\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "print(output[\"outputs\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "xKuzvoVdBegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "session = boto3.Session(region_name=\"ap-south-1\")\n",
        "sts = session.client(\"sts\")\n",
        "print(sts.get_caller_identity())  # should return Account, Arn, UserId\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5nKJfpQRoqH",
        "outputId": "6c4b9df1-8ef1-42e2-acc7-d3e67a45b3d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY0NTkzNTE3NDk6UjpJd1BrYlNOVg==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Fri, 29 Aug 2025 09:22:31 GMT'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'outputs': [{'text': '\\n\\nTCP/IP is a suite of protocols that defines the Internet. Originally designed for the UNIX operating system. The TCP/IP protocol suite is a four-layer model that provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed, and received.\\n\\nThe four layers of the TCP/IP model are:\\n\\n1. The Application Layer: This is the topmost layer of the TCP/IP', 'stop_reason': 'length'}]}"
      ],
      "metadata": {
        "id": "zlZJ3onqB_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opensearch-py requests-aws4auth\n",
        "\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "from requests_aws4auth import AWS4Auth\n",
        "import boto3\n",
        "\n",
        "# --- CONFIG ---\n",
        "host   = \"https://search-test1-annauniv-pcx3f52wxykhpd4md6v4bjeqdy.ap-south-1.es.amazonaws.com\"  # OpenSearch domain endpoint\n",
        "region = \"ap-south-1\"\n",
        "service = \"es\"\n",
        "\n",
        "# --- AWS SigV4 Auth ---\n",
        "session = boto3.Session()\n",
        "credentials = session.get_credentials()\n",
        "awsauth = AWS4Auth(\n",
        "    credentials.access_key,\n",
        "    credentials.secret_key,\n",
        "    region,\n",
        "    service,\n",
        "    session_token=credentials.token\n",
        ")\n",
        "\n",
        "# --- OpenSearch Client ---\n",
        "client = OpenSearch(\n",
        "    hosts=[host],\n",
        "    http_auth=awsauth,\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    connection_class=RequestsHttpConnection\n",
        ")\n",
        "\n",
        "# --- TEST: Get cluster info ---\n",
        "print(client.info())\n"
      ],
      "metadata": {
        "id": "l5z2sIp87q1F",
        "outputId": "fd28194c-422f-4221-b070-f9d0bb23b28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/371.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m368.6/371.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.5/371.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h{'name': 'd69eb8cacc6e2cf42750a5f4788db327', 'cluster_name': '850146468080:test1-annauniv', 'cluster_uuid': 'qhkHo9X9Suuw8REB9uInGQ', 'version': {'distribution': 'opensearch', 'number': '2.19.0', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-07-24T06:15:41.026838036Z', 'build_snapshot': False, 'lucene_version': '9.12.1', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"test-auqa\"\n",
        "\n",
        "index_body = {\n",
        "    \"settings\": {\n",
        "        \"index\": {\n",
        "            \"knn\": True,\n",
        "            \"knn.algo_param.ef_search\": 512,\n",
        "            \"number_of_shards\": 5,\n",
        "            \"number_of_replicas\": 1\n",
        "        }\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            # main chunk text\n",
        "            \"chunk_text\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # ids & names\n",
        "            \"course_id\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"filename\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # page number\n",
        "            \"page_no\": { \"type\": \"long\" },\n",
        "\n",
        "\n",
        "\n",
        "            # vector index used for ANN similarity search\n",
        "            \"vector_field\": {\n",
        "                \"type\": \"knn_vector\",\n",
        "                \"dimension\": 1024,\n",
        "                \"method\": {\n",
        "                    \"name\": \"hnsw\",\n",
        "                    \"space_type\": \"cosinesimil\",\n",
        "                    \"engine\": \"nmslib\",\n",
        "                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# recreate index\n",
        "if client.indices.exists(index=index_name):\n",
        "    print(f\"Index '{index_name}' already exists. Deleting and recreating...\")\n",
        "    client.indices.delete(index=index_name)\n",
        "\n",
        "client.indices.create(index=index_name, body=index_body)\n",
        "print(f\"✅ Index '{index_name}' created successfully!\")\n"
      ],
      "metadata": {
        "id": "UcTIfVU4RS02",
        "outputId": "0bfab4b4-84cc-4b99-9255-2b7123324d65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'test-auqa' already exists. Deleting and recreating...\n",
            "✅ Index 'test-auqa' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'test-annauniv': {'aliases': {}, 'mappings': {'properties': {'course_id': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'embedding': {'type': 'float'}, 'metadata': {'properties': {'page': {'type': 'long'}, 'source': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'page_no': {'type': 'long'}, 'text': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'vector_field': {'type': 'knn_vector', 'dimension': 1024, 'method': {'engine': 'nmslib', 'space_type': 'cosinesimil', 'name': 'hnsw', 'parameters': {'ef_construction': 512, 'm': 16}}}}}, 'settings': {'index': {'replication': {'type': 'DOCUMENT'}, 'refresh_interval': '1s', 'number_of_shards': '5', 'knn.algo_param': {'ef_search': '512'}, 'provided_name': 'test-annauniv', 'knn': 'true', 'creation_date': '1756144596121', 'number_of_replicas': '1', 'uuid': 'rumasDZaQC6xBy4G48v1KQ', 'version': {'created': '136407827'}}}}}\n",
        "\n"
      ],
      "metadata": {
        "id": "n-xX1sT3yxqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, time, re\n",
        "\n",
        "BUCKET = \"anna-univ-qna\"\n",
        "DOCUMENT = \"Textbooks/SPM.pdf\"\n",
        "REGION = \"ap-south-1\"\n",
        "COURSE_ID = \"CS6022\"   # <-- set this per textbook\n",
        "FILENAME = \"SPM.pdf\"\n",
        "\n",
        "textract = boto3.client(\"textract\", region_name=REGION)\n",
        "\n",
        "# --- Start async Textract job ---\n",
        "start = time.time()\n",
        "response = textract.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": BUCKET, \"Name\": DOCUMENT}}\n",
        ")\n",
        "print(\"FILENAME =\",FILENAME)\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"✅ Started Textract JobId: {job_id}\")\n",
        "\n",
        "# --- Wait for completion ---\n",
        "while True:\n",
        "    result = textract.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"❌ Textract job failed!\")\n",
        "\n",
        "pages = result[\"DocumentMetadata\"][\"Pages\"]\n",
        "print(f\"📄 Pages: {pages}\")\n",
        "print(f\"⏱️ Time taken: {elapsed:.2f} sec\")\n",
        "print(f\"💰 Estimated cost: ${(pages/1000)*1.5:.4f}\")\n",
        "\n",
        "# --- Collect page-wise text into docs with metadata ---\n",
        "page_texts = {}\n",
        "next_token = None\n",
        "\n",
        "while True:\n",
        "    if next_token:\n",
        "        result = textract.get_document_text_detection(JobId=job_id, NextToken=next_token)\n",
        "    else:\n",
        "        result = textract.get_document_text_detection(JobId=job_id)\n",
        "\n",
        "    for block in result[\"Blocks\"]:\n",
        "        if block[\"BlockType\"] == \"LINE\":\n",
        "            page_no = block[\"Page\"]\n",
        "            page_texts.setdefault(page_no, []).append(block[\"Text\"])\n",
        "\n",
        "    next_token = result.get(\"NextToken\")\n",
        "    if not next_token:\n",
        "        break\n",
        "\n",
        "# Build document objects (ready to send to Titan for embeddings)\n",
        "docs = []\n",
        "for page_no, lines in page_texts.items():\n",
        "    chunk_text = \"\\n\".join(lines)\n",
        "    doc = {\n",
        "        \"chunk_text\": chunk_text,\n",
        "        \"course_id\": COURSE_ID,\n",
        "        \"filename\": FILENAME,\n",
        "        \"page_no\": page_no\n",
        "    }\n",
        "    docs.append(doc)\n",
        "\n",
        "# --- Preview first 5 pages ---\n",
        "print(\"\\n📑 First 5 pages extracted:\")\n",
        "for d in docs[:5]:\n",
        "    print(f\"\\nPage {d['page_no']} | Course: {d['course_id']} | File: {d['filename']}\")\n",
        "    print(d[\"chunk_text\"][:400], \"...\")\n"
      ],
      "metadata": {
        "id": "DVaNNUjgqWN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "806b9018-75df-421c-b8ef-aeea73a65c2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FILENAME = SPM.pdf\n",
            "✅ Started Textract JobId: ce41d655412d458abda1771970d243aee8b1fcb47f07c25ecdfed15620863448\n",
            "📄 Pages: 396\n",
            "⏱️ Time taken: 231.58 sec\n",
            "💰 Estimated cost: $0.5940\n",
            "\n",
            "📑 First 5 pages extracted:\n",
            "\n",
            "Page 1 | Course: CS6022 | File: SPM.pdf\n",
            "BoB HUGHES AND MIKE COTTERELL\n",
            "Software\n",
            "Project\n",
            "Management Second Edition\n",
            "Tistimating\n",
            "www.mcgraw-hill.co.uk/hughes ...\n",
            "\n",
            "Page 2 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition) ...\n",
            "\n",
            "Page 3 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition)\n",
            "Bob Hughes and Mike Cotterell,\n",
            "School of Information Management, University of Brighton\n",
            "The McGraw-Hill Companies\n",
            "London\n",
            "Burr Ridge, IL\n",
            "New York\n",
            "St Louis\n",
            "San Francisco\n",
            "Auckland\n",
            "Bogotá Caracas\n",
            "Lisbon\n",
            "Madrid\n",
            "Mexico\n",
            "Milan\n",
            "Montreal\n",
            "New Delhi\n",
            "Panama\n",
            "Paris\n",
            "San Juan\n",
            "São Paulo\n",
            "Singapore\n",
            "Tokyo\n",
            "Toronto ...\n",
            "\n",
            "Page 4 | Course: CS6022 | File: SPM.pdf\n",
            "Published by\n",
            "McGraw-Hill Publishing Company\n",
            "SHOPPENHANGERS ROAD, MAIDENHEAD, BERKSHIRE, SL6 2QL, ENGLAND\n",
            "Telephone: +44(o) 1628 502500\n",
            "Fax: +44(o) 1628 770224\n",
            "Web site: http://www.megraw-hill.co.uk\n",
            "British Library Cataloguing in Publication Data\n",
            "A catalogue record for this book is available from the British Library\n",
            "ISBN 007 709505 7\n",
            "Library of Congress cataloguing in publication data\n",
            "The LOC data  ...\n",
            "\n",
            "Page 5 | Course: CS6022 | File: SPM.pdf\n",
            "The road to hell is paved with works-in-progress.\n",
            "Philip Roth ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, json, time\n",
        "\n",
        "# --- Config ---\n",
        "REGION = \"ap-south-1\"\n",
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "PRICE_PER_1K = 0.000024  # USD per 1k tokens\n",
        "\n",
        "# Assume docs is already built from Textract step\n",
        "# docs = [\n",
        "#   {\"chunk_text\": \"...\", \"course_id\":\"CS101\", \"filename\":\"SPM.pdf\", \"page_no\":1},\n",
        "#   ...\n",
        "# ]\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "all_embeddings = []\n",
        "total_tokens = 0\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for d in docs:\n",
        "    body = json.dumps({\n",
        "        \"inputText\": d[\"chunk_text\"]\n",
        "    })\n",
        "\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "\n",
        "    # Titan Embedding returns: { \"embedding\": [...], \"inputTextTokenCount\": N }\n",
        "    vector = result[\"embedding\"]\n",
        "    tokens = result.get(\"inputTextTokenCount\", len(d[\"chunk_text\"].split()))  # fallback\n",
        "\n",
        "    d[\"vector_field\"] = vector\n",
        "    d[\"tokens\"] = tokens\n",
        "\n",
        "    all_embeddings.append(d)\n",
        "\n",
        "    total_tokens += tokens\n",
        "    print(f\"📄 Page {d['page_no']}: {tokens} tokens\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "cost = (total_tokens / 1000) * PRICE_PER_1K\n",
        "\n",
        "print(\"\\n⏱️ Total time:\", round(elapsed, 2), \"seconds\")\n",
        "print(\"📊 Total tokens:\", total_tokens)\n",
        "print(f\"💰 Estimated embedding cost: ${cost:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f_xnQuxafnwr",
        "outputId": "419eb821-d9c8-46a0-e861-eadaf9e012da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Page 1: 37 tokens\n",
            "📄 Page 2: 9 tokens\n",
            "📄 Page 3: 96 tokens\n",
            "📄 Page 4: 345 tokens\n",
            "📄 Page 5: 14 tokens\n",
            "📄 Page 6: 393 tokens\n",
            "📄 Page 7: 424 tokens\n",
            "📄 Page 8: 393 tokens\n",
            "📄 Page 9: 393 tokens\n",
            "📄 Page 10: 253 tokens\n",
            "📄 Page 11: 517 tokens\n",
            "📄 Page 12: 482 tokens\n",
            "📄 Page 13: 235 tokens\n",
            "📄 Page 14: 460 tokens\n",
            "📄 Page 15: 551 tokens\n",
            "📄 Page 16: 373 tokens\n",
            "📄 Page 17: 575 tokens\n",
            "📄 Page 18: 463 tokens\n",
            "📄 Page 19: 400 tokens\n",
            "📄 Page 20: 529 tokens\n",
            "📄 Page 21: 457 tokens\n",
            "📄 Page 22: 410 tokens\n",
            "📄 Page 23: 645 tokens\n",
            "📄 Page 24: 322 tokens\n",
            "📄 Page 25: 558 tokens\n",
            "📄 Page 26: 550 tokens\n",
            "📄 Page 27: 411 tokens\n",
            "📄 Page 28: 588 tokens\n",
            "📄 Page 29: 457 tokens\n",
            "📄 Page 30: 558 tokens\n",
            "📄 Page 31: 365 tokens\n",
            "📄 Page 32: 603 tokens\n",
            "📄 Page 33: 403 tokens\n",
            "📄 Page 34: 273 tokens\n",
            "📄 Page 35: 515 tokens\n",
            "📄 Page 36: 542 tokens\n",
            "📄 Page 37: 588 tokens\n",
            "📄 Page 38: 553 tokens\n",
            "📄 Page 39: 520 tokens\n",
            "📄 Page 40: 540 tokens\n",
            "📄 Page 41: 276 tokens\n",
            "📄 Page 42: 443 tokens\n",
            "📄 Page 43: 369 tokens\n",
            "📄 Page 44: 600 tokens\n",
            "📄 Page 45: 532 tokens\n",
            "📄 Page 46: 529 tokens\n",
            "📄 Page 47: 435 tokens\n",
            "📄 Page 48: 530 tokens\n",
            "📄 Page 49: 328 tokens\n",
            "📄 Page 50: 339 tokens\n",
            "📄 Page 51: 507 tokens\n",
            "📄 Page 52: 577 tokens\n",
            "📄 Page 53: 568 tokens\n",
            "📄 Page 54: 619 tokens\n",
            "📄 Page 55: 590 tokens\n",
            "📄 Page 56: 425 tokens\n",
            "📄 Page 57: 731 tokens\n",
            "📄 Page 58: 750 tokens\n",
            "📄 Page 59: 579 tokens\n",
            "📄 Page 60: 701 tokens\n",
            "📄 Page 61: 712 tokens\n",
            "📄 Page 62: 622 tokens\n",
            "📄 Page 63: 553 tokens\n",
            "📄 Page 64: 617 tokens\n",
            "📄 Page 65: 578 tokens\n",
            "📄 Page 66: 315 tokens\n",
            "📄 Page 67: 350 tokens\n",
            "📄 Page 69: 319 tokens\n",
            "📄 Page 70: 269 tokens\n",
            "📄 Page 71: 518 tokens\n",
            "📄 Page 72: 529 tokens\n",
            "📄 Page 73: 579 tokens\n",
            "📄 Page 74: 597 tokens\n",
            "📄 Page 75: 402 tokens\n",
            "📄 Page 76: 598 tokens\n",
            "📄 Page 77: 372 tokens\n",
            "📄 Page 78: 422 tokens\n",
            "📄 Page 79: 559 tokens\n",
            "📄 Page 80: 334 tokens\n",
            "📄 Page 81: 513 tokens\n",
            "📄 Page 82: 515 tokens\n",
            "📄 Page 83: 481 tokens\n",
            "📄 Page 84: 535 tokens\n",
            "📄 Page 85: 329 tokens\n",
            "📄 Page 86: 468 tokens\n",
            "📄 Page 87: 527 tokens\n",
            "📄 Page 88: 506 tokens\n",
            "📄 Page 89: 470 tokens\n",
            "📄 Page 90: 108 tokens\n",
            "📄 Page 91: 328 tokens\n",
            "📄 Page 92: 729 tokens\n",
            "📄 Page 93: 627 tokens\n",
            "📄 Page 94: 708 tokens\n",
            "📄 Page 95: 415 tokens\n",
            "📄 Page 96: 700 tokens\n",
            "📄 Page 97: 565 tokens\n",
            "📄 Page 98: 604 tokens\n",
            "📄 Page 99: 607 tokens\n",
            "📄 Page 100: 625 tokens\n",
            "📄 Page 101: 664 tokens\n",
            "📄 Page 102: 488 tokens\n",
            "📄 Page 103: 546 tokens\n",
            "📄 Page 104: 550 tokens\n",
            "📄 Page 105: 379 tokens\n",
            "📄 Page 106: 525 tokens\n",
            "📄 Page 107: 451 tokens\n",
            "📄 Page 108: 482 tokens\n",
            "📄 Page 109: 624 tokens\n",
            "📄 Page 110: 560 tokens\n",
            "📄 Page 111: 430 tokens\n",
            "📄 Page 112: 625 tokens\n",
            "📄 Page 113: 557 tokens\n",
            "📄 Page 114: 501 tokens\n",
            "📄 Page 115: 273 tokens\n",
            "📄 Page 116: 426 tokens\n",
            "📄 Page 117: 80 tokens\n",
            "📄 Page 119: 301 tokens\n",
            "📄 Page 120: 571 tokens\n",
            "📄 Page 121: 676 tokens\n",
            "📄 Page 122: 312 tokens\n",
            "📄 Page 123: 568 tokens\n",
            "📄 Page 124: 507 tokens\n",
            "📄 Page 125: 654 tokens\n",
            "📄 Page 126: 488 tokens\n",
            "📄 Page 127: 475 tokens\n",
            "📄 Page 128: 604 tokens\n",
            "📄 Page 129: 410 tokens\n",
            "📄 Page 130: 603 tokens\n",
            "📄 Page 131: 498 tokens\n",
            "📄 Page 132: 186 tokens\n",
            "📄 Page 133: 392 tokens\n",
            "📄 Page 134: 445 tokens\n",
            "📄 Page 135: 490 tokens\n",
            "📄 Page 136: 544 tokens\n",
            "📄 Page 137: 436 tokens\n",
            "📄 Page 138: 500 tokens\n",
            "📄 Page 139: 518 tokens\n",
            "📄 Page 140: 552 tokens\n",
            "📄 Page 141: 600 tokens\n",
            "📄 Page 142: 661 tokens\n",
            "📄 Page 143: 425 tokens\n",
            "📄 Page 144: 166 tokens\n",
            "📄 Page 145: 334 tokens\n",
            "📄 Page 146: 282 tokens\n",
            "📄 Page 147: 546 tokens\n",
            "📄 Page 148: 495 tokens\n",
            "📄 Page 149: 624 tokens\n",
            "📄 Page 150: 580 tokens\n",
            "📄 Page 151: 642 tokens\n",
            "📄 Page 152: 399 tokens\n",
            "📄 Page 153: 674 tokens\n",
            "📄 Page 154: 487 tokens\n",
            "📄 Page 155: 482 tokens\n",
            "📄 Page 156: 538 tokens\n",
            "📄 Page 157: 402 tokens\n",
            "📄 Page 158: 513 tokens\n",
            "📄 Page 159: 541 tokens\n",
            "📄 Page 160: 477 tokens\n",
            "📄 Page 161: 423 tokens\n",
            "📄 Page 162: 340 tokens\n",
            "📄 Page 163: 354 tokens\n",
            "📄 Page 164: 298 tokens\n",
            "📄 Page 165: 533 tokens\n",
            "📄 Page 166: 789 tokens\n",
            "📄 Page 167: 567 tokens\n",
            "📄 Page 168: 331 tokens\n",
            "📄 Page 169: 460 tokens\n",
            "📄 Page 170: 564 tokens\n",
            "📄 Page 171: 858 tokens\n",
            "📄 Page 172: 271 tokens\n",
            "📄 Page 173: 553 tokens\n",
            "📄 Page 174: 591 tokens\n",
            "📄 Page 175: 141 tokens\n",
            "📄 Page 176: 777 tokens\n",
            "📄 Page 177: 472 tokens\n",
            "📄 Page 178: 320 tokens\n",
            "📄 Page 179: 257 tokens\n",
            "📄 Page 180: 237 tokens\n",
            "📄 Page 181: 260 tokens\n",
            "📄 Page 182: 304 tokens\n",
            "📄 Page 183: 425 tokens\n",
            "📄 Page 184: 429 tokens\n",
            "📄 Page 185: 559 tokens\n",
            "📄 Page 186: 471 tokens\n",
            "📄 Page 187: 512 tokens\n",
            "📄 Page 188: 510 tokens\n",
            "📄 Page 189: 559 tokens\n",
            "📄 Page 190: 525 tokens\n",
            "📄 Page 191: 370 tokens\n",
            "📄 Page 192: 456 tokens\n",
            "📄 Page 193: 372 tokens\n",
            "📄 Page 194: 484 tokens\n",
            "📄 Page 195: 249 tokens\n",
            "📄 Page 196: 485 tokens\n",
            "📄 Page 197: 474 tokens\n",
            "📄 Page 198: 602 tokens\n",
            "📄 Page 199: 679 tokens\n",
            "📄 Page 200: 468 tokens\n",
            "📄 Page 201: 382 tokens\n",
            "📄 Page 202: 465 tokens\n",
            "📄 Page 203: 299 tokens\n",
            "📄 Page 204: 788 tokens\n",
            "📄 Page 205: 526 tokens\n",
            "📄 Page 206: 485 tokens\n",
            "📄 Page 207: 614 tokens\n",
            "📄 Page 208: 561 tokens\n",
            "📄 Page 209: 601 tokens\n",
            "📄 Page 210: 582 tokens\n",
            "📄 Page 211: 526 tokens\n",
            "📄 Page 212: 728 tokens\n",
            "📄 Page 213: 662 tokens\n",
            "📄 Page 214: 567 tokens\n",
            "📄 Page 215: 478 tokens\n",
            "📄 Page 216: 447 tokens\n",
            "📄 Page 217: 537 tokens\n",
            "📄 Page 218: 594 tokens\n",
            "📄 Page 219: 671 tokens\n",
            "📄 Page 220: 479 tokens\n",
            "📄 Page 221: 347 tokens\n",
            "📄 Page 223: 242 tokens\n",
            "📄 Page 224: 263 tokens\n",
            "📄 Page 225: 621 tokens\n",
            "📄 Page 226: 572 tokens\n",
            "📄 Page 227: 804 tokens\n",
            "📄 Page 228: 558 tokens\n",
            "📄 Page 229: 578 tokens\n",
            "📄 Page 230: 654 tokens\n",
            "📄 Page 231: 464 tokens\n",
            "📄 Page 232: 562 tokens\n",
            "📄 Page 233: 490 tokens\n",
            "📄 Page 234: 622 tokens\n",
            "📄 Page 235: 513 tokens\n",
            "📄 Page 236: 589 tokens\n",
            "📄 Page 237: 607 tokens\n",
            "📄 Page 238: 440 tokens\n",
            "📄 Page 239: 446 tokens\n",
            "📄 Page 240: 579 tokens\n",
            "📄 Page 241: 470 tokens\n",
            "📄 Page 242: 607 tokens\n",
            "📄 Page 243: 648 tokens\n",
            "📄 Page 244: 423 tokens\n",
            "📄 Page 245: 296 tokens\n",
            "📄 Page 247: 331 tokens\n",
            "📄 Page 248: 266 tokens\n",
            "📄 Page 249: 552 tokens\n",
            "📄 Page 250: 496 tokens\n",
            "📄 Page 251: 385 tokens\n",
            "📄 Page 252: 681 tokens\n",
            "📄 Page 253: 443 tokens\n",
            "📄 Page 254: 420 tokens\n",
            "📄 Page 255: 464 tokens\n",
            "📄 Page 256: 593 tokens\n",
            "📄 Page 257: 493 tokens\n",
            "📄 Page 258: 554 tokens\n",
            "📄 Page 259: 215 tokens\n",
            "📄 Page 260: 630 tokens\n",
            "📄 Page 261: 415 tokens\n",
            "📄 Page 262: 389 tokens\n",
            "📄 Page 263: 697 tokens\n",
            "📄 Page 264: 517 tokens\n",
            "📄 Page 265: 630 tokens\n",
            "📄 Page 266: 557 tokens\n",
            "📄 Page 267: 604 tokens\n",
            "📄 Page 268: 446 tokens\n",
            "📄 Page 269: 678 tokens\n",
            "📄 Page 270: 389 tokens\n",
            "📄 Page 271: 136 tokens\n",
            "📄 Page 273: 364 tokens\n",
            "📄 Page 274: 587 tokens\n",
            "📄 Page 275: 602 tokens\n",
            "📄 Page 276: 504 tokens\n",
            "📄 Page 277: 511 tokens\n",
            "📄 Page 278: 517 tokens\n",
            "📄 Page 279: 385 tokens\n",
            "📄 Page 281: 479 tokens\n",
            "📄 Page 282: 410 tokens\n",
            "📄 Page 283: 364 tokens\n",
            "📄 Page 284: 733 tokens\n",
            "📄 Page 285: 642 tokens\n",
            "📄 Page 286: 272 tokens\n",
            "📄 Page 287: 593 tokens\n",
            "📄 Page 288: 512 tokens\n",
            "📄 Page 289: 569 tokens\n",
            "📄 Page 290: 609 tokens\n",
            "📄 Page 291: 567 tokens\n",
            "📄 Page 292: 375 tokens\n",
            "📄 Page 293: 606 tokens\n",
            "📄 Page 294: 587 tokens\n",
            "📄 Page 295: 426 tokens\n",
            "📄 Page 296: 252 tokens\n",
            "📄 Page 297: 374 tokens\n",
            "📄 Page 298: 350 tokens\n",
            "📄 Page 299: 595 tokens\n",
            "📄 Page 300: 204 tokens\n",
            "📄 Page 301: 555 tokens\n",
            "📄 Page 302: 260 tokens\n",
            "📄 Page 303: 294 tokens\n",
            "📄 Page 304: 483 tokens\n",
            "📄 Page 305: 343 tokens\n",
            "📄 Page 306: 335 tokens\n",
            "📄 Page 307: 488 tokens\n",
            "📄 Page 308: 206 tokens\n",
            "📄 Page 309: 626 tokens\n",
            "📄 Page 310: 339 tokens\n",
            "📄 Page 311: 418 tokens\n",
            "📄 Page 312: 540 tokens\n",
            "📄 Page 313: 544 tokens\n",
            "📄 Page 315: 454 tokens\n",
            "📄 Page 316: 363 tokens\n",
            "📄 Page 317: 374 tokens\n",
            "📄 Page 318: 579 tokens\n",
            "📄 Page 319: 136 tokens\n",
            "📄 Page 320: 588 tokens\n",
            "📄 Page 321: 388 tokens\n",
            "📄 Page 322: 407 tokens\n",
            "📄 Page 323: 240 tokens\n",
            "📄 Page 324: 181 tokens\n",
            "📄 Page 325: 270 tokens\n",
            "📄 Page 327: 460 tokens\n",
            "📄 Page 328: 685 tokens\n",
            "📄 Page 329: 190 tokens\n",
            "📄 Page 330: 635 tokens\n",
            "📄 Page 331: 470 tokens\n",
            "📄 Page 332: 564 tokens\n",
            "📄 Page 333: 306 tokens\n",
            "📄 Page 334: 520 tokens\n",
            "📄 Page 335: 381 tokens\n",
            "📄 Page 336: 516 tokens\n",
            "📄 Page 337: 232 tokens\n",
            "📄 Page 339: 428 tokens\n",
            "📄 Page 340: 522 tokens\n",
            "📄 Page 341: 263 tokens\n",
            "📄 Page 342: 195 tokens\n",
            "📄 Page 343: 345 tokens\n",
            "📄 Page 344: 348 tokens\n",
            "📄 Page 345: 239 tokens\n",
            "📄 Page 346: 179 tokens\n",
            "📄 Page 347: 387 tokens\n",
            "📄 Page 348: 559 tokens\n",
            "📄 Page 349: 545 tokens\n",
            "📄 Page 350: 380 tokens\n",
            "📄 Page 351: 376 tokens\n",
            "📄 Page 352: 337 tokens\n",
            "📄 Page 353: 316 tokens\n",
            "📄 Page 354: 341 tokens\n",
            "📄 Page 355: 416 tokens\n",
            "📄 Page 356: 319 tokens\n",
            "📄 Page 357: 370 tokens\n",
            "📄 Page 358: 417 tokens\n",
            "📄 Page 359: 551 tokens\n",
            "📄 Page 360: 303 tokens\n",
            "📄 Page 361: 469 tokens\n",
            "📄 Page 362: 542 tokens\n",
            "📄 Page 363: 418 tokens\n",
            "📄 Page 364: 477 tokens\n",
            "📄 Page 365: 703 tokens\n",
            "📄 Page 366: 369 tokens\n",
            "📄 Page 367: 346 tokens\n",
            "📄 Page 368: 226 tokens\n",
            "📄 Page 369: 485 tokens\n",
            "📄 Page 370: 307 tokens\n",
            "📄 Page 371: 405 tokens\n",
            "📄 Page 372: 315 tokens\n",
            "📄 Page 373: 460 tokens\n",
            "📄 Page 374: 523 tokens\n",
            "📄 Page 375: 317 tokens\n",
            "📄 Page 376: 413 tokens\n",
            "📄 Page 377: 479 tokens\n",
            "📄 Page 379: 450 tokens\n",
            "📄 Page 380: 449 tokens\n",
            "📄 Page 381: 435 tokens\n",
            "📄 Page 382: 324 tokens\n",
            "📄 Page 383: 537 tokens\n",
            "📄 Page 384: 737 tokens\n",
            "📄 Page 385: 646 tokens\n",
            "📄 Page 386: 751 tokens\n",
            "📄 Page 387: 751 tokens\n",
            "📄 Page 388: 674 tokens\n",
            "📄 Page 389: 693 tokens\n",
            "📄 Page 390: 646 tokens\n",
            "📄 Page 391: 705 tokens\n",
            "📄 Page 392: 646 tokens\n",
            "📄 Page 393: 662 tokens\n",
            "📄 Page 394: 821 tokens\n",
            "📄 Page 395: 656 tokens\n",
            "📄 Page 396: 304 tokens\n",
            "\n",
            "⏱️ Total time: 118.84 seconds\n",
            "📊 Total tokens: 182925\n",
            "💰 Estimated embedding cost: $0.004390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib, re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Normalize text to avoid minor whitespace/case changes causing new IDs.\"\"\"\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower()\n",
        "\n",
        "def make_chunk_id(course_id: str, chunk_text: str) -> str:\n",
        "    \"\"\"Deterministic ID based on course_id + chunk_text.\"\"\"\n",
        "    norm = normalize_text(chunk_text)\n",
        "    raw = f\"{course_id}|{norm}\"\n",
        "    return hashlib.blake2b(raw.encode(\"utf-8\"), digest_size=16).hexdigest()\n"
      ],
      "metadata": {
        "id": "rUwMCTl4sSMI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opensearchpy.helpers import parallel_bulk\n",
        "index_name = \"test-auqa\"\n",
        "def doc_to_action(doc):\n",
        "    doc_id = make_chunk_id(doc[\"course_id\"], doc[\"chunk_text\"])\n",
        "    return {\n",
        "        \"_op_type\": \"index\",          # overwrite if exists\n",
        "        \"_index\": index_name,\n",
        "        \"_id\": doc_id,\n",
        "        \"_source\": {\n",
        "            \"chunk_text\": doc[\"chunk_text\"],\n",
        "            \"course_id\": doc[\"course_id\"],\n",
        "            \"filename\": doc[\"filename\"],\n",
        "            \"page_no\": doc[\"page_no\"],\n",
        "            \"vector_field\": doc[\"vector_field\"]  # Titan embedding (1024 floats)\n",
        "        }\n",
        "    }\n",
        "\n",
        "actions = (doc_to_action(d) for d in all_embeddings)\n",
        "\n",
        "for ok, result in parallel_bulk(client, actions, thread_count=1, chunk_size=100):\n",
        "    if not ok:\n",
        "        print(\"❌ Failed:\", result)\n",
        "print(\"✅ Bulk upsert finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwKHeDGFsVgo",
        "outputId": "b591f4dc-8ca5-42e2-d1b3-33e59566dd79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bulk upsert finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "def get_titan_embedding(text: str):\n",
        "    body = json.dumps({\"inputText\": text})\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "    return result[\"embedding\"], result.get(\"inputTextTokenCount\", None)\n",
        "\n",
        "query_text = \"Cost-benefit evaluation techniques\"\n",
        "query_vector, token_count = get_titan_embedding(query_text)\n",
        "\n",
        "print(f\"✅ Query embedded: {len(query_vector)}-dim vector | Tokens: {token_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWNe5hit_6U",
        "outputId": "ce299fb8-8e7d-4bf5-9489-5dc2b9bd099e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Query embedded: 1024-dim vector | Tokens: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 10\n",
        "BM25_WEIGHT = 0.3\n",
        "VECTOR_WEIGHT = 0.7\n",
        "\n",
        "# --- Run BM25 ---\n",
        "bm25_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\"size\": TOP_N, \"query\": {\"match\": {\"chunk_text\": query_text}}}\n",
        ")\n",
        "bm25_hits = bm25_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Run Vector ---\n",
        "vector_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\n",
        "        \"size\": TOP_N,\n",
        "        \"query\": {\n",
        "            \"knn\": {\"vector_field\": {\"vector\": query_vector, \"k\": TOP_N}}\n",
        "        }\n",
        "    }\n",
        ")\n",
        "vector_hits = vector_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Normalize scores ---\n",
        "def normalize_scores(hits):\n",
        "    scores = [h[\"_score\"] for h in hits]\n",
        "    if not scores:\n",
        "        return {}\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if min_s == max_s:\n",
        "        return {h[\"_id\"]: 1.0 for h in hits}\n",
        "    return {h[\"_id\"]: (h[\"_score\"] - min_s) / (max_s - min_s) for h in hits}\n",
        "\n",
        "bm25_norm = normalize_scores(bm25_hits)\n",
        "vector_norm = normalize_scores(vector_hits)\n",
        "\n",
        "# --- Combine ---\n",
        "combined = {}\n",
        "for h in bm25_hits + vector_hits:\n",
        "    _id = h[\"_id\"]\n",
        "    src = h[\"_source\"]\n",
        "    bm25_s = bm25_norm.get(_id, 0.0)\n",
        "    vec_s = vector_norm.get(_id, 0.0)\n",
        "    hybrid_score = BM25_WEIGHT * bm25_s + VECTOR_WEIGHT * vec_s\n",
        "    combined[_id] = {\n",
        "        \"hybrid_score\": hybrid_score,\n",
        "        \"bm25_score\": bm25_s,\n",
        "        \"vector_score\": vec_s,\n",
        "        \"source\": src\n",
        "    }\n",
        "\n",
        "results = sorted(combined.values(), key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
        "\n",
        "# --- Print top 5 results ---\n",
        "print(\"\\n🔎 Manual Hybrid Results:\")\n",
        "for r in results[:]:\n",
        "    src = r[\"source\"]\n",
        "    print(f\"Hybrid={r['hybrid_score']:.3f} | BM25={r['bm25_score']:.3f} | Vec={r['vector_score']:.3f}\")\n",
        "    print(f\"Page={src['page_no']} | Course={src['course_id']} | File={src['filename']}\")\n",
        "    print(src[\"chunk_text\"][:10], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hecRPJZwHnp",
        "outputId": "b8cd13ac-6078-4469-996c-370a6f74d0b3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Manual Hybrid Results:\n",
            "Hybrid=0.830 | BM25=0.435 | Vec=1.000\n",
            "Page=55 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.561 | BM25=0.123 | Vec=0.748\n",
            "Page=53 | Course=CS6022 | File=SPM.pdf\n",
            "3.4 COST-B ...\n",
            "\n",
            "Hybrid=0.402 | BM25=0.000 | Vec=0.574\n",
            "Page=57 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.326 | BM25=1.000 | Vec=0.037\n",
            "Page=7 | Course=CS6022 | File=SPM.pdf\n",
            "viii\n",
            "CONTE ...\n",
            "\n",
            "Hybrid=0.318 | BM25=0.335 | Vec=0.311\n",
            "Page=61 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.305 | BM25=0.000 | Vec=0.435\n",
            "Page=59 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 COST-B ...\n",
            "\n",
            "Hybrid=0.252 | BM25=0.840 | Vec=0.000\n",
            "Page=155 | Course=CS6022 | File=SPM.pdf\n",
            "7.7 EVALUA ...\n",
            "\n",
            "Hybrid=0.179 | BM25=0.017 | Vec=0.248\n",
            "Page=52 | Course=CS6022 | File=SPM.pdf\n",
            "40\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.177 | BM25=0.590 | Vec=0.000\n",
            "Page=49 | Course=CS6022 | File=SPM.pdf\n",
            "Chapter 3\n",
            " ...\n",
            "\n",
            "Hybrid=0.137 | BM25=0.000 | Vec=0.196\n",
            "Page=63 | Course=CS6022 | File=SPM.pdf\n",
            "3.7 RISK E ...\n",
            "\n",
            "Hybrid=0.087 | BM25=0.000 | Vec=0.125\n",
            "Page=56 | Course=CS6022 | File=SPM.pdf\n",
            "44\n",
            "CHAPTER ...\n",
            "\n",
            "Hybrid=0.067 | BM25=0.222 | Vec=0.000\n",
            "Page=385 | Course=CS6022 | File=SPM.pdf\n",
            "INDEX\n",
            "373\n",
            " ...\n",
            "\n",
            "Hybrid=0.038 | BM25=0.125 | Vec=0.000\n",
            "Page=148 | Course=CS6022 | File=SPM.pdf\n",
            "136\n",
            "CHAPTE ...\n",
            "\n",
            "Hybrid=0.000 | BM25=0.000 | Vec=0.000\n",
            "Page=212 | Course=CS6022 | File=SPM.pdf\n",
            "200\n",
            "CHAPTE ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect top 5 page chunks into one string\n",
        "context_text = \"\\n\\n\".join([r[\"source\"][\"chunk_text\"] for r in results[:5]])\n",
        "ques_query = query_text   # the query you used\n",
        "no = 10\n"
      ],
      "metadata": {
        "id": "PiJhquiAyjO2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an AI assistant specialized in question generation.\n",
        "Your goal is to generate insightful questions based on the given context and user query.\n",
        "\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "User Query (Focus Topic): {ques_query}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identify key points and concepts from the context relevant to the query\n",
        "- Step 2: Consider what types of questions best explore the topic of interest\n",
        "- Step 3: Formulate meaningful and topic-specific questions\n",
        "\n",
        "Generate at least **{no} questions** from the context.\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M7L4Eiryjio",
        "outputId": "a2600e0f-bf4b-4582-f780-75c1d81f59b2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are an AI assistant specialized in question generation.\n",
            "Your goal is to generate insightful questions based on the given context and user query.\n",
            "\n",
            "Context:\n",
            "3.6 COST-BENEFIT EVALUATION TECHNIQUES\n",
            "43\n",
            "patterns it can be advisable to produce quarterly, or even monthly, cash flow\n",
            "forecasts.\n",
            "3.6 Cost-benefit evaluation techniques\n",
            "We would consider proceeding with a project only where the benefits outweigh the\n",
            "costs. However, in order to choose among projects, we need to take into account\n",
            "the timing of the costs and benefits as well as the benefits relative to the size of the\n",
            "investment.\n",
            "Consider the project cash flow estimates for four projects at IOE shown in Exercise 3.2\n",
            "Table 3.2. Negative values represent expenditure and positive values income.\n",
            "Rank the four projects in order of financial desirability and make a note of your\n",
            "reasons for ranking them in that way before reading further.\n",
            "In the following sections we will take a brief look at some common methods for\n",
            "comparing projects on the basis of their cash flow forecasts.\n",
            "Net profit\n",
            "The net profit of a project is the difference between the total costs and the total\n",
            "income over the life of the project. Project 2 in Table 3.2 shows the greatest net\n",
            "profit but this is at the expense of a large investment. Indeed, if we had £lm to\n",
            "invest, we might undertake all of the other three projects and obtain an even greater\n",
            "net profit. Note also, that all projects contain an element of risk and we might not\n",
            "be prepared to risk £1m. We shall look at the effects of risk and investment later in\n",
            "this chapter.\n",
            "Table 3.2\n",
            "Four project cash flow projections - figures are end of year totals (£)\n",
            "Year\n",
            "Project /\n",
            "Project 2\n",
            "Project 3\n",
            "Project 4\n",
            "0\n",
            "-100,000\n",
            "-1,000,000\n",
            "-100,000\n",
            "-120,000\n",
            "1\n",
            "10,000\n",
            "200,000\n",
            "30,000\n",
            "30,000\n",
            "Cash flows take place at\n",
            "the end of each year. The\n",
            "2\n",
            "10,000\n",
            "200,000\n",
            "30,000\n",
            "30,000\n",
            "year o figure represents\n",
            "3\n",
            "10,000\n",
            "200,000\n",
            "30,000\n",
            "30,000\n",
            "the initial investment made\n",
            "4\n",
            "20,000\n",
            "200,000\n",
            "30,000\n",
            "30,000\n",
            "at the start of the project.\n",
            "5\n",
            "100,000\n",
            "300,000\n",
            "30,000\n",
            "75,000\n",
            "Net profit\n",
            "50,000\n",
            "100,000\n",
            "50,000\n",
            "75,000\n",
            "Moreover, the simple net profit takes no account of the timing of the cash flows.\n",
            "Projects 1 and 3 each have a net profit of £50,000 and therefore, according to this\n",
            "selection criterion, would be equally preferable. The bulk of the income occurs\n",
            "late in the life of project 1, whereas project 3 returns a steady income throughout\n",
            "\n",
            "3.4 COST-BENEFIT ANALYSIS\n",
            "41\n",
            "Identifying and estimating all of the costs and benefits of carrying out the\n",
            "project This includes development costs of the system, the operating costs\n",
            "and the benefits that are expected to accrue from the operation of the system.\n",
            "Where the proposed system is replacing an existing one, these estimates should\n",
            "reflect the costs and benefits due to the new system. A sales order processing\n",
            "system, for example, could not claim to benefit an organization by the total\n",
            "value of sales - only by the increase due to the use of the new system.\n",
            "Expressing these costs and benefits in common units We must evaluate the\n",
            "net benefit, which is the difference between the total benefit and the total cost.\n",
            "To do this, we must express each cost and each benefit in monetary terms.\n",
            "Most costs are relatively easy to identify and quantify in approximate monetary\n",
            "Many costs are easy to\n",
            "terms. It is helpful to categorize costs according to where they originate in the life\n",
            "identify and measure in\n",
            "of the project.\n",
            "monetary terms.\n",
            "Development costs - include the salaries and other employment costs of the\n",
            "staff involved in the development project and all associated costs.\n",
            "Setup costs - include the costs of putting the system into place. These consist\n",
            "mainly of the costs of any new hardware and ancillary equipment but will also\n",
            "include costs of file conversion, recruitment and staff training.\n",
            "Operational costs - consist of the costs of operating the system once it has\n",
            "been installed.\n",
            "Benefits, on the other hand, are often quite difficult to quantify in monetary\n",
            "terms even once they have been identified. Benefits may be categorized as follows.\n",
            "Direct benefits - these accrue directly from the operation of the proposed\n",
            "Indirect benefits, which\n",
            "system. These could, for example, include the reduction in salary bills through\n",
            "are difficult to estimate,\n",
            "the introduction of a new, computerized system.\n",
            "are sometimes known as\n",
            "intangible benefits.\n",
            "Assessable indirect benefits - these are generally secondary benefits, such as\n",
            "increased accuracy through the introduction of a more user-friendly screen\n",
            "design where we might be able to estimate the reduction in errors, and hence\n",
            "costs, of the proposed system.\n",
            "Intangible benefits - these are generally longer term or benefits that are con-\n",
            "sidered very difficult to quantify. Enhanced job interest can lead to reduced\n",
            "staff turnover and, hence, lower recruitment costs.\n",
            "Brightmouth College are considering the replacement of the existing payroll\n",
            "Exercise 3.1\n",
            "service, operated by a third party, with a tailored, off-the-shelf computer-based\n",
            "system. List some of the costs and benefits they might consider under each of the\n",
            "six headings given above. For each cost or benefit, explain how, in principle, it\n",
            "might be measured in monetary terms.\n",
            "\n",
            "3.6 COST-BENEFIT EVALUATION TECHNIQUES\n",
            "45\n",
            "The return on investment provides a simple, easy to calculate measure of return on\n",
            "capital and is therefore quite popular. Unfortunately it suffers from two severe\n",
            "disadvantages. Like the net profitability, it takes no account of the timing of the\n",
            "cash flows. More importantly, it is tempting to compare the rate of return with\n",
            "current interest rates. However, this rate of return bears no relationship to the\n",
            "interest rates offered or charged by banks (or any other normal interest rate) since\n",
            "it takes no account of the timing of the cash flows or of the compounding of\n",
            "interest. It is therefore, potentially, very misleading.\n",
            "Net present value\n",
            "The calculation of net present value is a project evaluation technique that takes\n",
            "Net present value (NPV)\n",
            "into account the profitability of a project and the timing of the cash flows that are\n",
            "and internal rate of return\n",
            "produced. It does so by discounting future cash flows by a percentage known as\n",
            "(IRR) are collectively\n",
            "the discount rate. This is based on the view that receiving £100 today is better than\n",
            "known as discounted cash\n",
            "having to wait until next year to receive it, because the £100 next year is worth less\n",
            "flow (DCF) techniques.\n",
            "than £100 now. We could, for example, invest the £100 in a bank today and have\n",
            "£100 plus the interest in a year's time. If we say that the present value of £100 in\n",
            "a year's time is £91, we mean that £100 in a year's time is the equivalent of £91\n",
            "now.\n",
            "The equivalence of £91 now and £100 in a year's time means we are discounting\n",
            "Note that this example\n",
            "the future income by approximately 10% - that is, we would need an extra 10% to\n",
            "uses approximate figures\n",
            "make it worthwhile waiting for a year. An alternative way of considering the\n",
            "- when you have finished\n",
            "equivalence of the two is to consider that, if we received £91 now and invested for\n",
            "reading this section you\n",
            "a year at an annual interest rate of 10%, it would be worth £100 in a year's time.\n",
            "should be able to calculate\n",
            "The annual rate by which we discount future earnings is known as the discount\n",
            "the exact figures yourself.\n",
            "rate 10% in the above example.\n",
            "Similarly, £100 received in 2 years' time would have a present value of\n",
            "approximately £83 - in other words, £83 invested at an interest rate of 10% would\n",
            "yield approximately £100 in 2 years' time.\n",
            "The present value of any future cash flow may be obtained by applying the\n",
            "following formula\n",
            "present value =\n",
            "value in year I\n",
            "(1+r)'\n",
            "where r is the discount rate, expressed as a decimal value and 1 is the number of\n",
            "years into the future that the cash flow occurs.\n",
            "Alternatively, and rather more easily, the present value of a cash flow may be\n",
            "calculated by multiplying the cash flow by the appropriate discount factor. A small\n",
            "table of discount factors is given in Table 3.3.\n",
            "The NPV for a project is obtained by discounting each cash flow (both negative\n",
            "and positive) and summing the discounted values. It is normally assumed that any\n",
            "initial investment takes place immediately (indicated as year 0) and is not\n",
            "discounted. Later cash flows are normally assumed to take place at the end of each\n",
            "year and are discounted by the appropriate amount.\n",
            "\n",
            "viii\n",
            "CONTENTS\n",
            "3.4\n",
            "Cost-benefit analysis\n",
            "40\n",
            "3.5\n",
            "Cash flow forecasting\n",
            "42\n",
            "3.6\n",
            "Cost-benefit evaluation techniques\n",
            "43\n",
            "3.7\n",
            "Risk evaluation\n",
            "50\n",
            "3.8\n",
            "Conclusion\n",
            "55\n",
            "3.9\n",
            "Further exercises\n",
            "55\n",
            "4\n",
            "Selection of an appropriate project approach\n",
            "57\n",
            "4.1\n",
            "Introduction\n",
            "57\n",
            "4.2\n",
            "Choosing technologies\n",
            "59\n",
            "4.3\n",
            "Technical plan contents list\n",
            "63\n",
            "4.4\n",
            "Choice of process models\n",
            "63\n",
            "4.5\n",
            "Structured methods\n",
            "64\n",
            "4.6\n",
            "Rapid application development\n",
            "64\n",
            "4.7\n",
            "The waterfall model\n",
            "65\n",
            "4.8\n",
            "The V-process model\n",
            "66\n",
            "4.9\n",
            "The spiral model\n",
            "67\n",
            "4.10\n",
            "Software prototyping\n",
            "67\n",
            "4.11\n",
            "Other ways of categorizing prototypes\n",
            "70\n",
            "4.12\n",
            "Tools\n",
            "71\n",
            "4.13\n",
            "A prototyping example\n",
            "72\n",
            "4.14\n",
            "Incremental delivery\n",
            "73\n",
            "4.15\n",
            "An incremental example\n",
            "76\n",
            "4.16\n",
            "Selecting the most appropriate process model\n",
            "76\n",
            "4.17\n",
            "Conclusion\n",
            "77\n",
            "4.18\n",
            "Further exercises\n",
            "77\n",
            "5\n",
            "Software effort estimation\n",
            "79\n",
            "5.1\n",
            "Introduction\n",
            "79\n",
            "5.2\n",
            "Where are estimates done?\n",
            "81\n",
            "5.3\n",
            "Problems with over- and under-estimates\n",
            "82\n",
            "5.4\n",
            "The basis for software estimating\n",
            "84\n",
            "5.5\n",
            "Software effort estimation techniques\n",
            "85\n",
            "5.6\n",
            "Expert judgement\n",
            "87\n",
            "5.7\n",
            "Estimating by analogy\n",
            "88\n",
            "5.8\n",
            "Albrecht function point analysis\n",
            "89\n",
            "5.9\n",
            "Function points Mark II\n",
            "92\n",
            "5.10\n",
            "Object points\n",
            "94\n",
            "5.11\n",
            "A procedural code-oriented approach\n",
            "96\n",
            "5.12\n",
            "COCOMO: a parametric model\n",
            "97\n",
            "5.13\n",
            "Conclusion\n",
            "103\n",
            "5.14\n",
            "Additional exercises\n",
            "104\n",
            "\n",
            "3.6 COST-BENEFIT EVALUATION TECHNIQUES\n",
            "49\n",
            "that a project with an IRR greater than current interest rates will provide a better\n",
            "rate of return than lending the investment to a bank. We can also say that it will be\n",
            "worth borrowing to finance the project if it has an IRR greater than the interest rate\n",
            "charged on the loan.\n",
            "Table 3.6\n",
            "A project cash flow treated as an investment at 10%\n",
            "Equivalent investment at 10%\n",
            "(a)\n",
            "(b)\n",
            "(c)\n",
            "(d)\n",
            "(e)\n",
            "£100,000 invested at 10%\n",
            "Year\n",
            "Project cash\n",
            "Capital at\n",
            "Interest\n",
            "Capital at\n",
            "End of year\n",
            "may be used to generate\n",
            "flow forecast\n",
            "start of year\n",
            "during year\n",
            "end of year\n",
            "withdrawal\n",
            "the cash flows shown. At\n",
            "(£)\n",
            "(£)\n",
            "(£)\n",
            "(£)\n",
            "(£)\n",
            "the end of the 5-year\n",
            "0\n",
            "-100,000\n",
            "-\n",
            "-\n",
            "-\n",
            "-\n",
            "period the capital and the\n",
            "1\n",
            "10,000\n",
            "100,000\n",
            "10,000\n",
            "110,000\n",
            "10,000\n",
            "interest payments will be\n",
            "entirely consumed leaving\n",
            "2\n",
            "10,000\n",
            "100,000\n",
            "10,000\n",
            "110,000\n",
            "10,000\n",
            "a net balance of zero.\n",
            "3\n",
            "10,000\n",
            "100,000\n",
            "10,000\n",
            "110,000\n",
            "10,000\n",
            "4\n",
            "20,000\n",
            "100,000\n",
            "10,000\n",
            "110,000\n",
            "20,000\n",
            "5\n",
            "99,000\n",
            "90,000\n",
            "9,000\n",
            "99,000\n",
            "99,000\n",
            "6\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "One deficiency of the IRR is that it does not indicate the absolute size of the\n",
            "return. A project with an NPV of £100,000 and an IRR of 15% can be more\n",
            "attractive than one with an NPV of £10,000 and an IRR of 18% - the return on\n",
            "capital is lower but the net benefits greater.\n",
            "An often quoted objection to the internal rate of return is that, under certain\n",
            "conditions, it is possible to find more than one rate that will produce a zero NPV.\n",
            "This is not a valid objection since, if there are multiple solutions, it is always\n",
            "appropriate to take the lowest value and ignore the others. Spreadsheets will\n",
            "normally always return the lowest value if provided with zero as a seed value.\n",
            "NPV and IRR are not, however, a complete answer to economic project\n",
            "evaluation.\n",
            "A total evaluation must also take into account the problems of funding the cash\n",
            "flows - will we, for example, be able to repay the interest on any borrowed\n",
            "money and pay development staff salaries at the appropriate time?\n",
            "While a project's IRR might indicate a profitable project, future earnings from\n",
            "a project might be far less reliable than earnings from, say, investing with a\n",
            "bank. To take account of the risk inherent in investing in a project, we might\n",
            "require that a project earn a 'risk premium' (that is, it must earn, say, at least\n",
            "15% more than current interest rates) or we might undertake a more detailed\n",
            "risk analysis as described in the following sections of this chapter.\n",
            "We must also consider any one project within the financial and economic\n",
            "framework of the organization as a whole - if we fund this one, will we also be\n",
            "able to fund other worthy projects?\n",
            "\n",
            "User Query (Focus Topic): Cost-benefit evaluation techniques\n",
            "\n",
            "### Reasoning:\n",
            "- Step 1: Identify key points and concepts from the context relevant to the query\n",
            "- Step 2: Consider what types of questions best explore the topic of interest\n",
            "- Step 3: Formulate meaningful and topic-specific questions\n",
            "\n",
            "Generate at least **10 questions** from the context.\n",
            "\n",
            "### Questions:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, json, time\n",
        "\n",
        "REGION = \"ap-south-1\"\n",
        "PROFILE_ARN = \"arn:aws:bedrock:ap-south-1:850146468080:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "body = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 500,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "start = time.time()\n",
        "resp = bedrock_rt.invoke_model(modelId=PROFILE_ARN, body=json.dumps(body))\n",
        "elapsed = time.time() - start\n",
        "\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "\n",
        "reply = output[\"content\"][0][\"text\"]\n",
        "\n",
        "# Usage metadata (if present)\n",
        "usage = output.get(\"usage\", {})\n",
        "input_tokens = usage.get(\"input_tokens\", \"N/A\")\n",
        "output_tokens = usage.get(\"output_tokens\", \"N/A\")\n",
        "\n",
        "print(\"✅ Generated Output:\\n\")\n",
        "print(reply)\n",
        "\n",
        "print(\"\\n📊 Stats:\")\n",
        "print(\"  Input tokens :\", input_tokens)\n",
        "print(\"  Output tokens:\", output_tokens)\n",
        "print(f\"  Time taken   : {elapsed:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PimWoLJNyjlt",
        "outputId": "3ebf9e98-d5e3-4f0d-e99f-cc112d02c242"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generated Output:\n",
            "\n",
            "Based on the context about cost-benefit evaluation techniques, here are 10 relevant questions:\n",
            "\n",
            "1. What are the four main methods of cost-benefit evaluation techniques discussed in the text, and how do they differ from each other?\n",
            "\n",
            "2. Why is considering only net profit insufficient for evaluating project desirability? Provide at least two limitations mentioned in the text.\n",
            "\n",
            "3. How is Net Present Value (NPV) calculated, and why does it provide a more comprehensive evaluation than simple net profit?\n",
            "\n",
            "4. What role does the discount rate play in NPV calculations, and how does it relate to the concept of time value of money?\n",
            "\n",
            "5. Compare and contrast the advantages and disadvantages of Internal Rate of Return (IRR) as a project evaluation technique.\n",
            "\n",
            "6. According to the text, why might a project with a lower IRR but higher NPV be more attractive than one with a higher IRR but lower NPV?\n",
            "\n",
            "7. How does Return on Investment (ROI) differ from other evaluation techniques, and what are its main limitations as mentioned in the text?\n",
            "\n",
            "8. What additional factors beyond NPV and IRR should be considered when conducting a complete economic project evaluation?\n",
            "\n",
            "9. Using Table 3.2 as reference, how would you explain the importance of considering the timing of cash flows in project evaluation?\n",
            "\n",
            "10. What is meant by a \"risk premium\" in project evaluation, and why might organizations require one when assessing projects?\n",
            "\n",
            "These questions test understanding of the key concepts, methodologies, and practical applications of cost-benefit evaluation techniques while encouraging critical thinking about their relative strengths and limitations.\n",
            "\n",
            "📊 Stats:\n",
            "  Input tokens : 3304\n",
            "  Output tokens: 343\n",
            "  Time taken   : 8.83 seconds\n"
          ]
        }
      ]
    }
  ]
}