{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW0BE7CJbbAwrNp4nV7MHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auqa5325/AUQA-colab/blob/main/AUQA_questify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-uDEQ4a38AH",
        "outputId": "0dd9ea0e-b415-4b68-88c3-c24e64d2276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install boto3 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY_ID     = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "#AWS_SESSION_TOKEN     = userdata.get(\"AWS_SESSION_TOKEN\")  # may be None\n",
        "AWS_REGION = userdata.get(\"AWS_REGION\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
        "\n",
        "print(\"✅ Credentials set. Region:\", AWS_REGION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8aVrSE4ZiZ",
        "outputId": "bc768233-e5be-4b5c-e292-b3ca100c78e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Credentials set. Region: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]\n",
        "session = boto3.Session(region_name=REGION)\n",
        "bedrock         = session.client(\"bedrock\")\n",
        "bedrock_runtime = session.client(\"bedrock-runtime\")\n",
        "print(\"✅ boto3 session initialized in:\", session.region_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqqWGkX4-QA",
        "outputId": "97938a45-4fdd-4934-fd4b-0442e7f0e2b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ boto3 session initialized in: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "s = boto3.Session(region_name=\"ap-south-1\")\n",
        "print(s.client(\"sts\").get_caller_identity())   # must print Account, Arn\n",
        "creds = s.get_credentials().get_frozen_credentials()\n",
        "print(\"AccessKey:\", creds.access_key[:4], \"HasToken:\", bool(creds.token))\n"
      ],
      "metadata": {
        "id": "UT2f-KtPS8C0",
        "outputId": "50ec2908-7828-47b9-9e96-004a73232aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY1NDg4MzQxNDA6Ujp3Y0FQd1kzUA==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Sat, 30 Aug 2025 10:13:54 GMT'}, 'RetryAttempts': 0}}\n",
            "AccessKey: AKIA HasToken: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output dict structure :\n",
        "* id → unique identifier for this response (good for\n",
        "logging/debugging).\n",
        "\n",
        "* type → \"message\" → tells you this is a message object.\n",
        "\n",
        "* role → \"assistant\" → the speaker role (assistant vs. user).\n",
        "\n",
        "* model → which model gave this reply (claude-3-sonnet-20240229).\n",
        "\n",
        "* content → a list of parts that make up the response.\n",
        "\n",
        "* stop_reason → why the model stopped (e.g., end_turn, max_tokens).\n",
        "\n",
        "* stop_sequence → custom sequence that stopped generation (here it’s None).\n",
        "\n",
        "* usage → token usage info (handy for cost + rate limits)."
      ],
      "metadata": {
        "id": "wbqD05a1_0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "prompt = \"explain tcp ip?\"\n",
        "\n",
        "body = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 100,\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "print(output[\"outputs\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "xKuzvoVdBegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "session = boto3.Session(region_name=\"ap-south-1\")\n",
        "sts = session.client(\"sts\")\n",
        "print(sts.get_caller_identity())  # should return Account, Arn, UserId\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5nKJfpQRoqH",
        "outputId": "6c4b9df1-8ef1-42e2-acc7-d3e67a45b3d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY0NTkzNTE3NDk6UjpJd1BrYlNOVg==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Fri, 29 Aug 2025 09:22:31 GMT'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'outputs': [{'text': '\\n\\nTCP/IP is a suite of protocols that defines the Internet. Originally designed for the UNIX operating system. The TCP/IP protocol suite is a four-layer model that provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed, and received.\\n\\nThe four layers of the TCP/IP model are:\\n\\n1. The Application Layer: This is the topmost layer of the TCP/IP', 'stop_reason': 'length'}]}"
      ],
      "metadata": {
        "id": "zlZJ3onqB_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opensearch-py requests-aws4auth\n",
        "\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "from requests_aws4auth import AWS4Auth\n",
        "import boto3\n",
        "\n",
        "# --- CONFIG ---\n",
        "host   = \"https://search-test1-annauniv-pcx3f52wxykhpd4md6v4bjeqdy.ap-south-1.es.amazonaws.com\"  # OpenSearch domain endpoint\n",
        "region = \"ap-south-1\"\n",
        "service = \"es\"\n",
        "\n",
        "# --- AWS SigV4 Auth ---\n",
        "session = boto3.Session()\n",
        "credentials = session.get_credentials()\n",
        "awsauth = AWS4Auth(\n",
        "    credentials.access_key,\n",
        "    credentials.secret_key,\n",
        "    region,\n",
        "    service,\n",
        "    session_token=credentials.token\n",
        ")\n",
        "\n",
        "# --- OpenSearch Client ---\n",
        "client = OpenSearch(\n",
        "    hosts=[host],\n",
        "    http_auth=awsauth,\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    connection_class=RequestsHttpConnection\n",
        ")\n",
        "\n",
        "# --- TEST: Get cluster info ---\n",
        "print(client.info())\n"
      ],
      "metadata": {
        "id": "l5z2sIp87q1F",
        "outputId": "fd28194c-422f-4221-b070-f9d0bb23b28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/371.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m368.6/371.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.5/371.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h{'name': 'd69eb8cacc6e2cf42750a5f4788db327', 'cluster_name': '850146468080:test1-annauniv', 'cluster_uuid': 'qhkHo9X9Suuw8REB9uInGQ', 'version': {'distribution': 'opensearch', 'number': '2.19.0', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-07-24T06:15:41.026838036Z', 'build_snapshot': False, 'lucene_version': '9.12.1', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "index_body = {\n",
        "    \"settings\": {\n",
        "        \"index\": {\n",
        "            \"knn\": True,\n",
        "            \"knn.algo_param.ef_search\": 512,\n",
        "            \"number_of_shards\": 5,\n",
        "            \"number_of_replicas\": 1\n",
        "        }\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            # main chunk text\n",
        "            \"chunk_text\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # ids & names\n",
        "            \"course_id\": {\n",
        "                \"type\": \"keyword\"  # IDs are better stored as keyword for exact matches\n",
        "            },\n",
        "\n",
        "            \"filename\": {\n",
        "                \"type\": \"keyword\"  # filenames usually exact matches\n",
        "            },\n",
        "\n",
        "            # single-page (legacy support)\n",
        "            \"page_no\": {\"type\": \"long\"},\n",
        "\n",
        "            # NEW: page range string (e.g., \"1-6\")\n",
        "            \"page_range\": {\"type\": \"keyword\"},\n",
        "\n",
        "            # NEW: array of page numbers (e.g., [1,2,3,4,5,6])\n",
        "            \"pages\": {\"type\": \"long\"},\n",
        "\n",
        "            # vector index used for ANN similarity search\n",
        "            \"vector_field\": {\n",
        "                \"type\": \"knn_vector\",\n",
        "                \"dimension\": 1024,\n",
        "                \"method\": {\n",
        "                    \"name\": \"hnsw\",\n",
        "                    \"space_type\": \"cosinesimil\",\n",
        "                    \"engine\": \"nmslib\",\n",
        "                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# recreate index\n",
        "if client.indices.exists(index=index_name):\n",
        "    print(f\"Index '{index_name}' already exists. Deleting and recreating...\")\n",
        "    client.indices.delete(index=index_name)\n",
        "\n",
        "client.indices.create(index=index_name, body=index_body)\n",
        "print(f\"✅ Index '{index_name}' created successfully!\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UcTIfVU4RS02",
        "outputId": "c6a517ed-7845-4c1f-d450-c445c75646a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nindex_name = \"test-auqa\"\\n\\nindex_body = {\\n    \"settings\": {\\n        \"index\": {\\n            \"knn\": True,\\n            \"knn.algo_param.ef_search\": 512,\\n            \"number_of_shards\": 5,\\n            \"number_of_replicas\": 1\\n        }\\n    },\\n    \"mappings\": {\\n        \"properties\": {\\n            # main chunk text\\n            \"chunk_text\": {\\n                \"type\": \"text\",\\n                \"fields\": {\\n                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\\n                }\\n            },\\n\\n            # ids & names\\n            \"course_id\": {\\n                \"type\": \"keyword\"  # IDs are better stored as keyword for exact matches\\n            },\\n           \\n            \"filename\": {\\n                \"type\": \"keyword\"  # filenames usually exact matches\\n            },\\n\\n            # single-page (legacy support)\\n            \"page_no\": {\"type\": \"long\"},\\n\\n            # NEW: page range string (e.g., \"1-6\")\\n            \"page_range\": {\"type\": \"keyword\"},\\n\\n            # NEW: array of page numbers (e.g., [1,2,3,4,5,6])\\n            \"pages\": {\"type\": \"long\"},\\n\\n            # vector index used for ANN similarity search\\n            \"vector_field\": {\\n                \"type\": \"knn_vector\",\\n                \"dimension\": 1024,\\n                \"method\": {\\n                    \"name\": \"hnsw\",\\n                    \"space_type\": \"cosinesimil\",\\n                    \"engine\": \"nmslib\",\\n                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\\n                }\\n            }\\n        }\\n    }\\n}\\n\\n# recreate index\\nif client.indices.exists(index=index_name):\\n    print(f\"Index \\'{index_name}\\' already exists. Deleting and recreating...\")\\n    client.indices.delete(index=index_name)\\n\\nclient.indices.create(index=index_name, body=index_body)\\nprint(f\"✅ Index \\'{index_name}\\' created successfully!\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'test-annauniv': {'aliases': {}, 'mappings': {'properties': {'course_id': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'embedding': {'type': 'float'}, 'metadata': {'properties': {'page': {'type': 'long'}, 'source': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'page_no': {'type': 'long'}, 'text': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'vector_field': {'type': 'knn_vector', 'dimension': 1024, 'method': {'engine': 'nmslib', 'space_type': 'cosinesimil', 'name': 'hnsw', 'parameters': {'ef_construction': 512, 'm': 16}}}}}, 'settings': {'index': {'replication': {'type': 'DOCUMENT'}, 'refresh_interval': '1s', 'number_of_shards': '5', 'knn.algo_param': {'ef_search': '512'}, 'provided_name': 'test-annauniv', 'knn': 'true', 'creation_date': '1756144596121', 'number_of_replicas': '1', 'uuid': 'rumasDZaQC6xBy4G48v1KQ', 'version': {'created': '136407827'}}}}}\n",
        "\n"
      ],
      "metadata": {
        "id": "n-xX1sT3yxqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, time, re\n",
        "\n",
        "BUCKET = \"anna-univ-qna\"\n",
        "DOCUMENT = \"Textbooks/SPM.pdf\"\n",
        "REGION = \"ap-south-1\"\n",
        "COURSE_ID = \"CS6022\"   # <-- set this per textbook\n",
        "FILENAME = \"SPM.pdf\"\n",
        "\n",
        "textract = boto3.client(\"textract\", region_name=REGION)\n",
        "\n",
        "# --- Start async Textract job ---\n",
        "start = time.time()\n",
        "response = textract.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": BUCKET, \"Name\": DOCUMENT}}\n",
        ")\n",
        "print(\"FILENAME =\",FILENAME)\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"✅ Started Textract JobId: {job_id}\")\n",
        "\n",
        "# --- Wait for completion ---\n",
        "while True:\n",
        "    result = textract.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"❌ Textract job failed!\")\n",
        "\n",
        "pages = result[\"DocumentMetadata\"][\"Pages\"]\n",
        "print(f\"📄 Pages: {pages}\")\n",
        "print(f\"⏱️ Time taken: {elapsed:.2f} sec\")\n",
        "print(f\"💰 Estimated cost: ${(pages/1000)*1.5:.4f}\")\n",
        "\n",
        "# --- Collect page-wise text into docs with metadata ---\n",
        "page_texts = {}\n",
        "next_token = None\n",
        "\n",
        "while True:\n",
        "    if next_token:\n",
        "        result = textract.get_document_text_detection(JobId=job_id, NextToken=next_token)\n",
        "    else:\n",
        "        result = textract.get_document_text_detection(JobId=job_id)\n",
        "\n",
        "    for block in result[\"Blocks\"]:\n",
        "        if block[\"BlockType\"] == \"LINE\":\n",
        "            page_no = block[\"Page\"]\n",
        "            page_texts.setdefault(page_no, []).append(block[\"Text\"])\n",
        "\n",
        "    next_token = result.get(\"NextToken\")\n",
        "    if not next_token:\n",
        "        break\n",
        "\n",
        "# Build document objects (ready to send to Titan for embeddings)\n",
        "docs = []\n",
        "for page_no, lines in page_texts.items():\n",
        "    chunk_text = \"\\n\".join(lines)\n",
        "    doc = {\n",
        "        \"chunk_text\": chunk_text,\n",
        "        \"course_id\": COURSE_ID,\n",
        "        \"filename\": FILENAME,\n",
        "        \"page_no\": page_no\n",
        "    }\n",
        "    docs.append(doc)\n",
        "\n",
        "# --- Preview first 5 pages ---\n",
        "print(\"\\n📑 First 5 pages extracted:\")\n",
        "for d in docs[:5]:\n",
        "    print(f\"\\nPage {d['page_no']} | Course: {d['course_id']} | File: {d['filename']}\")\n",
        "    print(d[\"chunk_text\"][:400], \"...\")\n"
      ],
      "metadata": {
        "id": "DVaNNUjgqWN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "806b9018-75df-421c-b8ef-aeea73a65c2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FILENAME = SPM.pdf\n",
            "✅ Started Textract JobId: ce41d655412d458abda1771970d243aee8b1fcb47f07c25ecdfed15620863448\n",
            "📄 Pages: 396\n",
            "⏱️ Time taken: 231.58 sec\n",
            "💰 Estimated cost: $0.5940\n",
            "\n",
            "📑 First 5 pages extracted:\n",
            "\n",
            "Page 1 | Course: CS6022 | File: SPM.pdf\n",
            "BoB HUGHES AND MIKE COTTERELL\n",
            "Software\n",
            "Project\n",
            "Management Second Edition\n",
            "Tistimating\n",
            "www.mcgraw-hill.co.uk/hughes ...\n",
            "\n",
            "Page 2 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition) ...\n",
            "\n",
            "Page 3 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition)\n",
            "Bob Hughes and Mike Cotterell,\n",
            "School of Information Management, University of Brighton\n",
            "The McGraw-Hill Companies\n",
            "London\n",
            "Burr Ridge, IL\n",
            "New York\n",
            "St Louis\n",
            "San Francisco\n",
            "Auckland\n",
            "Bogotá Caracas\n",
            "Lisbon\n",
            "Madrid\n",
            "Mexico\n",
            "Milan\n",
            "Montreal\n",
            "New Delhi\n",
            "Panama\n",
            "Paris\n",
            "San Juan\n",
            "São Paulo\n",
            "Singapore\n",
            "Tokyo\n",
            "Toronto ...\n",
            "\n",
            "Page 4 | Course: CS6022 | File: SPM.pdf\n",
            "Published by\n",
            "McGraw-Hill Publishing Company\n",
            "SHOPPENHANGERS ROAD, MAIDENHEAD, BERKSHIRE, SL6 2QL, ENGLAND\n",
            "Telephone: +44(o) 1628 502500\n",
            "Fax: +44(o) 1628 770224\n",
            "Web site: http://www.megraw-hill.co.uk\n",
            "British Library Cataloguing in Publication Data\n",
            "A catalogue record for this book is available from the British Library\n",
            "ISBN 007 709505 7\n",
            "Library of Congress cataloguing in publication data\n",
            "The LOC data  ...\n",
            "\n",
            "Page 5 | Course: CS6022 | File: SPM.pdf\n",
            "The road to hell is paved with works-in-progress.\n",
            "Philip Roth ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_page_chunks(docs, chunk_size=6, overlap=2):\n",
        "    \"\"\"\n",
        "    Re-chunk page-level docs into multi-page chunks.\n",
        "    Each chunk contains `chunk_size` pages, with `overlap` pages between chunks.\n",
        "    \"\"\"\n",
        "    # Ensure pages are sorted\n",
        "    docs_sorted = sorted(docs, key=lambda x: x[\"page_no\"])\n",
        "\n",
        "    new_chunks = []\n",
        "    step = chunk_size - overlap\n",
        "    total_pages = len(docs_sorted)\n",
        "\n",
        "    for i in range(0, total_pages, step):\n",
        "        # Select pages for this chunk\n",
        "        chunk_docs = docs_sorted[i:i+chunk_size]\n",
        "        if not chunk_docs:\n",
        "            continue\n",
        "\n",
        "        # Combine texts\n",
        "        chunk_text = \"\\n\\n\".join(d[\"chunk_text\"] for d in chunk_docs)\n",
        "\n",
        "        # Metadata: first + last page\n",
        "        page_start = chunk_docs[0][\"page_no\"]\n",
        "        page_end = chunk_docs[-1][\"page_no\"]\n",
        "\n",
        "        new_chunks.append({\n",
        "            \"chunk_text\": chunk_text,\n",
        "            \"course_id\": chunk_docs[0][\"course_id\"],   # same for all\n",
        "            \"filename\": chunk_docs[0][\"filename\"],     # same file\n",
        "            \"page_range\": f\"{page_start}-{page_end}\",\n",
        "            \"pages\": [d[\"page_no\"] for d in chunk_docs]\n",
        "        })\n",
        "\n",
        "    return new_chunks\n",
        "\n",
        "# Example usage\n",
        "chunked_docs = make_page_chunks(docs, chunk_size=6, overlap=2)\n",
        "print(f\"✅ Created {len(chunked_docs)} multi-page chunks\")\n",
        "for c in chunked_docs[:]:\n",
        "    print(f\"Chunk pages: {c['page_range']}, length={len(c['chunk_text'])} chars\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjcmg5jTkxyj",
        "outputId": "d3d5887a-a82d-4ae3-9228-ad3b1947d4c9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created 97 multi-page chunks\n",
            "Chunk pages: 1-6, length=3159 chars\n",
            "Chunk pages: 5-10, length=5704 chars\n",
            "Chunk pages: 9-14, length=10492 chars\n",
            "Chunk pages: 13-18, length=13310 chars\n",
            "Chunk pages: 17-22, length=14129 chars\n",
            "Chunk pages: 21-26, length=14491 chars\n",
            "Chunk pages: 25-30, length=15321 chars\n",
            "Chunk pages: 29-34, length=12528 chars\n",
            "Chunk pages: 33-38, length=13568 chars\n",
            "Chunk pages: 37-42, length=13716 chars\n",
            "Chunk pages: 41-46, length=12544 chars\n",
            "Chunk pages: 45-50, length=12730 chars\n",
            "Chunk pages: 49-54, length=14228 chars\n",
            "Chunk pages: 53-58, length=14591 chars\n",
            "Chunk pages: 57-62, length=15423 chars\n",
            "Chunk pages: 61-66, length=14447 chars\n",
            "Chunk pages: 65-71, length=10503 chars\n",
            "Chunk pages: 70-75, length=14014 chars\n",
            "Chunk pages: 74-79, length=14289 chars\n",
            "Chunk pages: 78-83, length=13774 chars\n",
            "Chunk pages: 82-87, length=13389 chars\n",
            "Chunk pages: 86-91, length=11503 chars\n",
            "Chunk pages: 90-95, length=12682 chars\n",
            "Chunk pages: 94-99, length=16610 chars\n",
            "Chunk pages: 98-103, length=15775 chars\n",
            "Chunk pages: 102-107, length=12250 chars\n",
            "Chunk pages: 106-111, length=13370 chars\n",
            "Chunk pages: 110-115, length=12789 chars\n",
            "Chunk pages: 114-120, length=9942 chars\n",
            "Chunk pages: 119-124, length=14297 chars\n",
            "Chunk pages: 123-128, length=14853 chars\n",
            "Chunk pages: 127-132, length=11457 chars\n",
            "Chunk pages: 131-136, length=10390 chars\n",
            "Chunk pages: 135-140, length=11392 chars\n",
            "Chunk pages: 139-144, length=11952 chars\n",
            "Chunk pages: 143-148, length=10381 chars\n",
            "Chunk pages: 147-152, length=15665 chars\n",
            "Chunk pages: 151-156, length=14839 chars\n",
            "Chunk pages: 155-160, length=11040 chars\n",
            "Chunk pages: 159-164, length=9761 chars\n",
            "Chunk pages: 163-168, length=11511 chars\n",
            "Chunk pages: 167-172, length=11717 chars\n",
            "Chunk pages: 171-176, length=12574 chars\n",
            "Chunk pages: 175-180, length=8531 chars\n",
            "Chunk pages: 179-184, length=9134 chars\n",
            "Chunk pages: 183-188, length=12970 chars\n",
            "Chunk pages: 187-192, length=12034 chars\n",
            "Chunk pages: 191-196, length=9644 chars\n",
            "Chunk pages: 195-200, length=13323 chars\n",
            "Chunk pages: 199-204, length=15025 chars\n",
            "Chunk pages: 203-208, length=15139 chars\n",
            "Chunk pages: 207-212, length=16705 chars\n",
            "Chunk pages: 211-216, length=16676 chars\n",
            "Chunk pages: 215-220, length=16013 chars\n",
            "Chunk pages: 219-225, length=12604 chars\n",
            "Chunk pages: 224-229, length=16003 chars\n",
            "Chunk pages: 228-233, length=15902 chars\n",
            "Chunk pages: 232-237, length=16062 chars\n",
            "Chunk pages: 236-241, length=15295 chars\n",
            "Chunk pages: 240-245, length=15118 chars\n",
            "Chunk pages: 244-250, length=11429 chars\n",
            "Chunk pages: 249-254, length=14292 chars\n",
            "Chunk pages: 253-258, length=13835 chars\n",
            "Chunk pages: 257-262, length=12882 chars\n",
            "Chunk pages: 261-266, length=15623 chars\n",
            "Chunk pages: 265-270, length=16077 chars\n",
            "Chunk pages: 269-275, length=13040 chars\n",
            "Chunk pages: 274-279, length=14645 chars\n",
            "Chunk pages: 278-284, length=13634 chars\n",
            "Chunk pages: 283-288, length=14710 chars\n",
            "Chunk pages: 287-292, length=14894 chars\n",
            "Chunk pages: 291-296, length=12845 chars\n",
            "Chunk pages: 295-300, length=10340 chars\n",
            "Chunk pages: 299-304, length=11275 chars\n",
            "Chunk pages: 303-308, length=10101 chars\n",
            "Chunk pages: 307-312, length=12653 chars\n",
            "Chunk pages: 311-317, length=13026 chars\n",
            "Chunk pages: 316-321, length=11655 chars\n",
            "Chunk pages: 320-325, length=9942 chars\n",
            "Chunk pages: 324-330, length=11445 chars\n",
            "Chunk pages: 329-334, length=12400 chars\n",
            "Chunk pages: 333-339, length=11444 chars\n",
            "Chunk pages: 337-343, length=9823 chars\n",
            "Chunk pages: 342-347, length=8047 chars\n",
            "Chunk pages: 346-351, length=10048 chars\n",
            "Chunk pages: 350-355, length=8323 chars\n",
            "Chunk pages: 354-359, length=7353 chars\n",
            "Chunk pages: 358-363, length=9737 chars\n",
            "Chunk pages: 362-367, length=9549 chars\n",
            "Chunk pages: 366-371, length=8948 chars\n",
            "Chunk pages: 370-375, length=11163 chars\n",
            "Chunk pages: 374-380, length=11792 chars\n",
            "Chunk pages: 379-384, length=10993 chars\n",
            "Chunk pages: 383-388, length=13900 chars\n",
            "Chunk pages: 387-392, length=14084 chars\n",
            "Chunk pages: 391-396, length=12416 chars\n",
            "Chunk pages: 395-396, length=3030 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = \"ap-south-1\"\n",
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "PRICE_PER_1K = 0.000024 # USD per 1k tokens\n",
        "\n",
        "all_embeddings = []\n",
        "total_tokens = 0\n",
        "start = time.time()\n",
        "\n",
        "for d in chunked_docs:\n",
        "    body = json.dumps({\"inputText\": d[\"chunk_text\"]})\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "\n",
        "    d[\"vector_field\"] = result[\"embedding\"]\n",
        "    d[\"tokens\"] = result.get(\"inputTextTokenCount\", len(d[\"chunk_text\"].split()))\n",
        "\n",
        "    all_embeddings.append(d)\n",
        "    total_tokens += d[\"tokens\"]\n",
        "    print(f\"📄 Pages {d['page_range']}: {d['tokens']} tokens\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "cost = (total_tokens / 1000) * PRICE_PER_1K\n",
        "\n",
        "print(\"\\n⏱️ Total time:\", round(elapsed, 2), \"seconds\")\n",
        "print(\"📊 Total tokens:\", total_tokens)\n",
        "print(f\"💰 Estimated embedding cost: ${cost:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTROPiAfkyCg",
        "outputId": "eb15995c-766d-482a-d9c8-f0d9c953c5fe"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Pages 1-6: 893 tokens\n",
            "📄 Pages 5-10: 1870 tokens\n",
            "📄 Pages 9-14: 2338 tokens\n",
            "📄 Pages 13-18: 2655 tokens\n",
            "📄 Pages 17-22: 2830 tokens\n",
            "📄 Pages 21-26: 2938 tokens\n",
            "📄 Pages 25-30: 3119 tokens\n",
            "📄 Pages 29-34: 2655 tokens\n",
            "📄 Pages 33-38: 2870 tokens\n",
            "📄 Pages 37-42: 2915 tokens\n",
            "📄 Pages 41-46: 2745 tokens\n",
            "📄 Pages 45-50: 2688 tokens\n",
            "📄 Pages 49-54: 2934 tokens\n",
            "📄 Pages 53-58: 3680 tokens\n",
            "📄 Pages 57-62: 4093 tokens\n",
            "📄 Pages 61-66: 3394 tokens\n",
            "📄 Pages 65-71: 2345 tokens\n",
            "📄 Pages 70-75: 2889 tokens\n",
            "📄 Pages 74-79: 2947 tokens\n",
            "📄 Pages 78-83: 2821 tokens\n",
            "📄 Pages 82-87: 2851 tokens\n",
            "📄 Pages 86-91: 2403 tokens\n",
            "📄 Pages 90-95: 2910 tokens\n",
            "📄 Pages 94-99: 3596 tokens\n",
            "📄 Pages 98-103: 3532 tokens\n",
            "📄 Pages 102-107: 2936 tokens\n",
            "📄 Pages 106-111: 3069 tokens\n",
            "📄 Pages 110-115: 2942 tokens\n",
            "📄 Pages 114-120: 2150 tokens\n",
            "📄 Pages 119-124: 2933 tokens\n",
            "📄 Pages 123-128: 3292 tokens\n",
            "📄 Pages 127-132: 2771 tokens\n",
            "📄 Pages 131-136: 2551 tokens\n",
            "📄 Pages 135-140: 3036 tokens\n",
            "📄 Pages 139-144: 2917 tokens\n",
            "📄 Pages 143-148: 2244 tokens\n",
            "📄 Pages 147-152: 3282 tokens\n",
            "📄 Pages 151-156: 3219 tokens\n",
            "📄 Pages 155-160: 2949 tokens\n",
            "📄 Pages 159-164: 2429 tokens\n",
            "📄 Pages 163-168: 2870 tokens\n",
            "📄 Pages 167-172: 3049 tokens\n",
            "📄 Pages 171-176: 3189 tokens\n",
            "📄 Pages 175-180: 2201 tokens\n",
            "📄 Pages 179-184: 1909 tokens\n",
            "📄 Pages 183-188: 2903 tokens\n",
            "📄 Pages 187-192: 2931 tokens\n",
            "📄 Pages 191-196: 2413 tokens\n",
            "📄 Pages 195-200: 2953 tokens\n",
            "📄 Pages 199-204: 3078 tokens\n",
            "📄 Pages 203-208: 3269 tokens\n",
            "📄 Pages 207-212: 3608 tokens\n",
            "📄 Pages 211-216: 3404 tokens\n",
            "📄 Pages 215-220: 3203 tokens\n",
            "📄 Pages 219-225: 2619 tokens\n",
            "📄 Pages 224-229: 3393 tokens\n",
            "📄 Pages 228-233: 3304 tokens\n",
            "📄 Pages 232-237: 3380 tokens\n",
            "📄 Pages 236-241: 3130 tokens\n",
            "📄 Pages 240-245: 3019 tokens\n",
            "📄 Pages 244-250: 2359 tokens\n",
            "📄 Pages 249-254: 2974 tokens\n",
            "📄 Pages 253-258: 2966 tokens\n",
            "📄 Pages 257-262: 2692 tokens\n",
            "📄 Pages 261-266: 3200 tokens\n",
            "📄 Pages 265-270: 3301 tokens\n",
            "📄 Pages 269-275: 2751 tokens\n",
            "📄 Pages 274-279: 3101 tokens\n",
            "📄 Pages 278-284: 2885 tokens\n",
            "📄 Pages 283-288: 3114 tokens\n",
            "📄 Pages 287-292: 3223 tokens\n",
            "📄 Pages 291-296: 2809 tokens\n",
            "📄 Pages 295-300: 2199 tokens\n",
            "📄 Pages 299-304: 2388 tokens\n",
            "📄 Pages 303-308: 2147 tokens\n",
            "📄 Pages 307-312: 2612 tokens\n",
            "📄 Pages 311-317: 2688 tokens\n",
            "📄 Pages 316-321: 2425 tokens\n",
            "📄 Pages 320-325: 2072 tokens\n",
            "📄 Pages 324-330: 2418 tokens\n",
            "📄 Pages 329-334: 2684 tokens\n",
            "📄 Pages 333-339: 2383 tokens\n",
            "📄 Pages 337-343: 1983 tokens\n",
            "📄 Pages 342-347: 1688 tokens\n",
            "📄 Pages 346-351: 2424 tokens\n",
            "📄 Pages 350-355: 2162 tokens\n",
            "📄 Pages 354-359: 2411 tokens\n",
            "📄 Pages 358-363: 2697 tokens\n",
            "📄 Pages 362-367: 2850 tokens\n",
            "📄 Pages 366-371: 2133 tokens\n",
            "📄 Pages 370-375: 2322 tokens\n",
            "📄 Pages 374-380: 2626 tokens\n",
            "📄 Pages 379-384: 2928 tokens\n",
            "📄 Pages 383-388: 4096 tokens\n",
            "📄 Pages 387-392: 4114 tokens\n",
            "📄 Pages 391-396: 3794 tokens\n",
            "📄 Pages 395-396: 960 tokens\n",
            "\n",
            "⏱️ Total time: 35.84 seconds\n",
            "📊 Total tokens: 272035\n",
            "💰 Estimated embedding cost: $0.006529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib, re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Normalize text to avoid minor whitespace/case changes causing new IDs.\"\"\"\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower()\n",
        "\n",
        "def make_chunk_id(course_id: str, chunk_text: str) -> str:\n",
        "    \"\"\"Deterministic ID based on course_id + chunk_text.\"\"\"\n",
        "    norm = normalize_text(chunk_text)\n",
        "    raw = f\"{course_id}|{norm}\"\n",
        "    return hashlib.blake2b(raw.encode(\"utf-8\"), digest_size=16).hexdigest()\n"
      ],
      "metadata": {
        "id": "rUwMCTl4sSMI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opensearchpy.helpers import parallel_bulk\n",
        "\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "def doc_to_action(doc):\n",
        "    # ✅ Keep original ID logic: course_id + chunk_text\n",
        "    doc_id = make_chunk_id(doc[\"course_id\"], doc[\"chunk_text\"])\n",
        "\n",
        "    return {\n",
        "        \"_op_type\": \"index\",    # overwrite if exists\n",
        "        \"_index\": index_name,\n",
        "        \"_id\": doc_id,\n",
        "        \"_source\": {\n",
        "            \"chunk_text\": doc[\"chunk_text\"],\n",
        "            \"course_id\": doc[\"course_id\"],\n",
        "            \"filename\": doc[\"filename\"],\n",
        "            # Add both page_range (string) and pages (list)\n",
        "            \"page_range\": doc.get(\"page_range\"),\n",
        "            \"pages\": doc.get(\"pages\", [doc.get(\"page_no\")]),\n",
        "            \"vector_field\": doc[\"vector_field\"]   # Titan embedding (1024 floats)\n",
        "        }\n",
        "    }\n",
        "\n",
        "# build actions generator\n",
        "actions = (doc_to_action(d) for d in all_embeddings)\n",
        "\n",
        "# bulk insert\n",
        "for ok, result in parallel_bulk(client, actions, thread_count=1, chunk_size=50):\n",
        "    if not ok:\n",
        "        print(\"❌ Failed:\", result)\n",
        "\n",
        "print(\"✅ Bulk upsert finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwKHeDGFsVgo",
        "outputId": "a9591645-db38-4279-d9ba-70ce671a34cd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bulk upsert finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "def get_titan_embedding(text: str):\n",
        "    body = json.dumps({\"inputText\": text})\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "    return result[\"embedding\"], result.get(\"inputTextTokenCount\", None)\n",
        "\n",
        "query_text = \"Cost-benefit evaluation techniques\"\n",
        "query_vector, token_count = get_titan_embedding(query_text)\n",
        "\n",
        "print(f\"✅ Query embedded: {len(query_vector)}-dim vector | Tokens: {token_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWNe5hit_6U",
        "outputId": "19ecf40a-cc9d-4cbc-ed53-ce58e431b164"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Query embedded: 1024-dim vector | Tokens: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 5\n",
        "BM25_WEIGHT = 0.3\n",
        "VECTOR_WEIGHT = 0.7\n",
        "\n",
        "# --- Run BM25 ---\n",
        "bm25_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\"size\": TOP_N, \"query\": {\"match\": {\"chunk_text\": query_text}}}\n",
        ")\n",
        "bm25_hits = bm25_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Run Vector ---\n",
        "vector_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\n",
        "        \"size\": TOP_N,\n",
        "        \"query\": {\n",
        "            \"knn\": {\"vector_field\": {\"vector\": query_vector, \"k\": TOP_N}}\n",
        "        }\n",
        "    }\n",
        ")\n",
        "vector_hits = vector_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Normalize scores ---\n",
        "def normalize_scores(hits):\n",
        "    scores = [h[\"_score\"] for h in hits]\n",
        "    if not scores:\n",
        "        return {}\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if min_s == max_s:\n",
        "        return {h[\"_id\"]: 1.0 for h in hits}\n",
        "    return {h[\"_id\"]: (h[\"_score\"] - min_s) / (max_s - min_s) for h in hits}\n",
        "\n",
        "bm25_norm = normalize_scores(bm25_hits)\n",
        "vector_norm = normalize_scores(vector_hits)\n",
        "\n",
        "# --- Combine ---\n",
        "combined = {}\n",
        "for h in bm25_hits + vector_hits:\n",
        "    _id = h[\"_id\"]\n",
        "    src = h[\"_source\"]\n",
        "    bm25_s = bm25_norm.get(_id, 0.0)\n",
        "    vec_s = vector_norm.get(_id, 0.0)\n",
        "    hybrid_score = BM25_WEIGHT * bm25_s + VECTOR_WEIGHT * vec_s\n",
        "    combined[_id] = {\n",
        "        \"hybrid_score\": hybrid_score,\n",
        "        \"bm25_score\": bm25_s,\n",
        "        \"vector_score\": vec_s,\n",
        "        \"source\": src\n",
        "    }\n",
        "\n",
        "results = sorted(combined.values(), key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
        "\n",
        "# --- Print top results ---\n",
        "print(\"\\n🔎 Manual Hybrid Results:\")\n",
        "for r in results[:TOP_N]:   # ✅ only top N\n",
        "    src = r[\"source\"]\n",
        "    # prefer page_range if exists\n",
        "    page_info = src.get(\"page_range\", src.get(\"page_no\", \"N/A\"))\n",
        "    print(f\"Hybrid={r['hybrid_score']:.3f} | BM25={r['bm25_score']:.3f} | Vec={r['vector_score']:.3f}\")\n",
        "    print(f\"Pages={page_info} | Course={src.get('course_id','N/A')} | File={src.get('filename','N/A')}\")\n",
        "    print(src[\"chunk_text\"][:5], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hecRPJZwHnp",
        "outputId": "cc0c5fa2-e71a-4d36-f7de-82c6337c17eb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Manual Hybrid Results:\n",
            "Hybrid=0.888 | BM25=1.000 | Vec=0.840\n",
            "Pages=53-58 | Course=CS6022 | File=SPM.pdf\n",
            "3.4 C ...\n",
            "\n",
            "Hybrid=0.781 | BM25=0.271 | Vec=1.000\n",
            "Pages=65-71 | Course=CS6022 | File=SPM.pdf\n",
            "3.7 R ...\n",
            "\n",
            "Hybrid=0.749 | BM25=0.638 | Vec=0.797\n",
            "Pages=49-54 | Course=CS6022 | File=SPM.pdf\n",
            "Chapt ...\n",
            "\n",
            "Hybrid=0.097 | BM25=0.323 | Vec=0.000\n",
            "Pages=155-160 | Course=CS6022 | File=SPM.pdf\n",
            "7.7 E ...\n",
            "\n",
            "Hybrid=0.007 | BM25=0.000 | Vec=0.010\n",
            "Pages=61-66 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 C ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect top 5 page chunks into one string\n",
        "context_text = \"\\n\\n\".join([r[\"source\"][\"chunk_text\"] for r in results[:]])\n",
        "ques_query = query_text   # the query you used\n",
        "no = 20\n"
      ],
      "metadata": {
        "id": "PiJhquiAyjO2"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an AI assistant specialized in **question generation and analysis**.\n",
        "\n",
        "Your goals:\n",
        "1. Generate insightful and generic questions based on the given context and user query.\n",
        "   - Do not copy sentences from the context.\n",
        "   - Make questions broad and meaningful, not text-specific.\n",
        "   - Generate at least **{no} questions**.\n",
        "2. For each generated question, classify it by:\n",
        "   - **Difficulty level**:\n",
        "       * Easy → simple recall/basic understanding (1-sentence answers).\n",
        "       * Medium → requires moderate understanding, application, or 2–3 steps of reasoning.\n",
        "       * Hard → requires deep understanding, critical thinking, multi-step reasoning.\n",
        "   - **Bloom's taxonomy level**:\n",
        "       * Choose one of: Remember, Understand, Apply, Analyze, Evaluate, Create.\n",
        "\n",
        "### Input\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "User Query (Focus Topic):\n",
        "{ques_query}\n",
        "\n",
        "### Output\n",
        "Return the results in **valid JSON** format, as a list of objects.\n",
        "Each object must look like this:\n",
        "{{\n",
        "  \"question\": \"...\",\n",
        "  \"difficulty\": \"...\",\n",
        "  \"blooms_level\": \"...\"\n",
        "}}\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8M7L4Eiryjio"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "REGION = \"ap-south-1\"\n",
        "PROFILE_ARN = \"arn:aws:bedrock:ap-south-1:850146468080:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "body = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 500,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "start = time.time()\n",
        "resp = bedrock_rt.invoke_model(modelId=PROFILE_ARN, body=json.dumps(body))\n",
        "elapsed = time.time() - start\n",
        "\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "\n",
        "reply = output[\"content\"][0][\"text\"]\n",
        "\n",
        "# Usage metadata (if present)\n",
        "usage = output.get(\"usage\", {})\n",
        "input_tokens = usage.get(\"input_tokens\", \"N/A\")\n",
        "output_tokens = usage.get(\"output_tokens\", \"N/A\")\n",
        "\n",
        "print(\"✅ Generated Output:\\n\")\n",
        "print(reply)\n",
        "\n",
        "print(\"\\n📊 Stats:\")\n",
        "print(\"  Input tokens :\", input_tokens)\n",
        "print(\"  Output tokens:\", output_tokens)\n",
        "print(f\"  Time taken   : {elapsed:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PimWoLJNyjlt",
        "outputId": "516e1536-b0c9-4e97-e8b9-3f77dab8fc20"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generated Output:\n",
            "\n",
            "{\n",
            "  \"questions\": [\n",
            "    {\n",
            "      \"question\": \"What are the key limitations of using simple net profit as a method for evaluating projects?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How does the payback period method help in project selection and what are its drawbacks?\",\n",
            "      \"difficulty\": \"Easy\", \n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why might a project with lower IRR be more attractive than one with higher IRR?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What factors should be considered when selecting an appropriate discount rate for NPV calculations?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Evaluate\" \n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How does Net Present Value (NPV) account for the time value of money in project evaluation?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the key differences between Return on Investment (ROI) and Internal Rate of Return (IRR)?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why is cash flow forecasting important in project evaluation?\",\n",
            "      \"difficulty\": \"Easy\",\n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can sensitivity analysis be used to evaluate project risks?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Apply\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the main categories of benefits in cost-benefit analysis and why are some harder to quantify than others?\",\n",
            "      \"difficulty\": \"Medium\", \n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can decision trees be used to evaluate project alternatives under uncertainty?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_\n",
            "\n",
            "📊 Stats:\n",
            "  Input tokens : 22997\n",
            "  Output tokens: 500\n",
            "  Time taken   : 11.60 seconds\n"
          ]
        }
      ]
    }
  ]
}