{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW0BE7CJbbAwrNp4nV7MHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auqa5325/AUQA-colab/blob/main/AUQA_questify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-uDEQ4a38AH",
        "outputId": "0dd9ea0e-b415-4b68-88c3-c24e64d2276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install boto3 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY_ID     = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "#AWS_SESSION_TOKEN     = userdata.get(\"AWS_SESSION_TOKEN\")  # may be None\n",
        "AWS_REGION = userdata.get(\"AWS_REGION\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
        "\n",
        "print(\"‚úÖ Credentials set. Region:\", AWS_REGION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8aVrSE4ZiZ",
        "outputId": "bc768233-e5be-4b5c-e292-b3ca100c78e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Credentials set. Region: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]\n",
        "session = boto3.Session(region_name=REGION)\n",
        "bedrock         = session.client(\"bedrock\")\n",
        "bedrock_runtime = session.client(\"bedrock-runtime\")\n",
        "print(\"‚úÖ boto3 session initialized in:\", session.region_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqqWGkX4-QA",
        "outputId": "97938a45-4fdd-4934-fd4b-0442e7f0e2b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ boto3 session initialized in: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "s = boto3.Session(region_name=\"ap-south-1\")\n",
        "print(s.client(\"sts\").get_caller_identity())   # must print Account, Arn\n",
        "creds = s.get_credentials().get_frozen_credentials()\n",
        "print(\"AccessKey:\", creds.access_key[:4], \"HasToken:\", bool(creds.token))\n"
      ],
      "metadata": {
        "id": "UT2f-KtPS8C0",
        "outputId": "50ec2908-7828-47b9-9e96-004a73232aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '47d69c2b-c3ec-4e7f-836f-2c1f10053209', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY1NDg4MzQxNDA6Ujp3Y0FQd1kzUA==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Sat, 30 Aug 2025 10:13:54 GMT'}, 'RetryAttempts': 0}}\n",
            "AccessKey: AKIA HasToken: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output dict structure :\n",
        "* id ‚Üí unique identifier for this response (good for\n",
        "logging/debugging).\n",
        "\n",
        "* type ‚Üí \"message\" ‚Üí tells you this is a message object.\n",
        "\n",
        "* role ‚Üí \"assistant\" ‚Üí the speaker role (assistant vs. user).\n",
        "\n",
        "* model ‚Üí which model gave this reply (claude-3-sonnet-20240229).\n",
        "\n",
        "* content ‚Üí a list of parts that make up the response.\n",
        "\n",
        "* stop_reason ‚Üí why the model stopped (e.g., end_turn, max_tokens).\n",
        "\n",
        "* stop_sequence ‚Üí custom sequence that stopped generation (here it‚Äôs None).\n",
        "\n",
        "* usage ‚Üí token usage info (handy for cost + rate limits)."
      ],
      "metadata": {
        "id": "wbqD05a1_0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "prompt = \"explain tcp ip?\"\n",
        "\n",
        "body = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 100,\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "print(output[\"outputs\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "xKuzvoVdBegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "session = boto3.Session(region_name=\"ap-south-1\")\n",
        "sts = session.client(\"sts\")\n",
        "print(sts.get_caller_identity())  # should return Account, Arn, UserId\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5nKJfpQRoqH",
        "outputId": "6c4b9df1-8ef1-42e2-acc7-d3e67a45b3d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0318db39-9d37-4c6e-bb25-6767026e89f2', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY0NTkzNTE3NDk6UjpJd1BrYlNOVg==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Fri, 29 Aug 2025 09:22:31 GMT'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'outputs': [{'text': '\\n\\nTCP/IP is a suite of protocols that defines the Internet. Originally designed for the UNIX operating system. The TCP/IP protocol suite is a four-layer model that provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed, and received.\\n\\nThe four layers of the TCP/IP model are:\\n\\n1. The Application Layer: This is the topmost layer of the TCP/IP', 'stop_reason': 'length'}]}"
      ],
      "metadata": {
        "id": "zlZJ3onqB_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opensearch-py requests-aws4auth\n",
        "\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "from requests_aws4auth import AWS4Auth\n",
        "import boto3\n",
        "\n",
        "# --- CONFIG ---\n",
        "host   = \"https://search-test1-annauniv-pcx3f52wxykhpd4md6v4bjeqdy.ap-south-1.es.amazonaws.com\"  # OpenSearch domain endpoint\n",
        "region = \"ap-south-1\"\n",
        "service = \"es\"\n",
        "\n",
        "# --- AWS SigV4 Auth ---\n",
        "session = boto3.Session()\n",
        "credentials = session.get_credentials()\n",
        "awsauth = AWS4Auth(\n",
        "    credentials.access_key,\n",
        "    credentials.secret_key,\n",
        "    region,\n",
        "    service,\n",
        "    session_token=credentials.token\n",
        ")\n",
        "\n",
        "# --- OpenSearch Client ---\n",
        "client = OpenSearch(\n",
        "    hosts=[host],\n",
        "    http_auth=awsauth,\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    connection_class=RequestsHttpConnection\n",
        ")\n",
        "\n",
        "# --- TEST: Get cluster info ---\n",
        "print(client.info())\n"
      ],
      "metadata": {
        "id": "l5z2sIp87q1F",
        "outputId": "fd28194c-422f-4221-b070-f9d0bb23b28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/371.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m368.6/371.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m371.5/371.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h{'name': 'd69eb8cacc6e2cf42750a5f4788db327', 'cluster_name': '850146468080:test1-annauniv', 'cluster_uuid': 'qhkHo9X9Suuw8REB9uInGQ', 'version': {'distribution': 'opensearch', 'number': '2.19.0', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-07-24T06:15:41.026838036Z', 'build_snapshot': False, 'lucene_version': '9.12.1', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "index_body = {\n",
        "    \"settings\": {\n",
        "        \"index\": {\n",
        "            \"knn\": True,\n",
        "            \"knn.algo_param.ef_search\": 512,\n",
        "            \"number_of_shards\": 5,\n",
        "            \"number_of_replicas\": 1\n",
        "        }\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            # main chunk text\n",
        "            \"chunk_text\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # ids & names\n",
        "            \"course_id\": {\n",
        "                \"type\": \"keyword\"  # IDs are better stored as keyword for exact matches\n",
        "            },\n",
        "\n",
        "            \"filename\": {\n",
        "                \"type\": \"keyword\"  # filenames usually exact matches\n",
        "            },\n",
        "\n",
        "            # single-page (legacy support)\n",
        "            \"page_no\": {\"type\": \"long\"},\n",
        "\n",
        "            # NEW: page range string (e.g., \"1-6\")\n",
        "            \"page_range\": {\"type\": \"keyword\"},\n",
        "\n",
        "            # NEW: array of page numbers (e.g., [1,2,3,4,5,6])\n",
        "            \"pages\": {\"type\": \"long\"},\n",
        "\n",
        "            # vector index used for ANN similarity search\n",
        "            \"vector_field\": {\n",
        "                \"type\": \"knn_vector\",\n",
        "                \"dimension\": 1024,\n",
        "                \"method\": {\n",
        "                    \"name\": \"hnsw\",\n",
        "                    \"space_type\": \"cosinesimil\",\n",
        "                    \"engine\": \"nmslib\",\n",
        "                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# recreate index\n",
        "if client.indices.exists(index=index_name):\n",
        "    print(f\"Index '{index_name}' already exists. Deleting and recreating...\")\n",
        "    client.indices.delete(index=index_name)\n",
        "\n",
        "client.indices.create(index=index_name, body=index_body)\n",
        "print(f\"‚úÖ Index '{index_name}' created successfully!\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UcTIfVU4RS02",
        "outputId": "c6a517ed-7845-4c1f-d450-c445c75646a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nindex_name = \"test-auqa\"\\n\\nindex_body = {\\n    \"settings\": {\\n        \"index\": {\\n            \"knn\": True,\\n            \"knn.algo_param.ef_search\": 512,\\n            \"number_of_shards\": 5,\\n            \"number_of_replicas\": 1\\n        }\\n    },\\n    \"mappings\": {\\n        \"properties\": {\\n            # main chunk text\\n            \"chunk_text\": {\\n                \"type\": \"text\",\\n                \"fields\": {\\n                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\\n                }\\n            },\\n\\n            # ids & names\\n            \"course_id\": {\\n                \"type\": \"keyword\"  # IDs are better stored as keyword for exact matches\\n            },\\n           \\n            \"filename\": {\\n                \"type\": \"keyword\"  # filenames usually exact matches\\n            },\\n\\n            # single-page (legacy support)\\n            \"page_no\": {\"type\": \"long\"},\\n\\n            # NEW: page range string (e.g., \"1-6\")\\n            \"page_range\": {\"type\": \"keyword\"},\\n\\n            # NEW: array of page numbers (e.g., [1,2,3,4,5,6])\\n            \"pages\": {\"type\": \"long\"},\\n\\n            # vector index used for ANN similarity search\\n            \"vector_field\": {\\n                \"type\": \"knn_vector\",\\n                \"dimension\": 1024,\\n                \"method\": {\\n                    \"name\": \"hnsw\",\\n                    \"space_type\": \"cosinesimil\",\\n                    \"engine\": \"nmslib\",\\n                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\\n                }\\n            }\\n        }\\n    }\\n}\\n\\n# recreate index\\nif client.indices.exists(index=index_name):\\n    print(f\"Index \\'{index_name}\\' already exists. Deleting and recreating...\")\\n    client.indices.delete(index=index_name)\\n\\nclient.indices.create(index=index_name, body=index_body)\\nprint(f\"‚úÖ Index \\'{index_name}\\' created successfully!\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'test-annauniv': {'aliases': {}, 'mappings': {'properties': {'course_id': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'embedding': {'type': 'float'}, 'metadata': {'properties': {'page': {'type': 'long'}, 'source': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'page_no': {'type': 'long'}, 'text': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'vector_field': {'type': 'knn_vector', 'dimension': 1024, 'method': {'engine': 'nmslib', 'space_type': 'cosinesimil', 'name': 'hnsw', 'parameters': {'ef_construction': 512, 'm': 16}}}}}, 'settings': {'index': {'replication': {'type': 'DOCUMENT'}, 'refresh_interval': '1s', 'number_of_shards': '5', 'knn.algo_param': {'ef_search': '512'}, 'provided_name': 'test-annauniv', 'knn': 'true', 'creation_date': '1756144596121', 'number_of_replicas': '1', 'uuid': 'rumasDZaQC6xBy4G48v1KQ', 'version': {'created': '136407827'}}}}}\n",
        "\n"
      ],
      "metadata": {
        "id": "n-xX1sT3yxqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, time, re\n",
        "\n",
        "BUCKET = \"anna-univ-qna\"\n",
        "DOCUMENT = \"Textbooks/SPM.pdf\"\n",
        "REGION = \"ap-south-1\"\n",
        "COURSE_ID = \"CS6022\"   # <-- set this per textbook\n",
        "FILENAME = \"SPM.pdf\"\n",
        "\n",
        "textract = boto3.client(\"textract\", region_name=REGION)\n",
        "\n",
        "# --- Start async Textract job ---\n",
        "start = time.time()\n",
        "response = textract.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": BUCKET, \"Name\": DOCUMENT}}\n",
        ")\n",
        "print(\"FILENAME =\",FILENAME)\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"‚úÖ Started Textract JobId: {job_id}\")\n",
        "\n",
        "# --- Wait for completion ---\n",
        "while True:\n",
        "    result = textract.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"‚ùå Textract job failed!\")\n",
        "\n",
        "pages = result[\"DocumentMetadata\"][\"Pages\"]\n",
        "print(f\"üìÑ Pages: {pages}\")\n",
        "print(f\"‚è±Ô∏è Time taken: {elapsed:.2f} sec\")\n",
        "print(f\"üí∞ Estimated cost: ${(pages/1000)*1.5:.4f}\")\n",
        "\n",
        "# --- Collect page-wise text into docs with metadata ---\n",
        "page_texts = {}\n",
        "next_token = None\n",
        "\n",
        "while True:\n",
        "    if next_token:\n",
        "        result = textract.get_document_text_detection(JobId=job_id, NextToken=next_token)\n",
        "    else:\n",
        "        result = textract.get_document_text_detection(JobId=job_id)\n",
        "\n",
        "    for block in result[\"Blocks\"]:\n",
        "        if block[\"BlockType\"] == \"LINE\":\n",
        "            page_no = block[\"Page\"]\n",
        "            page_texts.setdefault(page_no, []).append(block[\"Text\"])\n",
        "\n",
        "    next_token = result.get(\"NextToken\")\n",
        "    if not next_token:\n",
        "        break\n",
        "\n",
        "# Build document objects (ready to send to Titan for embeddings)\n",
        "docs = []\n",
        "for page_no, lines in page_texts.items():\n",
        "    chunk_text = \"\\n\".join(lines)\n",
        "    doc = {\n",
        "        \"chunk_text\": chunk_text,\n",
        "        \"course_id\": COURSE_ID,\n",
        "        \"filename\": FILENAME,\n",
        "        \"page_no\": page_no\n",
        "    }\n",
        "    docs.append(doc)\n",
        "\n",
        "# --- Preview first 5 pages ---\n",
        "print(\"\\nüìë First 5 pages extracted:\")\n",
        "for d in docs[:5]:\n",
        "    print(f\"\\nPage {d['page_no']} | Course: {d['course_id']} | File: {d['filename']}\")\n",
        "    print(d[\"chunk_text\"][:400], \"...\")\n"
      ],
      "metadata": {
        "id": "DVaNNUjgqWN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "806b9018-75df-421c-b8ef-aeea73a65c2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FILENAME = SPM.pdf\n",
            "‚úÖ Started Textract JobId: ce41d655412d458abda1771970d243aee8b1fcb47f07c25ecdfed15620863448\n",
            "üìÑ Pages: 396\n",
            "‚è±Ô∏è Time taken: 231.58 sec\n",
            "üí∞ Estimated cost: $0.5940\n",
            "\n",
            "üìë First 5 pages extracted:\n",
            "\n",
            "Page 1 | Course: CS6022 | File: SPM.pdf\n",
            "BoB HUGHES AND MIKE COTTERELL\n",
            "Software\n",
            "Project\n",
            "Management Second Edition\n",
            "Tistimating\n",
            "www.mcgraw-hill.co.uk/hughes ...\n",
            "\n",
            "Page 2 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition) ...\n",
            "\n",
            "Page 3 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition)\n",
            "Bob Hughes and Mike Cotterell,\n",
            "School of Information Management, University of Brighton\n",
            "The McGraw-Hill Companies\n",
            "London\n",
            "Burr Ridge, IL\n",
            "New York\n",
            "St Louis\n",
            "San Francisco\n",
            "Auckland\n",
            "Bogot√° Caracas\n",
            "Lisbon\n",
            "Madrid\n",
            "Mexico\n",
            "Milan\n",
            "Montreal\n",
            "New Delhi\n",
            "Panama\n",
            "Paris\n",
            "San Juan\n",
            "S√£o Paulo\n",
            "Singapore\n",
            "Tokyo\n",
            "Toronto ...\n",
            "\n",
            "Page 4 | Course: CS6022 | File: SPM.pdf\n",
            "Published by\n",
            "McGraw-Hill Publishing Company\n",
            "SHOPPENHANGERS ROAD, MAIDENHEAD, BERKSHIRE, SL6 2QL, ENGLAND\n",
            "Telephone: +44(o) 1628 502500\n",
            "Fax: +44(o) 1628 770224\n",
            "Web site: http://www.megraw-hill.co.uk\n",
            "British Library Cataloguing in Publication Data\n",
            "A catalogue record for this book is available from the British Library\n",
            "ISBN 007 709505 7\n",
            "Library of Congress cataloguing in publication data\n",
            "The LOC data  ...\n",
            "\n",
            "Page 5 | Course: CS6022 | File: SPM.pdf\n",
            "The road to hell is paved with works-in-progress.\n",
            "Philip Roth ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_page_chunks(docs, chunk_size=6, overlap=2):\n",
        "    \"\"\"\n",
        "    Re-chunk page-level docs into multi-page chunks.\n",
        "    Each chunk contains `chunk_size` pages, with `overlap` pages between chunks.\n",
        "    \"\"\"\n",
        "    # Ensure pages are sorted\n",
        "    docs_sorted = sorted(docs, key=lambda x: x[\"page_no\"])\n",
        "\n",
        "    new_chunks = []\n",
        "    step = chunk_size - overlap\n",
        "    total_pages = len(docs_sorted)\n",
        "\n",
        "    for i in range(0, total_pages, step):\n",
        "        # Select pages for this chunk\n",
        "        chunk_docs = docs_sorted[i:i+chunk_size]\n",
        "        if not chunk_docs:\n",
        "            continue\n",
        "\n",
        "        # Combine texts\n",
        "        chunk_text = \"\\n\\n\".join(d[\"chunk_text\"] for d in chunk_docs)\n",
        "\n",
        "        # Metadata: first + last page\n",
        "        page_start = chunk_docs[0][\"page_no\"]\n",
        "        page_end = chunk_docs[-1][\"page_no\"]\n",
        "\n",
        "        new_chunks.append({\n",
        "            \"chunk_text\": chunk_text,\n",
        "            \"course_id\": chunk_docs[0][\"course_id\"],   # same for all\n",
        "            \"filename\": chunk_docs[0][\"filename\"],     # same file\n",
        "            \"page_range\": f\"{page_start}-{page_end}\",\n",
        "            \"pages\": [d[\"page_no\"] for d in chunk_docs]\n",
        "        })\n",
        "\n",
        "    return new_chunks\n",
        "\n",
        "# Example usage\n",
        "chunked_docs = make_page_chunks(docs, chunk_size=6, overlap=2)\n",
        "print(f\"‚úÖ Created {len(chunked_docs)} multi-page chunks\")\n",
        "for c in chunked_docs[:]:\n",
        "    print(f\"Chunk pages: {c['page_range']}, length={len(c['chunk_text'])} chars\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjcmg5jTkxyj",
        "outputId": "d3d5887a-a82d-4ae3-9228-ad3b1947d4c9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created 97 multi-page chunks\n",
            "Chunk pages: 1-6, length=3159 chars\n",
            "Chunk pages: 5-10, length=5704 chars\n",
            "Chunk pages: 9-14, length=10492 chars\n",
            "Chunk pages: 13-18, length=13310 chars\n",
            "Chunk pages: 17-22, length=14129 chars\n",
            "Chunk pages: 21-26, length=14491 chars\n",
            "Chunk pages: 25-30, length=15321 chars\n",
            "Chunk pages: 29-34, length=12528 chars\n",
            "Chunk pages: 33-38, length=13568 chars\n",
            "Chunk pages: 37-42, length=13716 chars\n",
            "Chunk pages: 41-46, length=12544 chars\n",
            "Chunk pages: 45-50, length=12730 chars\n",
            "Chunk pages: 49-54, length=14228 chars\n",
            "Chunk pages: 53-58, length=14591 chars\n",
            "Chunk pages: 57-62, length=15423 chars\n",
            "Chunk pages: 61-66, length=14447 chars\n",
            "Chunk pages: 65-71, length=10503 chars\n",
            "Chunk pages: 70-75, length=14014 chars\n",
            "Chunk pages: 74-79, length=14289 chars\n",
            "Chunk pages: 78-83, length=13774 chars\n",
            "Chunk pages: 82-87, length=13389 chars\n",
            "Chunk pages: 86-91, length=11503 chars\n",
            "Chunk pages: 90-95, length=12682 chars\n",
            "Chunk pages: 94-99, length=16610 chars\n",
            "Chunk pages: 98-103, length=15775 chars\n",
            "Chunk pages: 102-107, length=12250 chars\n",
            "Chunk pages: 106-111, length=13370 chars\n",
            "Chunk pages: 110-115, length=12789 chars\n",
            "Chunk pages: 114-120, length=9942 chars\n",
            "Chunk pages: 119-124, length=14297 chars\n",
            "Chunk pages: 123-128, length=14853 chars\n",
            "Chunk pages: 127-132, length=11457 chars\n",
            "Chunk pages: 131-136, length=10390 chars\n",
            "Chunk pages: 135-140, length=11392 chars\n",
            "Chunk pages: 139-144, length=11952 chars\n",
            "Chunk pages: 143-148, length=10381 chars\n",
            "Chunk pages: 147-152, length=15665 chars\n",
            "Chunk pages: 151-156, length=14839 chars\n",
            "Chunk pages: 155-160, length=11040 chars\n",
            "Chunk pages: 159-164, length=9761 chars\n",
            "Chunk pages: 163-168, length=11511 chars\n",
            "Chunk pages: 167-172, length=11717 chars\n",
            "Chunk pages: 171-176, length=12574 chars\n",
            "Chunk pages: 175-180, length=8531 chars\n",
            "Chunk pages: 179-184, length=9134 chars\n",
            "Chunk pages: 183-188, length=12970 chars\n",
            "Chunk pages: 187-192, length=12034 chars\n",
            "Chunk pages: 191-196, length=9644 chars\n",
            "Chunk pages: 195-200, length=13323 chars\n",
            "Chunk pages: 199-204, length=15025 chars\n",
            "Chunk pages: 203-208, length=15139 chars\n",
            "Chunk pages: 207-212, length=16705 chars\n",
            "Chunk pages: 211-216, length=16676 chars\n",
            "Chunk pages: 215-220, length=16013 chars\n",
            "Chunk pages: 219-225, length=12604 chars\n",
            "Chunk pages: 224-229, length=16003 chars\n",
            "Chunk pages: 228-233, length=15902 chars\n",
            "Chunk pages: 232-237, length=16062 chars\n",
            "Chunk pages: 236-241, length=15295 chars\n",
            "Chunk pages: 240-245, length=15118 chars\n",
            "Chunk pages: 244-250, length=11429 chars\n",
            "Chunk pages: 249-254, length=14292 chars\n",
            "Chunk pages: 253-258, length=13835 chars\n",
            "Chunk pages: 257-262, length=12882 chars\n",
            "Chunk pages: 261-266, length=15623 chars\n",
            "Chunk pages: 265-270, length=16077 chars\n",
            "Chunk pages: 269-275, length=13040 chars\n",
            "Chunk pages: 274-279, length=14645 chars\n",
            "Chunk pages: 278-284, length=13634 chars\n",
            "Chunk pages: 283-288, length=14710 chars\n",
            "Chunk pages: 287-292, length=14894 chars\n",
            "Chunk pages: 291-296, length=12845 chars\n",
            "Chunk pages: 295-300, length=10340 chars\n",
            "Chunk pages: 299-304, length=11275 chars\n",
            "Chunk pages: 303-308, length=10101 chars\n",
            "Chunk pages: 307-312, length=12653 chars\n",
            "Chunk pages: 311-317, length=13026 chars\n",
            "Chunk pages: 316-321, length=11655 chars\n",
            "Chunk pages: 320-325, length=9942 chars\n",
            "Chunk pages: 324-330, length=11445 chars\n",
            "Chunk pages: 329-334, length=12400 chars\n",
            "Chunk pages: 333-339, length=11444 chars\n",
            "Chunk pages: 337-343, length=9823 chars\n",
            "Chunk pages: 342-347, length=8047 chars\n",
            "Chunk pages: 346-351, length=10048 chars\n",
            "Chunk pages: 350-355, length=8323 chars\n",
            "Chunk pages: 354-359, length=7353 chars\n",
            "Chunk pages: 358-363, length=9737 chars\n",
            "Chunk pages: 362-367, length=9549 chars\n",
            "Chunk pages: 366-371, length=8948 chars\n",
            "Chunk pages: 370-375, length=11163 chars\n",
            "Chunk pages: 374-380, length=11792 chars\n",
            "Chunk pages: 379-384, length=10993 chars\n",
            "Chunk pages: 383-388, length=13900 chars\n",
            "Chunk pages: 387-392, length=14084 chars\n",
            "Chunk pages: 391-396, length=12416 chars\n",
            "Chunk pages: 395-396, length=3030 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = \"ap-south-1\"\n",
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "PRICE_PER_1K = 0.000024 # USD per 1k tokens\n",
        "\n",
        "all_embeddings = []\n",
        "total_tokens = 0\n",
        "start = time.time()\n",
        "\n",
        "for d in chunked_docs:\n",
        "    body = json.dumps({\"inputText\": d[\"chunk_text\"]})\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "\n",
        "    d[\"vector_field\"] = result[\"embedding\"]\n",
        "    d[\"tokens\"] = result.get(\"inputTextTokenCount\", len(d[\"chunk_text\"].split()))\n",
        "\n",
        "    all_embeddings.append(d)\n",
        "    total_tokens += d[\"tokens\"]\n",
        "    print(f\"üìÑ Pages {d['page_range']}: {d['tokens']} tokens\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "cost = (total_tokens / 1000) * PRICE_PER_1K\n",
        "\n",
        "print(\"\\n‚è±Ô∏è Total time:\", round(elapsed, 2), \"seconds\")\n",
        "print(\"üìä Total tokens:\", total_tokens)\n",
        "print(f\"üí∞ Estimated embedding cost: ${cost:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTROPiAfkyCg",
        "outputId": "eb15995c-766d-482a-d9c8-f0d9c953c5fe"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Pages 1-6: 893 tokens\n",
            "üìÑ Pages 5-10: 1870 tokens\n",
            "üìÑ Pages 9-14: 2338 tokens\n",
            "üìÑ Pages 13-18: 2655 tokens\n",
            "üìÑ Pages 17-22: 2830 tokens\n",
            "üìÑ Pages 21-26: 2938 tokens\n",
            "üìÑ Pages 25-30: 3119 tokens\n",
            "üìÑ Pages 29-34: 2655 tokens\n",
            "üìÑ Pages 33-38: 2870 tokens\n",
            "üìÑ Pages 37-42: 2915 tokens\n",
            "üìÑ Pages 41-46: 2745 tokens\n",
            "üìÑ Pages 45-50: 2688 tokens\n",
            "üìÑ Pages 49-54: 2934 tokens\n",
            "üìÑ Pages 53-58: 3680 tokens\n",
            "üìÑ Pages 57-62: 4093 tokens\n",
            "üìÑ Pages 61-66: 3394 tokens\n",
            "üìÑ Pages 65-71: 2345 tokens\n",
            "üìÑ Pages 70-75: 2889 tokens\n",
            "üìÑ Pages 74-79: 2947 tokens\n",
            "üìÑ Pages 78-83: 2821 tokens\n",
            "üìÑ Pages 82-87: 2851 tokens\n",
            "üìÑ Pages 86-91: 2403 tokens\n",
            "üìÑ Pages 90-95: 2910 tokens\n",
            "üìÑ Pages 94-99: 3596 tokens\n",
            "üìÑ Pages 98-103: 3532 tokens\n",
            "üìÑ Pages 102-107: 2936 tokens\n",
            "üìÑ Pages 106-111: 3069 tokens\n",
            "üìÑ Pages 110-115: 2942 tokens\n",
            "üìÑ Pages 114-120: 2150 tokens\n",
            "üìÑ Pages 119-124: 2933 tokens\n",
            "üìÑ Pages 123-128: 3292 tokens\n",
            "üìÑ Pages 127-132: 2771 tokens\n",
            "üìÑ Pages 131-136: 2551 tokens\n",
            "üìÑ Pages 135-140: 3036 tokens\n",
            "üìÑ Pages 139-144: 2917 tokens\n",
            "üìÑ Pages 143-148: 2244 tokens\n",
            "üìÑ Pages 147-152: 3282 tokens\n",
            "üìÑ Pages 151-156: 3219 tokens\n",
            "üìÑ Pages 155-160: 2949 tokens\n",
            "üìÑ Pages 159-164: 2429 tokens\n",
            "üìÑ Pages 163-168: 2870 tokens\n",
            "üìÑ Pages 167-172: 3049 tokens\n",
            "üìÑ Pages 171-176: 3189 tokens\n",
            "üìÑ Pages 175-180: 2201 tokens\n",
            "üìÑ Pages 179-184: 1909 tokens\n",
            "üìÑ Pages 183-188: 2903 tokens\n",
            "üìÑ Pages 187-192: 2931 tokens\n",
            "üìÑ Pages 191-196: 2413 tokens\n",
            "üìÑ Pages 195-200: 2953 tokens\n",
            "üìÑ Pages 199-204: 3078 tokens\n",
            "üìÑ Pages 203-208: 3269 tokens\n",
            "üìÑ Pages 207-212: 3608 tokens\n",
            "üìÑ Pages 211-216: 3404 tokens\n",
            "üìÑ Pages 215-220: 3203 tokens\n",
            "üìÑ Pages 219-225: 2619 tokens\n",
            "üìÑ Pages 224-229: 3393 tokens\n",
            "üìÑ Pages 228-233: 3304 tokens\n",
            "üìÑ Pages 232-237: 3380 tokens\n",
            "üìÑ Pages 236-241: 3130 tokens\n",
            "üìÑ Pages 240-245: 3019 tokens\n",
            "üìÑ Pages 244-250: 2359 tokens\n",
            "üìÑ Pages 249-254: 2974 tokens\n",
            "üìÑ Pages 253-258: 2966 tokens\n",
            "üìÑ Pages 257-262: 2692 tokens\n",
            "üìÑ Pages 261-266: 3200 tokens\n",
            "üìÑ Pages 265-270: 3301 tokens\n",
            "üìÑ Pages 269-275: 2751 tokens\n",
            "üìÑ Pages 274-279: 3101 tokens\n",
            "üìÑ Pages 278-284: 2885 tokens\n",
            "üìÑ Pages 283-288: 3114 tokens\n",
            "üìÑ Pages 287-292: 3223 tokens\n",
            "üìÑ Pages 291-296: 2809 tokens\n",
            "üìÑ Pages 295-300: 2199 tokens\n",
            "üìÑ Pages 299-304: 2388 tokens\n",
            "üìÑ Pages 303-308: 2147 tokens\n",
            "üìÑ Pages 307-312: 2612 tokens\n",
            "üìÑ Pages 311-317: 2688 tokens\n",
            "üìÑ Pages 316-321: 2425 tokens\n",
            "üìÑ Pages 320-325: 2072 tokens\n",
            "üìÑ Pages 324-330: 2418 tokens\n",
            "üìÑ Pages 329-334: 2684 tokens\n",
            "üìÑ Pages 333-339: 2383 tokens\n",
            "üìÑ Pages 337-343: 1983 tokens\n",
            "üìÑ Pages 342-347: 1688 tokens\n",
            "üìÑ Pages 346-351: 2424 tokens\n",
            "üìÑ Pages 350-355: 2162 tokens\n",
            "üìÑ Pages 354-359: 2411 tokens\n",
            "üìÑ Pages 358-363: 2697 tokens\n",
            "üìÑ Pages 362-367: 2850 tokens\n",
            "üìÑ Pages 366-371: 2133 tokens\n",
            "üìÑ Pages 370-375: 2322 tokens\n",
            "üìÑ Pages 374-380: 2626 tokens\n",
            "üìÑ Pages 379-384: 2928 tokens\n",
            "üìÑ Pages 383-388: 4096 tokens\n",
            "üìÑ Pages 387-392: 4114 tokens\n",
            "üìÑ Pages 391-396: 3794 tokens\n",
            "üìÑ Pages 395-396: 960 tokens\n",
            "\n",
            "‚è±Ô∏è Total time: 35.84 seconds\n",
            "üìä Total tokens: 272035\n",
            "üí∞ Estimated embedding cost: $0.006529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib, re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Normalize text to avoid minor whitespace/case changes causing new IDs.\"\"\"\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower()\n",
        "\n",
        "def make_chunk_id(course_id: str, chunk_text: str) -> str:\n",
        "    \"\"\"Deterministic ID based on course_id + chunk_text.\"\"\"\n",
        "    norm = normalize_text(chunk_text)\n",
        "    raw = f\"{course_id}|{norm}\"\n",
        "    return hashlib.blake2b(raw.encode(\"utf-8\"), digest_size=16).hexdigest()\n"
      ],
      "metadata": {
        "id": "rUwMCTl4sSMI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opensearchpy.helpers import parallel_bulk\n",
        "\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "def doc_to_action(doc):\n",
        "    # ‚úÖ Keep original ID logic: course_id + chunk_text\n",
        "    doc_id = make_chunk_id(doc[\"course_id\"], doc[\"chunk_text\"])\n",
        "\n",
        "    return {\n",
        "        \"_op_type\": \"index\",    # overwrite if exists\n",
        "        \"_index\": index_name,\n",
        "        \"_id\": doc_id,\n",
        "        \"_source\": {\n",
        "            \"chunk_text\": doc[\"chunk_text\"],\n",
        "            \"course_id\": doc[\"course_id\"],\n",
        "            \"filename\": doc[\"filename\"],\n",
        "            # Add both page_range (string) and pages (list)\n",
        "            \"page_range\": doc.get(\"page_range\"),\n",
        "            \"pages\": doc.get(\"pages\", [doc.get(\"page_no\")]),\n",
        "            \"vector_field\": doc[\"vector_field\"]   # Titan embedding (1024 floats)\n",
        "        }\n",
        "    }\n",
        "\n",
        "# build actions generator\n",
        "actions = (doc_to_action(d) for d in all_embeddings)\n",
        "\n",
        "# bulk insert\n",
        "for ok, result in parallel_bulk(client, actions, thread_count=1, chunk_size=50):\n",
        "    if not ok:\n",
        "        print(\"‚ùå Failed:\", result)\n",
        "\n",
        "print(\"‚úÖ Bulk upsert finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwKHeDGFsVgo",
        "outputId": "a9591645-db38-4279-d9ba-70ce671a34cd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Bulk upsert finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "def get_titan_embedding(text: str):\n",
        "    body = json.dumps({\"inputText\": text})\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "    return result[\"embedding\"], result.get(\"inputTextTokenCount\", None)\n",
        "\n",
        "query_text = \"Cost-benefit evaluation techniques\"\n",
        "query_vector, token_count = get_titan_embedding(query_text)\n",
        "\n",
        "print(f\"‚úÖ Query embedded: {len(query_vector)}-dim vector | Tokens: {token_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWNe5hit_6U",
        "outputId": "19ecf40a-cc9d-4cbc-ed53-ce58e431b164"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Query embedded: 1024-dim vector | Tokens: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 5\n",
        "BM25_WEIGHT = 0.3\n",
        "VECTOR_WEIGHT = 0.7\n",
        "\n",
        "# --- Run BM25 ---\n",
        "bm25_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\"size\": TOP_N, \"query\": {\"match\": {\"chunk_text\": query_text}}}\n",
        ")\n",
        "bm25_hits = bm25_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Run Vector ---\n",
        "vector_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\n",
        "        \"size\": TOP_N,\n",
        "        \"query\": {\n",
        "            \"knn\": {\"vector_field\": {\"vector\": query_vector, \"k\": TOP_N}}\n",
        "        }\n",
        "    }\n",
        ")\n",
        "vector_hits = vector_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Normalize scores ---\n",
        "def normalize_scores(hits):\n",
        "    scores = [h[\"_score\"] for h in hits]\n",
        "    if not scores:\n",
        "        return {}\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if min_s == max_s:\n",
        "        return {h[\"_id\"]: 1.0 for h in hits}\n",
        "    return {h[\"_id\"]: (h[\"_score\"] - min_s) / (max_s - min_s) for h in hits}\n",
        "\n",
        "bm25_norm = normalize_scores(bm25_hits)\n",
        "vector_norm = normalize_scores(vector_hits)\n",
        "\n",
        "# --- Combine ---\n",
        "combined = {}\n",
        "for h in bm25_hits + vector_hits:\n",
        "    _id = h[\"_id\"]\n",
        "    src = h[\"_source\"]\n",
        "    bm25_s = bm25_norm.get(_id, 0.0)\n",
        "    vec_s = vector_norm.get(_id, 0.0)\n",
        "    hybrid_score = BM25_WEIGHT * bm25_s + VECTOR_WEIGHT * vec_s\n",
        "    combined[_id] = {\n",
        "        \"hybrid_score\": hybrid_score,\n",
        "        \"bm25_score\": bm25_s,\n",
        "        \"vector_score\": vec_s,\n",
        "        \"source\": src\n",
        "    }\n",
        "\n",
        "results = sorted(combined.values(), key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
        "\n",
        "# --- Print top results ---\n",
        "print(\"\\nüîé Manual Hybrid Results:\")\n",
        "for r in results[:TOP_N]:   # ‚úÖ only top N\n",
        "    src = r[\"source\"]\n",
        "    # prefer page_range if exists\n",
        "    page_info = src.get(\"page_range\", src.get(\"page_no\", \"N/A\"))\n",
        "    print(f\"Hybrid={r['hybrid_score']:.3f} | BM25={r['bm25_score']:.3f} | Vec={r['vector_score']:.3f}\")\n",
        "    print(f\"Pages={page_info} | Course={src.get('course_id','N/A')} | File={src.get('filename','N/A')}\")\n",
        "    print(src[\"chunk_text\"][:5], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hecRPJZwHnp",
        "outputId": "cc0c5fa2-e71a-4d36-f7de-82c6337c17eb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Manual Hybrid Results:\n",
            "Hybrid=0.888 | BM25=1.000 | Vec=0.840\n",
            "Pages=53-58 | Course=CS6022 | File=SPM.pdf\n",
            "3.4 C ...\n",
            "\n",
            "Hybrid=0.781 | BM25=0.271 | Vec=1.000\n",
            "Pages=65-71 | Course=CS6022 | File=SPM.pdf\n",
            "3.7 R ...\n",
            "\n",
            "Hybrid=0.749 | BM25=0.638 | Vec=0.797\n",
            "Pages=49-54 | Course=CS6022 | File=SPM.pdf\n",
            "Chapt ...\n",
            "\n",
            "Hybrid=0.097 | BM25=0.323 | Vec=0.000\n",
            "Pages=155-160 | Course=CS6022 | File=SPM.pdf\n",
            "7.7 E ...\n",
            "\n",
            "Hybrid=0.007 | BM25=0.000 | Vec=0.010\n",
            "Pages=61-66 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 C ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect top 5 page chunks into one string\n",
        "context_text = \"\\n\\n\".join([r[\"source\"][\"chunk_text\"] for r in results[:]])\n",
        "ques_query = query_text   # the query you used\n",
        "no = 20\n"
      ],
      "metadata": {
        "id": "PiJhquiAyjO2"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an AI assistant specialized in **question generation and analysis**.\n",
        "\n",
        "Your goals:\n",
        "1. Generate insightful and generic questions based on the given context and user query.\n",
        "   - Do not copy sentences from the context.\n",
        "   - Make questions broad and meaningful, not text-specific.\n",
        "   - Generate at least **{no} questions**.\n",
        "2. For each generated question, classify it by:\n",
        "   - **Difficulty level**:\n",
        "       * Easy ‚Üí simple recall/basic understanding (1-sentence answers).\n",
        "       * Medium ‚Üí requires moderate understanding, application, or 2‚Äì3 steps of reasoning.\n",
        "       * Hard ‚Üí requires deep understanding, critical thinking, multi-step reasoning.\n",
        "   - **Bloom's taxonomy level**:\n",
        "       * Choose one of: Remember, Understand, Apply, Analyze, Evaluate, Create.\n",
        "\n",
        "### Input\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "User Query (Focus Topic):\n",
        "{ques_query}\n",
        "\n",
        "### Output\n",
        "Return the results in **valid JSON** format, as a list of objects.\n",
        "Each object must look like this:\n",
        "{{\n",
        "  \"question\": \"...\",\n",
        "  \"difficulty\": \"...\",\n",
        "  \"blooms_level\": \"...\"\n",
        "}}\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8M7L4Eiryjio"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "REGION = \"ap-south-1\"\n",
        "PROFILE_ARN = \"arn:aws:bedrock:ap-south-1:850146468080:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "body = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 500,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "start = time.time()\n",
        "resp = bedrock_rt.invoke_model(modelId=PROFILE_ARN, body=json.dumps(body))\n",
        "elapsed = time.time() - start\n",
        "\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "\n",
        "reply = output[\"content\"][0][\"text\"]\n",
        "\n",
        "# Usage metadata (if present)\n",
        "usage = output.get(\"usage\", {})\n",
        "input_tokens = usage.get(\"input_tokens\", \"N/A\")\n",
        "output_tokens = usage.get(\"output_tokens\", \"N/A\")\n",
        "\n",
        "print(\"‚úÖ Generated Output:\\n\")\n",
        "print(reply)\n",
        "\n",
        "print(\"\\nüìä Stats:\")\n",
        "print(\"  Input tokens :\", input_tokens)\n",
        "print(\"  Output tokens:\", output_tokens)\n",
        "print(f\"  Time taken   : {elapsed:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PimWoLJNyjlt",
        "outputId": "516e1536-b0c9-4e97-e8b9-3f77dab8fc20"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated Output:\n",
            "\n",
            "{\n",
            "  \"questions\": [\n",
            "    {\n",
            "      \"question\": \"What are the key limitations of using simple net profit as a method for evaluating projects?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How does the payback period method help in project selection and what are its drawbacks?\",\n",
            "      \"difficulty\": \"Easy\", \n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why might a project with lower IRR be more attractive than one with higher IRR?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What factors should be considered when selecting an appropriate discount rate for NPV calculations?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Evaluate\" \n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How does Net Present Value (NPV) account for the time value of money in project evaluation?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the key differences between Return on Investment (ROI) and Internal Rate of Return (IRR)?\",\n",
            "      \"difficulty\": \"Medium\",\n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why is cash flow forecasting important in project evaluation?\",\n",
            "      \"difficulty\": \"Easy\",\n",
            "      \"blooms_level\": \"Understand\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can sensitivity analysis be used to evaluate project risks?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_level\": \"Apply\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the main categories of benefits in cost-benefit analysis and why are some harder to quantify than others?\",\n",
            "      \"difficulty\": \"Medium\", \n",
            "      \"blooms_level\": \"Analyze\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How can decision trees be used to evaluate project alternatives under uncertainty?\",\n",
            "      \"difficulty\": \"Hard\",\n",
            "      \"blooms_\n",
            "\n",
            "üìä Stats:\n",
            "  Input tokens : 22997\n",
            "  Output tokens: 500\n",
            "  Time taken   : 11.60 seconds\n"
          ]
        }
      ]
    }
  ]
}