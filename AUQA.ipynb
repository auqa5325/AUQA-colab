{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM98MEH4C29PqUzSGq8iS5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auqa5325/AUQA-colab/blob/main/AUQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-uDEQ4a38AH",
        "outputId": "667b2462-ea95-4c45-f470-01b03d317664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install boto3 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY_ID     = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "#AWS_SESSION_TOKEN     = userdata.get(\"AWS_SESSION_TOKEN\")  # may be None\n",
        "AWS_REGION = userdata.get(\"AWS_REGION\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
        "\n",
        "print(\"✅ Credentials set. Region:\", AWS_REGION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8aVrSE4ZiZ",
        "outputId": "7dc80e01-61c1-41dc-8e2d-5f50717fef05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Credentials set. Region: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]\n",
        "session = boto3.Session(region_name=REGION)\n",
        "bedrock         = session.client(\"bedrock\")\n",
        "bedrock_runtime = session.client(\"bedrock-runtime\")\n",
        "print(\"✅ boto3 session initialized in:\", session.region_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqqWGkX4-QA",
        "outputId": "29edf02a-9a73-4ae9-ebd5-02c6bb6f1210"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ boto3 session initialized in: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output dict structure :\n",
        "* id → unique identifier for this response (good for\n",
        "logging/debugging).\n",
        "\n",
        "* type → \"message\" → tells you this is a message object.\n",
        "\n",
        "* role → \"assistant\" → the speaker role (assistant vs. user).\n",
        "\n",
        "* model → which model gave this reply (claude-3-sonnet-20240229).\n",
        "\n",
        "* content → a list of parts that make up the response.\n",
        "\n",
        "* stop_reason → why the model stopped (e.g., end_turn, max_tokens).\n",
        "\n",
        "* stop_sequence → custom sequence that stopped generation (here it’s None).\n",
        "\n",
        "* usage → token usage info (handy for cost + rate limits)."
      ],
      "metadata": {
        "id": "wbqD05a1_0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "prompt = \"Hello, can you briefly introduce yourself?\"\n",
        "\n",
        "body = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 100,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "reply = output[\"content\"][0][\"text\"]\n",
        "usage = output.get(\"usage\", {})\n",
        "input_tokens = usage.get(\"input_tokens\")\n",
        "output_tokens = usage.get(\"output_tokens\")\n",
        "print(\"Prompt:\\n\",prompt)\n",
        "print(\"Reply:\\n\", reply)\n",
        "print(\"\\n📊 Usage:\")\n",
        "print(\"  Input tokens :\", input_tokens)\n",
        "print(\"  Output tokens:\", output_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "dXl1upt35CqY",
        "outputId": "c886ad93-0ecd-46c6-b892-4669c24ec472"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "An error occurred (UnrecognizedClientException) when calling the InvokeModel operation: The security token included in the request is invalid.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2471995210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbedrock_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 )\n\u001b[1;32m    601\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botocore/context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1076\u001b[0m             ) or error_info.get(\"Code\")\n\u001b[1;32m   1077\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: An error occurred (UnrecognizedClientException) when calling the InvokeModel operation: The security token included in the request is invalid."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "prompt = \"explain tcp ip?\"\n",
        "\n",
        "body = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 100,\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "print(output[\"outputs\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "xKuzvoVdBegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'outputs': [{'text': '\\n\\nTCP/IP is a suite of protocols that defines the Internet. Originally designed for the UNIX operating system. The TCP/IP protocol suite is a four-layer model that provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed, and received.\\n\\nThe four layers of the TCP/IP model are:\\n\\n1. The Application Layer: This is the topmost layer of the TCP/IP', 'stop_reason': 'length'}]}"
      ],
      "metadata": {
        "id": "zlZJ3onqB_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opensearch-py requests-aws4auth\n",
        "\n",
        "\n",
        "import os, boto3, urllib.parse\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "from requests_aws4auth import AWS4Auth\n",
        "\n",
        "OPENSEARCH_URL = \"https://search-test1-annauniv-pcx3f52wxykhpd4md6v4bjeqdy.ap-south-1.es.amazonaws.com\"\n",
        "INDEX_NAME     = \"test-annauniv\"\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]  # you already set this above\n",
        "host = urllib.parse.urlparse(OPENSEARCH_URL).netloc\n",
        "\n",
        "session = boto3.Session(region_name=REGION)\n",
        "creds   = session.get_credentials().get_frozen_credentials()\n",
        "awsauth = AWS4Auth(creds.access_key, creds.secret_key, REGION, \"es\", session_token=creds.token)\n",
        "\n",
        "os_client = OpenSearch(\n",
        "    hosts=[{\"host\": host, \"port\": 443}],\n",
        "    http_auth=awsauth,\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    connection_class=RequestsHttpConnection,\n",
        "    timeout=60,                 # per-request socket timeout (default ~10s)\n",
        "    max_retries=3,\n",
        "    retry_on_timeout=True,\n",
        ")\n",
        "\n",
        "print(\"Ping:\", os_client.ping())\n"
      ],
      "metadata": {
        "id": "l5z2sIp87q1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get index settings + mappings\n",
        "info = os_client.indices.get(index=INDEX_NAME)\n",
        "print(info)\n",
        "\n"
      ],
      "metadata": {
        "id": "UcTIfVU4RS02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch all documents (up to 10 by default)\n",
        "res = os_client.search(index=INDEX_NAME, body={\"query\": {\"match_all\": {}}})\n",
        "for hit in res[\"hits\"][\"hits\"]:\n",
        "    print(hit[\"_id\"], hit[\"_source\"])\n"
      ],
      "metadata": {
        "id": "APEgJaPdRof0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List\n",
        "\n",
        "# Titan Text Embeddings v2 (1024 dims)\n",
        "EMBED_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "def embed_text(text: str) -> List[float]:\n",
        "    payload = {\n",
        "        \"inputText\": text,\n",
        "        # \"dimensions\": 1024,  # optional for v2; uncomment if you want to be explicit\n",
        "    }\n",
        "    resp = bedrock_runtime.invoke_model(\n",
        "        modelId=EMBED_MODEL_ID,\n",
        "        body=json.dumps(payload),\n",
        "        accept=\"application/json\",\n",
        "        contentType=\"application/json\",\n",
        "\n",
        "    )\n",
        "    body = json.loads(resp[\"body\"].read())\n",
        "    # Titan v2 returns {\"embedding\": [floats...]}\n",
        "    vec = body[\"embedding\"]\n",
        "    if len(vec) != 1024:\n",
        "        raise ValueError(f\"Expected 1024-dim vector, got {len(vec)}\")\n",
        "    return vec\n"
      ],
      "metadata": {
        "id": "C2dnLZvjTlA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_ID = \"q-001\"\n",
        "DOC_TEXT = \"Explain convolutional neural networks used in image recognition.\"\n",
        "\n",
        "vec = embed_text(DOC_TEXT)\n",
        "\n",
        "doc = {\n",
        "    \"text\": DOC_TEXT,\n",
        "    \"vector_field\": vec,\n",
        "    # optional extras to fit your mapping:\n",
        "    \"course_id\": \"CS8791\",\n",
        "    \"page_no\": 1,\n",
        "    \"metadata\": {\"source\": \"notes.md\", \"page\": 1},\n",
        "}\n",
        "\n",
        "resp = os_client.index(index=INDEX_NAME, id=DOC_ID, body=doc, refresh=True)\n",
        "print(\"Indexed:\", resp[\"result\"])\n"
      ],
      "metadata": {
        "id": "316VQd01ToPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_TEXT = \"Explain CNNs in image recognition\"\n",
        "qvec = embed_text(QUERY_TEXT)\n",
        "\n",
        "hybrid_query = {\n",
        "    \"size\": 5,\n",
        "    \"query\": {\n",
        "        \"bool\": {\n",
        "            \"should\": [\n",
        "                {   # semantic part\n",
        "                    \"knn\": {\n",
        "                        \"vector_field\": {\n",
        "                            \"vector\": qvec,\n",
        "                            \"k\": 5,\n",
        "                           # \"num_candidates\": 100\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                {   # keyword part\n",
        "                    \"match\": {\n",
        "                        \"text\": QUERY_TEXT\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "res = os_client.search(index=INDEX_NAME, body=hybrid_query)\n",
        "for h in res[\"hits\"][\"hits\"]:\n",
        "    print(round(h[\"_score\"],3), \"→\", h[\"_source\"].get(\"text\",\"\")[:80])\n"
      ],
      "metadata": {
        "id": "XFn4KQ5pT_Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema =  {\n",
        "  \"settings\": {\n",
        "    \"number_of_shards\": 5,\n",
        "    \"number_of_replicas\": 1,\n",
        "    \"index\": {\n",
        "      \"knn\": True,\n",
        "      \"knn.algo_param.ef_search\": 512\n",
        "    }\n",
        "  },\n",
        "  \"mappings\": {\n",
        "    \"properties\": {\n",
        "      \"course_id\": {\n",
        "        \"type\": \"text\",\n",
        "        \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } }\n",
        "      },\n",
        "      \"text\": {\n",
        "        \"type\": \"text\",\n",
        "        \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } }\n",
        "      },\n",
        "      \"page_no\": { \"type\": \"long\" },\n",
        "      \"metadata\": {\n",
        "        \"properties\": {\n",
        "          \"source\": {\n",
        "            \"type\": \"text\",\n",
        "            \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } }\n",
        "          },\n",
        "          \"page\": { \"type\": \"long\" }\n",
        "        }\n",
        "      },\n",
        "      \"embedding\": { \"type\": \"float\" },\n",
        "      \"vector_field\": {\n",
        "        \"type\": \"knn_vector\",\n",
        "        \"dimension\": 1024,\n",
        "        \"method\": {\n",
        "          \"name\": \"hnsw\",\n",
        "          \"engine\": \"nmslib\",\n",
        "          \"space_type\": \"cosinesimil\",\n",
        "          \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "  # paste the JSON above as a Python dict\n",
        "\n",
        "\n",
        "os_client.indices.create(index=INDEX_NAME, body=schema)\n",
        "print(\"Created:\", INDEX_NAME)\n",
        "\n"
      ],
      "metadata": {
        "id": "KpwBhnOxV91Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlgLT-roW8zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, numpy as np\n",
        "\n",
        "EMBED_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "def embed_text(text: str):\n",
        "    payload = {\"inputText\": text}\n",
        "    resp = bedrock_runtime.invoke_model(\n",
        "        modelId=EMBED_MODEL_ID,\n",
        "        body=json.dumps(payload),\n",
        "        accept=\"application/json\",\n",
        "        contentType=\"application/json\",\n",
        "    )\n",
        "    body = json.loads(resp[\"body\"].read())\n",
        "    vec = body[\"embedding\"]\n",
        "    if len(vec) != 1024:\n",
        "        raise ValueError(f\"Expected 1024 dims, got {len(vec)}\")\n",
        "    return vec\n",
        "\n",
        "def l2_normalize(vec):\n",
        "    v = np.array(vec, dtype=\"float32\")\n",
        "    n = np.linalg.norm(v)\n",
        "    return (v if n == 0 else v / n).tolist()\n",
        "\n",
        "def embed_text_norm(text: str):\n",
        "    return l2_normalize(embed_text(text))\n"
      ],
      "metadata": {
        "id": "QbFanZOtY9Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "from google.colab import files\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def read_txt(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def read_pdf_pages(path: str):\n",
        "    reader = PdfReader(path)\n",
        "    pages = []\n",
        "    for i, p in enumerate(reader.pages, start=1):\n",
        "        try:\n",
        "            t = p.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            t = \"\"\n",
        "        pages.append((i, t))\n",
        "    return pages\n",
        "\n",
        "print(\"Select files (.pdf or .txt)\")\n",
        "uploaded = files.upload()  # UI prompt\n",
        "print(\"Uploaded:\", list(uploaded.keys()))\n"
      ],
      "metadata": {
        "id": "y0VFrVeXY-yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Tuple\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s.replace(\"\\x00\", \" \")\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def chunk_text(text: str, chunk_size=900, overlap=150) -> List[str]:\n",
        "    text = clean_text(text)\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(n, start + chunk_size)\n",
        "        # try to cut at a sentence boundary near the end\n",
        "        window = text[start:end]\n",
        "        cut = window.rfind(\". \")\n",
        "        if cut == -1 or cut < int(0.6 * len(window)):\n",
        "            cut = len(window)\n",
        "        chunk = window[:cut].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        start = start + max(1, cut - overlap)\n",
        "    return chunks\n",
        "\n",
        "def chunk_pdf_pages(pages: List[Tuple[int, str]], chunk_size=900, overlap=150):\n",
        "    out = []\n",
        "    for page_no, text in pages:\n",
        "        for ch in chunk_text(text, chunk_size, overlap):\n",
        "            out.append((page_no, ch))\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "MG8Afq-mZRpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "from opensearchpy.helpers import bulk\n",
        "\n",
        "def bulk_index_chunks(index_name: str, chunks: List[Tuple[str, str, int, str]]):\n",
        "    \"\"\"\n",
        "    chunks: list of tuples (text, course_id, page_no, source)\n",
        "    \"\"\"\n",
        "    def gen():\n",
        "        for text, course_id, page_no, source in chunks:\n",
        "            if not text.strip():\n",
        "                continue\n",
        "            vec = embed_text_norm(text)\n",
        "            yield {\n",
        "                \"_op_type\": \"index\",\n",
        "                \"_index\": index_name,\n",
        "                \"_id\": str(uuid4()),\n",
        "                \"_source\": {\n",
        "                    \"text\": text,\n",
        "                    \"course_id\": course_id,\n",
        "                    \"page_no\": page_no,\n",
        "                    \"metadata\": {\"source\": source, \"page\": page_no},\n",
        "                    \"vector_field\": vec\n",
        "                }\n",
        "            }\n",
        "    ok, fail = bulk(os_client, gen(), chunk_size=200, request_timeout=120)\n",
        "    os_client.indices.refresh(index=index_name)\n",
        "    return ok, fail\n"
      ],
      "metadata": {
        "id": "ZLDQSa3SZVjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure this per your dataset\n",
        "DEFAULT_COURSE_ID = \"DSA\"\n",
        "\n",
        "ingest_items = []  # (text, course_id, page_no, source)\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    path = f\"/content/{fname}\"\n",
        "    if fname.lower().endswith(\".pdf\"):\n",
        "        pages = read_pdf_pages(path)\n",
        "        for page_no, txt in chunk_pdf_pages(pages):\n",
        "            ingest_items.append((txt, DEFAULT_COURSE_ID, page_no, fname))\n",
        "    elif fname.lower().endswith(\".txt\"):\n",
        "        txt = read_txt(path)\n",
        "        for i, ch in enumerate(chunk_text(txt), start=1):\n",
        "            ingest_items.append((ch, DEFAULT_COURSE_ID, i, fname))\n",
        "    else:\n",
        "        print(\"Skipping unsupported:\", fname)\n",
        "\n",
        "print(\"Chunks prepared:\", len(ingest_items))\n",
        "#ok, fail = bulk_index_chunks(INDEX_NAME, ingest_items)\n",
        "#print(f\"Indexed ok={ok}, failed={fail}\")\n"
      ],
      "metadata": {
        "id": "XoQ-NQMnZrCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import time, random, concurrent.futures as cf\n",
        "\n",
        "def make_id(source: str, page_no: int, text: str) -> str:\n",
        "    raw = f\"{source}-{page_no}-{text}\".encode(\"utf-8\")\n",
        "    return hashlib.sha1(raw).hexdigest()\n",
        "def enable_bulk_mode(index):\n",
        "    os_client.indices.put_settings(index=index, body={\"index\": {\n",
        "        \"refresh_interval\": \"-1\", \"number_of_replicas\": 0\n",
        "    }})\n",
        "\n",
        "def disable_bulk_mode(index, replicas=1):\n",
        "    os_client.indices.put_settings(index=index, body={\"index\": {\n",
        "        \"refresh_interval\": \"1s\", \"number_of_replicas\": replicas\n",
        "    }})\n",
        "    os_client.indices.refresh(index=index)\n",
        "\n",
        "\n",
        "def _with_retry(fn, max_tries=5, base=0.5, cap=8.0):\n",
        "    delay = base\n",
        "    for i in range(max_tries):\n",
        "        try:\n",
        "            return fn()\n",
        "        except Exception:\n",
        "            if i == max_tries - 1: raise\n",
        "            time.sleep(delay + random.random()*0.2)\n",
        "            delay = min(delay*2, cap)\n",
        "\n",
        "def embed_many(texts, max_workers=8):\n",
        "    def one(t): return _with_retry(lambda: embed_text_norm(t))\n",
        "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        return list(ex.map(one, texts))\n"
      ],
      "metadata": {
        "id": "H7rklKhweoif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "from opensearchpy.helpers import bulk\n",
        "import time\n",
        "\n",
        "def stream_bulk(index_name, items_iter, batch_size=64, os_chunk_size=1000, workers=8):\n",
        "    \"\"\"\n",
        "    items_iter yields tuples: (text, course_id, page_no, source)\n",
        "    \"\"\"\n",
        "    total_ok = total_fail = 0\n",
        "    seen = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    enable_bulk_mode(index_name)\n",
        "    try:\n",
        "        batch = []\n",
        "        for itm in items_iter:\n",
        "            batch.append(itm)\n",
        "            if len(batch) >= batch_size:\n",
        "                total_ok, total_fail, seen = _flush_batch(\n",
        "                    index_name, batch, total_ok, total_fail, seen,\n",
        "                    os_chunk_size=os_chunk_size, workers=workers, t0=t0\n",
        "                )\n",
        "                batch = []\n",
        "        if batch:\n",
        "            total_ok, total_fail, seen = _flush_batch(\n",
        "                index_name, batch, total_ok, total_fail, seen,\n",
        "                os_chunk_size=os_chunk_size, workers=workers, t0=t0\n",
        "            )\n",
        "    finally:\n",
        "        disable_bulk_mode(index_name, replicas=1)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"\\nDone. ok={total_ok}, failed={total_fail}, elapsed={elapsed:.1f}s, rate={seen/elapsed:.1f} docs/s\")\n",
        "    return total_ok, total_fail\n",
        "\n",
        "def _flush_batch(index_name, batch, total_ok, total_fail, seen, *, os_chunk_size, workers, t0):\n",
        "    texts  = [t for (t,_,_,_) in batch]\n",
        "    vecs   = embed_many(texts, max_workers=workers)\n",
        "\n",
        "    actions = []\n",
        "    for (text, course_id, page_no, source), vec in zip(batch, vecs):\n",
        "        if not text.strip():\n",
        "            continue\n",
        "        actions.append({\n",
        "            \"_op_type\": \"index\",\n",
        "            \"_index\": index_name,\n",
        "            \"_id\": make_id(source, page_no, text),  # deterministic ID (if you added make_id)\n",
        "            \"_source\": {\n",
        "                \"text\": text,\n",
        "                \"course_id\": course_id,\n",
        "                \"page_no\": page_no,\n",
        "                \"metadata\": {\"source\": source, \"page\": page_no},\n",
        "                \"vector_field\": vec\n",
        "            }\n",
        "        })\n",
        "\n",
        "    ok, fail = 0, 0\n",
        "    if actions:\n",
        "        # errors is a LIST; don't try to += it into an int\n",
        "        ok, errors = bulk(\n",
        "            os_client,\n",
        "            actions,\n",
        "            chunk_size=os_chunk_size,\n",
        "            request_timeout=300,\n",
        "            raise_on_error=False,      # collect errors instead of raising\n",
        "        )\n",
        "        fail = len(errors) if isinstance(errors, list) else int(errors or 0)\n",
        "        if fail:\n",
        "            # optional: peek at a couple of errors for visibility\n",
        "            print(\"Sample errors:\", errors[:2])\n",
        "\n",
        "    seen += len(actions)\n",
        "    total_ok   += ok\n",
        "    total_fail += fail\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    rate = (seen/elapsed) if elapsed > 0 else 0\n",
        "    print(f\"seen={seen} ok+={ok} fail+={fail} elapsed={elapsed:.1f}s rate≈{rate:.1f} docs/s\")\n",
        "    return total_ok, total_fail, seen\n",
        "\n",
        "def iter_ingest_items(ingest_items):\n",
        "    for it in ingest_items:\n",
        "        yield it\n",
        "\n",
        "# tune these 3 knobs to match your limits:\n",
        "OK, FAIL = stream_bulk(\n",
        "    INDEX_NAME,\n",
        "    items_iter=iter_ingest_items(ingest_items),\n",
        "    batch_size=96,\n",
        "    os_chunk_size=500,     # see (B) below\n",
        "    workers=12\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "0y6W9LwUf_7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_search(query_text: str, k=5,  course_id: str=None):\n",
        "    qvec = embed_text_norm(query_text)\n",
        "\n",
        "    must_clause = [\n",
        "        {\"knn\": {\"vector_field\": {\"vector\": qvec, \"k\": k}}}\n",
        "    ]\n",
        "    should_clause = [\n",
        "        {\"match\": {\"text\": {\"query\": query_text, \"boost\": 2.0}}}  # keyword gets a little boost\n",
        "    ]\n",
        "\n",
        "    body = {\n",
        "        \"size\": k,\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": must_clause,\n",
        "                \"should\": should_clause,\n",
        "                \"minimum_should_match\": 0,\n",
        "                \"filter\": []\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    if course_id:\n",
        "        body[\"query\"][\"bool\"][\"filter\"].append({\"term\": {\"course_id.keyword\": course_id}})\n",
        "\n",
        "    res = os_client.search(index=INDEX_NAME, body=body)\n",
        "    return [\n",
        "        (round(h[\"_score\"], 3),\n",
        "         h[\"_source\"].get(\"text\",\"\")[:].replace(\"\\n\",\" \"),\n",
        "         h[\"_source\"].get(\"metadata\",{}).get(\"source\"),\n",
        "         h[\"_source\"].get(\"metadata\",{}).get(\"page\"),\n",
        "         h[\"_id\"])\n",
        "        for h in res[\"hits\"][\"hits\"]\n",
        "    ]\n",
        "\n",
        "# Try it:\n",
        "results = hybrid_search(\"Heap Sort Algorithm\", k=10, course_id=DEFAULT_COURSE_ID)\n",
        "for r in results:\n",
        "    print(r)\n"
      ],
      "metadata": {
        "id": "xXpt3xJpa381"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}