{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMMBC4pO9/+YYoR8p1RT+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auqa5325/AUQA-colab/blob/main/AUQA_questify_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-uDEQ4a38AH",
        "outputId": "e273e5cd-2391-4269-9334-a3f8ccd0da87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install boto3  -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY_ID     = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
        "#AWS_SESSION_TOKEN     = userdata.get(\"AWS_SESSION_TOKEN\")  # may be None\n",
        "AWS_REGION = userdata.get(\"AWS_REGION\")\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
        "\n",
        "print(\"✅ Credentials set. Region:\", AWS_REGION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE8aVrSE4ZiZ",
        "outputId": "e168c658-d32d-45ab-88d5-d1a4037f61a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Credentials set. Region: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "\n",
        "REGION = os.environ[\"AWS_REGION\"]\n",
        "session = boto3.Session(region_name=REGION)\n",
        "bedrock         = session.client(\"bedrock\")\n",
        "bedrock_runtime = session.client(\"bedrock-runtime\")\n",
        "print(\"✅ boto3 session initialized in:\", session.region_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqqWGkX4-QA",
        "outputId": "eaffecc2-7091-4697-c772-d9f67a33c00b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ boto3 session initialized in: ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "s = boto3.Session(region_name=\"ap-south-1\")\n",
        "print(s.client(\"sts\").get_caller_identity())   # must print Account, Arn\n",
        "creds = s.get_credentials().get_frozen_credentials()\n",
        "print(\"AccessKey:\", creds.access_key[:4], \"HasToken:\", bool(creds.token))\n"
      ],
      "metadata": {
        "id": "UT2f-KtPS8C0",
        "outputId": "b6e114f0-f0c9-4b40-d496-60f7db53a337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UserId': 'AIDA4L4FNUDYI73ZUG6BD', 'Account': '850146468080', 'Arn': 'arn:aws:iam::850146468080:user/Anna_university_L1', 'ResponseMetadata': {'RequestId': '7b70a4bc-d577-4030-a192-95684775eefc', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '7b70a4bc-d577-4030-a192-95684775eefc', 'x-amz-sts-extended-request-id': 'MTphcC1zb3V0aC0xOjE3NTY4MDYwNTcwMDQ6UjpJSHdLdkFHWA==', 'content-type': 'text/xml', 'content-length': '415', 'date': 'Tue, 02 Sep 2025 09:40:57 GMT'}, 'RetryAttempts': 0}}\n",
            "AccessKey: AKIA HasToken: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output dict structure :\n",
        "* id → unique identifier for this response (good for\n",
        "logging/debugging).\n",
        "\n",
        "* type → \"message\" → tells you this is a message object.\n",
        "\n",
        "* role → \"assistant\" → the speaker role (assistant vs. user).\n",
        "\n",
        "* model → which model gave this reply (claude-3-sonnet-20240229).\n",
        "\n",
        "* content → a list of parts that make up the response.\n",
        "\n",
        "* stop_reason → why the model stopped (e.g., end_turn, max_tokens).\n",
        "\n",
        "* stop_sequence → custom sequence that stopped generation (here it’s None).\n",
        "\n",
        "* usage → token usage info (handy for cost + rate limits)."
      ],
      "metadata": {
        "id": "wbqD05a1_0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "prompt = \"explain RAG in GENAI\"\n",
        "\n",
        "body = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 540,\n",
        "}\n",
        "\n",
        "resp = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(body))\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "print(output[\"outputs\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "xKuzvoVdBegK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80d5690-9459-46af-9f57-9c7b550803b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "RAG stands for Recombination-Activating Genes, which are critical components of the adaptive immune system in jawed vertebrates. The RAG genes, RAG1 and RAG2, encode proteins that initiate V(D)J recombination, a process that generates diversity in the antigen receptor genes of T and B lymphocytes.\n",
            "\n",
            "During V(D)J recombination, the RAG proteins recognize and cleave specific DNA sequences called recombination signal sequences (RSSs) that flank gene segments in the antigen receptor loci. This cleavage event creates double-strand breaks, which are then processed by other proteins to form a diverse repertoire of antigen receptors.\n",
            "\n",
            "The RAG proteins are essential for the development and function of the adaptive immune system, and mutations in the RAG genes can lead to severe combined immunodeficiency (SCID) in humans. SCID is a rare genetic disorder characterized by the absence or dysfunction of T and B lymphocytes, leading to a profound immunodeficiency that leaves affected individuals susceptible to opportunistic infections.\n",
            "\n",
            "In the context of generative AI, RAG may refer to a specific type of language model that incorporates the RAG proteins' ability to generate diverse sequences by recombining and editing existing sequences. This approach could potentially be used to generate more diverse and creative text outputs in natural language processing applications. However, it is important to note that this is a speculative application, and there is currently no established generative AI model that incorporates the RAG proteins' functionality in this way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'outputs': [{'text': '\\n\\nTCP/IP is a suite of protocols that defines the Internet. Originally designed for the UNIX operating system. The TCP/IP protocol suite is a four-layer model that provides end-to-end connectivity specifying how data should be packetized, addressed, transmitted, routed, and received.\\n\\nThe four layers of the TCP/IP model are:\\n\\n1. The Application Layer: This is the topmost layer of the TCP/IP', 'stop_reason': 'length'}]}"
      ],
      "metadata": {
        "id": "zlZJ3onqB_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opensearch-py requests-aws4auth\n",
        "\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "from requests_aws4auth import AWS4Auth\n",
        "import boto3\n",
        "\n",
        "# --- CONFIG ---\n",
        "host   = \"https://search-test1-annauniv-pcx3f52wxykhpd4md6v4bjeqdy.ap-south-1.es.amazonaws.com\"  # OpenSearch domain endpoint\n",
        "region = \"ap-south-1\"\n",
        "service = \"es\"\n",
        "\n",
        "# --- AWS SigV4 Auth ---\n",
        "session = boto3.Session()\n",
        "credentials = session.get_credentials()\n",
        "awsauth = AWS4Auth(\n",
        "    credentials.access_key,\n",
        "    credentials.secret_key,\n",
        "    region,\n",
        "    service,\n",
        "    session_token=credentials.token\n",
        ")\n",
        "\n",
        "# --- OpenSearch Client ---\n",
        "client = OpenSearch(\n",
        "    hosts=[host],\n",
        "    http_auth=awsauth,\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    connection_class=RequestsHttpConnection\n",
        ")\n",
        "\n",
        "# --- TEST: Get cluster info ---\n",
        "print(client.info())\n"
      ],
      "metadata": {
        "id": "l5z2sIp87q1F",
        "outputId": "7b13436c-683e-479a-827b-625d8c9c0cba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/371.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m368.6/371.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.5/371.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h{'name': 'd69eb8cacc6e2cf42750a5f4788db327', 'cluster_name': '850146468080:test1-annauniv', 'cluster_uuid': 'qhkHo9X9Suuw8REB9uInGQ', 'version': {'distribution': 'opensearch', 'number': '2.19.0', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-07-24T06:15:41.026838036Z', 'build_snapshot': False, 'lucene_version': '9.12.1', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "index_body = {\n",
        "    \"settings\": {\n",
        "        \"index\": {\n",
        "            \"knn\": True,\n",
        "            \"knn.algo_param.ef_search\": 512,\n",
        "            \"number_of_shards\": 5,\n",
        "            \"number_of_replicas\": 1\n",
        "        }\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            # main chunk text\n",
        "            \"chunk_text\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # ids & names\n",
        "            \"course_id\": {\n",
        "                \"type\": \"keyword\"  # IDs are better stored as keyword for exact matches\n",
        "            },\n",
        "\n",
        "            \"filename\": {\n",
        "                \"type\": \"keyword\"  # filenames usually exact matches\n",
        "            },\n",
        "\n",
        "            # single-page (legacy support)\n",
        "            \"page_no\": {\"type\": \"long\"},\n",
        "\n",
        "            # NEW: page range string (e.g., \"1-6\")\n",
        "            \"page_range\": {\"type\": \"keyword\"},\n",
        "\n",
        "            # NEW: array of page numbers (e.g., [1,2,3,4,5,6])\n",
        "            \"pages\": {\"type\": \"long\"},\n",
        "\n",
        "            # vector index used for ANN similarity search\n",
        "            \"vector_field\": {\n",
        "                \"type\": \"knn_vector\",\n",
        "                \"dimension\": 1024,\n",
        "                \"method\": {\n",
        "                    \"name\": \"hnsw\",\n",
        "                    \"space_type\": \"cosinesimil\",\n",
        "                    \"engine\": \"nmslib\",\n",
        "                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# recreate index\n",
        "if client.indices.exists(index=index_name):\n",
        "    print(f\"Index '{index_name}' already exists.\")\n",
        "    #client.indices.delete(index=index_name)\n",
        "\n",
        "client.indices.create(index=index_name, body=index_body)\n",
        "print(f\"✅ Index '{index_name}' created successfully!\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UcTIfVU4RS02",
        "outputId": "c6a517ed-7845-4c1f-d450-c445c75646a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nindex_name = \"test-auqa\"\\n\\nindex_body = {\\n    \"settings\": {\\n        \"index\": {\\n            \"knn\": True,\\n            \"knn.algo_param.ef_search\": 512,\\n            \"number_of_shards\": 5,\\n            \"number_of_replicas\": 1\\n        }\\n    },\\n    \"mappings\": {\\n        \"properties\": {\\n            # main chunk text\\n            \"chunk_text\": {\\n                \"type\": \"text\",\\n                \"fields\": {\\n                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\\n                }\\n            },\\n\\n            # ids & names\\n            \"course_id\": {\\n                \"type\": \"keyword\"  # IDs are better stored as keyword for exact matches\\n            },\\n           \\n            \"filename\": {\\n                \"type\": \"keyword\"  # filenames usually exact matches\\n            },\\n\\n            # single-page (legacy support)\\n            \"page_no\": {\"type\": \"long\"},\\n\\n            # NEW: page range string (e.g., \"1-6\")\\n            \"page_range\": {\"type\": \"keyword\"},\\n\\n            # NEW: array of page numbers (e.g., [1,2,3,4,5,6])\\n            \"pages\": {\"type\": \"long\"},\\n\\n            # vector index used for ANN similarity search\\n            \"vector_field\": {\\n                \"type\": \"knn_vector\",\\n                \"dimension\": 1024,\\n                \"method\": {\\n                    \"name\": \"hnsw\",\\n                    \"space_type\": \"cosinesimil\",\\n                    \"engine\": \"nmslib\",\\n                    \"parameters\": { \"ef_construction\": 512, \"m\": 16 }\\n                }\\n            }\\n        }\\n    }\\n}\\n\\n# recreate index\\nif client.indices.exists(index=index_name):\\n    print(f\"Index \\'{index_name}\\' already exists. Deleting and recreating...\")\\n    client.indices.delete(index=index_name)\\n\\nclient.indices.create(index=index_name, body=index_body)\\nprint(f\"✅ Index \\'{index_name}\\' created successfully!\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'test-annauniv': {'aliases': {}, 'mappings': {'properties': {'course_id': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'embedding': {'type': 'float'}, 'metadata': {'properties': {'page': {'type': 'long'}, 'source': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'page_no': {'type': 'long'}, 'text': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'vector_field': {'type': 'knn_vector', 'dimension': 1024, 'method': {'engine': 'nmslib', 'space_type': 'cosinesimil', 'name': 'hnsw', 'parameters': {'ef_construction': 512, 'm': 16}}}}}, 'settings': {'index': {'replication': {'type': 'DOCUMENT'}, 'refresh_interval': '1s', 'number_of_shards': '5', 'knn.algo_param': {'ef_search': '512'}, 'provided_name': 'test-annauniv', 'knn': 'true', 'creation_date': '1756144596121', 'number_of_replicas': '1', 'uuid': 'rumasDZaQC6xBy4G48v1KQ', 'version': {'created': '136407827'}}}}}\n",
        "\n"
      ],
      "metadata": {
        "id": "n-xX1sT3yxqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, time, re\n",
        "\n",
        "BUCKET = \"anna-univ-qna\"\n",
        "DOCUMENT = \"Textbooks/SPM.pdf\"\n",
        "REGION = \"ap-south-1\"\n",
        "COURSE_ID = \"CS6022\"   # <-- set this per textbook\n",
        "FILENAME = \"SPM.pdf\"\n",
        "\n",
        "textract = boto3.client(\"textract\", region_name=REGION)\n",
        "\n",
        "# --- Start async Textract job ---\n",
        "start = time.time()\n",
        "response = textract.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": BUCKET, \"Name\": DOCUMENT}}\n",
        ")\n",
        "print(\"FILENAME =\",FILENAME)\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"✅ Started Textract JobId: {job_id}\")\n",
        "\n",
        "# --- Wait for completion ---\n",
        "while True:\n",
        "    result = textract.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"❌ Textract job failed!\")\n",
        "\n",
        "pages = result[\"DocumentMetadata\"][\"Pages\"]\n",
        "print(f\"📄 Pages: {pages}\")\n",
        "print(f\"⏱️ Time taken: {elapsed:.2f} sec\")\n",
        "print(f\"💰 Estimated cost: ${(pages/1000)*1.5:.4f}\")\n",
        "\n",
        "# --- Collect page-wise text into docs with metadata ---\n",
        "page_texts = {}\n",
        "next_token = None\n",
        "\n",
        "while True:\n",
        "    if next_token:\n",
        "        result = textract.get_document_text_detection(JobId=job_id, NextToken=next_token)\n",
        "    else:\n",
        "        result = textract.get_document_text_detection(JobId=job_id)\n",
        "\n",
        "    for block in result[\"Blocks\"]:\n",
        "        if block[\"BlockType\"] == \"LINE\":\n",
        "            page_no = block[\"Page\"]\n",
        "            page_texts.setdefault(page_no, []).append(block[\"Text\"])\n",
        "\n",
        "    next_token = result.get(\"NextToken\")\n",
        "    if not next_token:\n",
        "        break\n",
        "\n",
        "# Build document objects (ready to send to Titan for embeddings)\n",
        "docs = []\n",
        "for page_no, lines in page_texts.items():\n",
        "    chunk_text = \"\\n\".join(lines)\n",
        "    doc = {\n",
        "        \"chunk_text\": chunk_text,\n",
        "        \"course_id\": COURSE_ID,\n",
        "        \"filename\": FILENAME,\n",
        "        \"page_no\": page_no\n",
        "    }\n",
        "    docs.append(doc)\n",
        "\n",
        "# --- Preview first 5 pages ---\n",
        "print(\"\\n📑 First 5 pages extracted:\")\n",
        "for d in docs[:5]:\n",
        "    print(f\"\\nPage {d['page_no']} | Course: {d['course_id']} | File: {d['filename']}\")\n",
        "    print(d[\"chunk_text\"][:400], \"...\")\n"
      ],
      "metadata": {
        "id": "DVaNNUjgqWN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b8d38c90-af17-4b3a-ace1-6ffb1d823bab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FILENAME = SPM.pdf\n",
            "✅ Started Textract JobId: b5028af2eb0ddfcab545858e3da1eafb4bbcb99eb51119ecdf87fbf6a7edae6e\n",
            "📄 Pages: 396\n",
            "⏱️ Time taken: 199.15 sec\n",
            "💰 Estimated cost: $0.5940\n",
            "\n",
            "📑 First 5 pages extracted:\n",
            "\n",
            "Page 1 | Course: CS6022 | File: SPM.pdf\n",
            "BoB HUGHES AND MIKE COTTERELL\n",
            "Software\n",
            "Project\n",
            "Management Second Edition\n",
            "Tistimating\n",
            "www.mcgraw-hill.co.uk/hughes ...\n",
            "\n",
            "Page 2 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition) ...\n",
            "\n",
            "Page 3 | Course: CS6022 | File: SPM.pdf\n",
            "Software Project Management\n",
            "(Second Edition)\n",
            "Bob Hughes and Mike Cotterell,\n",
            "School of Information Management, University of Brighton\n",
            "The McGraw-Hill Companies\n",
            "London\n",
            "Burr Ridge, IL\n",
            "New York\n",
            "St Louis\n",
            "San Francisco\n",
            "Auckland\n",
            "Bogotá Caracas\n",
            "Lisbon\n",
            "Madrid\n",
            "Mexico\n",
            "Milan\n",
            "Montreal\n",
            "New Delhi\n",
            "Panama\n",
            "Paris\n",
            "San Juan\n",
            "São Paulo\n",
            "Singapore\n",
            "Tokyo\n",
            "Toronto ...\n",
            "\n",
            "Page 4 | Course: CS6022 | File: SPM.pdf\n",
            "Published by\n",
            "McGraw-Hill Publishing Company\n",
            "SHOPPENHANGERS ROAD, MAIDENHEAD, BERKSHIRE, SL6 2QL, ENGLAND\n",
            "Telephone: +44(o) 1628 502500\n",
            "Fax: +44(o) 1628 770224\n",
            "Web site: http://www.megraw-hill.co.uk\n",
            "British Library Cataloguing in Publication Data\n",
            "A catalogue record for this book is available from the British Library\n",
            "ISBN 007 709505 7\n",
            "Library of Congress cataloguing in publication data\n",
            "The LOC data  ...\n",
            "\n",
            "Page 5 | Course: CS6022 | File: SPM.pdf\n",
            "The road to hell is paved with works-in-progress.\n",
            "Philip Roth ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_page_chunks(docs, chunk_size=6, overlap=2):\n",
        "    \"\"\"\n",
        "    Re-chunk page-level docs into multi-page chunks.\n",
        "    Each chunk contains `chunk_size` pages, with `overlap` pages between chunks.\n",
        "    \"\"\"\n",
        "    # Ensure pages are sorted\n",
        "    docs_sorted = sorted(docs, key=lambda x: x[\"page_no\"])\n",
        "\n",
        "    new_chunks = []\n",
        "    step = chunk_size - overlap\n",
        "    total_pages = len(docs_sorted)\n",
        "\n",
        "    for i in range(0, total_pages, step):\n",
        "        # Select pages for this chunk\n",
        "        chunk_docs = docs_sorted[i:i+chunk_size]\n",
        "        if not chunk_docs:\n",
        "            continue\n",
        "\n",
        "        # Combine texts\n",
        "        chunk_text = \"\\n\\n\".join(d[\"chunk_text\"] for d in chunk_docs)\n",
        "\n",
        "        # Metadata: first + last page\n",
        "        page_start = chunk_docs[0][\"page_no\"]\n",
        "        page_end = chunk_docs[-1][\"page_no\"]\n",
        "\n",
        "        new_chunks.append({\n",
        "            \"chunk_text\": chunk_text,\n",
        "            \"course_id\": chunk_docs[0][\"course_id\"],   # same for all\n",
        "            \"filename\": chunk_docs[0][\"filename\"],     # same file\n",
        "            \"page_range\": f\"{page_start}-{page_end}\",\n",
        "            \"pages\": [d[\"page_no\"] for d in chunk_docs]\n",
        "        })\n",
        "\n",
        "    return new_chunks\n",
        "\n",
        "# Example usage\n",
        "chunked_docs = make_page_chunks(docs, chunk_size=6, overlap=2)\n",
        "print(f\"✅ Created {len(chunked_docs)} multi-page chunks\")\n",
        "for c in chunked_docs[:]:\n",
        "    print(f\"Chunk pages: {c['page_range']}, length={len(c['chunk_text'])} chars\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjcmg5jTkxyj",
        "outputId": "56c4434e-e681-45c9-d1a9-454a0942c0b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created 97 multi-page chunks\n",
            "Chunk pages: 1-6, length=3159 chars\n",
            "Chunk pages: 5-10, length=5704 chars\n",
            "Chunk pages: 9-14, length=10492 chars\n",
            "Chunk pages: 13-18, length=13310 chars\n",
            "Chunk pages: 17-22, length=14129 chars\n",
            "Chunk pages: 21-26, length=14491 chars\n",
            "Chunk pages: 25-30, length=15321 chars\n",
            "Chunk pages: 29-34, length=12528 chars\n",
            "Chunk pages: 33-38, length=13568 chars\n",
            "Chunk pages: 37-42, length=13716 chars\n",
            "Chunk pages: 41-46, length=12544 chars\n",
            "Chunk pages: 45-50, length=12730 chars\n",
            "Chunk pages: 49-54, length=14228 chars\n",
            "Chunk pages: 53-58, length=14591 chars\n",
            "Chunk pages: 57-62, length=15423 chars\n",
            "Chunk pages: 61-66, length=14447 chars\n",
            "Chunk pages: 65-71, length=10503 chars\n",
            "Chunk pages: 70-75, length=14014 chars\n",
            "Chunk pages: 74-79, length=14289 chars\n",
            "Chunk pages: 78-83, length=13774 chars\n",
            "Chunk pages: 82-87, length=13389 chars\n",
            "Chunk pages: 86-91, length=11503 chars\n",
            "Chunk pages: 90-95, length=12682 chars\n",
            "Chunk pages: 94-99, length=16610 chars\n",
            "Chunk pages: 98-103, length=15775 chars\n",
            "Chunk pages: 102-107, length=12250 chars\n",
            "Chunk pages: 106-111, length=13370 chars\n",
            "Chunk pages: 110-115, length=12789 chars\n",
            "Chunk pages: 114-120, length=9942 chars\n",
            "Chunk pages: 119-124, length=14297 chars\n",
            "Chunk pages: 123-128, length=14853 chars\n",
            "Chunk pages: 127-132, length=11457 chars\n",
            "Chunk pages: 131-136, length=10390 chars\n",
            "Chunk pages: 135-140, length=11392 chars\n",
            "Chunk pages: 139-144, length=11952 chars\n",
            "Chunk pages: 143-148, length=10381 chars\n",
            "Chunk pages: 147-152, length=15665 chars\n",
            "Chunk pages: 151-156, length=14839 chars\n",
            "Chunk pages: 155-160, length=11040 chars\n",
            "Chunk pages: 159-164, length=9761 chars\n",
            "Chunk pages: 163-168, length=11511 chars\n",
            "Chunk pages: 167-172, length=11717 chars\n",
            "Chunk pages: 171-176, length=12574 chars\n",
            "Chunk pages: 175-180, length=8531 chars\n",
            "Chunk pages: 179-184, length=9134 chars\n",
            "Chunk pages: 183-188, length=12970 chars\n",
            "Chunk pages: 187-192, length=12034 chars\n",
            "Chunk pages: 191-196, length=9644 chars\n",
            "Chunk pages: 195-200, length=13323 chars\n",
            "Chunk pages: 199-204, length=15025 chars\n",
            "Chunk pages: 203-208, length=15139 chars\n",
            "Chunk pages: 207-212, length=16705 chars\n",
            "Chunk pages: 211-216, length=16676 chars\n",
            "Chunk pages: 215-220, length=16013 chars\n",
            "Chunk pages: 219-225, length=12604 chars\n",
            "Chunk pages: 224-229, length=16003 chars\n",
            "Chunk pages: 228-233, length=15902 chars\n",
            "Chunk pages: 232-237, length=16062 chars\n",
            "Chunk pages: 236-241, length=15295 chars\n",
            "Chunk pages: 240-245, length=15118 chars\n",
            "Chunk pages: 244-250, length=11429 chars\n",
            "Chunk pages: 249-254, length=14292 chars\n",
            "Chunk pages: 253-258, length=13835 chars\n",
            "Chunk pages: 257-262, length=12882 chars\n",
            "Chunk pages: 261-266, length=15623 chars\n",
            "Chunk pages: 265-270, length=16077 chars\n",
            "Chunk pages: 269-275, length=13040 chars\n",
            "Chunk pages: 274-279, length=14645 chars\n",
            "Chunk pages: 278-284, length=13634 chars\n",
            "Chunk pages: 283-288, length=14710 chars\n",
            "Chunk pages: 287-292, length=14894 chars\n",
            "Chunk pages: 291-296, length=12845 chars\n",
            "Chunk pages: 295-300, length=10340 chars\n",
            "Chunk pages: 299-304, length=11275 chars\n",
            "Chunk pages: 303-308, length=10101 chars\n",
            "Chunk pages: 307-312, length=12653 chars\n",
            "Chunk pages: 311-317, length=13026 chars\n",
            "Chunk pages: 316-321, length=11655 chars\n",
            "Chunk pages: 320-325, length=9942 chars\n",
            "Chunk pages: 324-330, length=11445 chars\n",
            "Chunk pages: 329-334, length=12400 chars\n",
            "Chunk pages: 333-339, length=11444 chars\n",
            "Chunk pages: 337-343, length=9823 chars\n",
            "Chunk pages: 342-347, length=8047 chars\n",
            "Chunk pages: 346-351, length=10048 chars\n",
            "Chunk pages: 350-355, length=8323 chars\n",
            "Chunk pages: 354-359, length=7353 chars\n",
            "Chunk pages: 358-363, length=9737 chars\n",
            "Chunk pages: 362-367, length=9549 chars\n",
            "Chunk pages: 366-371, length=8948 chars\n",
            "Chunk pages: 370-375, length=11163 chars\n",
            "Chunk pages: 374-380, length=11792 chars\n",
            "Chunk pages: 379-384, length=10993 chars\n",
            "Chunk pages: 383-388, length=13900 chars\n",
            "Chunk pages: 387-392, length=14084 chars\n",
            "Chunk pages: 391-396, length=12416 chars\n",
            "Chunk pages: 395-396, length=3030 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = \"ap-south-1\"\n",
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "PRICE_PER_1K = 0.000024 # USD per 1k tokens\n",
        "\n",
        "all_embeddings = []\n",
        "total_tokens = 0\n",
        "start = time.time()\n",
        "\n",
        "for d in chunked_docs:\n",
        "    body = json.dumps({\"inputText\": d[\"chunk_text\"]})\n",
        "    resp = bedrock_runtime.invoke_model(modelId=MODEL_ID, body=body)\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "\n",
        "    d[\"vector_field\"] = result[\"embedding\"]\n",
        "    d[\"tokens\"] = result.get(\"inputTextTokenCount\", len(d[\"chunk_text\"].split()))\n",
        "\n",
        "    all_embeddings.append(d)\n",
        "    total_tokens += d[\"tokens\"]\n",
        "    print(f\"📄 Pages {d['page_range']}: {d['tokens']} tokens\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "cost = (total_tokens / 1000) * PRICE_PER_1K\n",
        "\n",
        "print(\"\\n⏱️ Total time:\", round(elapsed, 2), \"seconds\")\n",
        "print(\"📊 Total tokens:\", total_tokens)\n",
        "print(f\"💰 Estimated embedding cost: ${cost:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTROPiAfkyCg",
        "outputId": "7b40359b-82eb-4009-9ebf-332e2b27fc45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Pages 1-6: 893 tokens\n",
            "📄 Pages 5-10: 1870 tokens\n",
            "📄 Pages 9-14: 2338 tokens\n",
            "📄 Pages 13-18: 2655 tokens\n",
            "📄 Pages 17-22: 2830 tokens\n",
            "📄 Pages 21-26: 2938 tokens\n",
            "📄 Pages 25-30: 3119 tokens\n",
            "📄 Pages 29-34: 2655 tokens\n",
            "📄 Pages 33-38: 2870 tokens\n",
            "📄 Pages 37-42: 2915 tokens\n",
            "📄 Pages 41-46: 2745 tokens\n",
            "📄 Pages 45-50: 2688 tokens\n",
            "📄 Pages 49-54: 2934 tokens\n",
            "📄 Pages 53-58: 3680 tokens\n",
            "📄 Pages 57-62: 4093 tokens\n",
            "📄 Pages 61-66: 3394 tokens\n",
            "📄 Pages 65-71: 2345 tokens\n",
            "📄 Pages 70-75: 2889 tokens\n",
            "📄 Pages 74-79: 2947 tokens\n",
            "📄 Pages 78-83: 2821 tokens\n",
            "📄 Pages 82-87: 2851 tokens\n",
            "📄 Pages 86-91: 2403 tokens\n",
            "📄 Pages 90-95: 2910 tokens\n",
            "📄 Pages 94-99: 3596 tokens\n",
            "📄 Pages 98-103: 3532 tokens\n",
            "📄 Pages 102-107: 2936 tokens\n",
            "📄 Pages 106-111: 3069 tokens\n",
            "📄 Pages 110-115: 2942 tokens\n",
            "📄 Pages 114-120: 2150 tokens\n",
            "📄 Pages 119-124: 2933 tokens\n",
            "📄 Pages 123-128: 3292 tokens\n",
            "📄 Pages 127-132: 2771 tokens\n",
            "📄 Pages 131-136: 2551 tokens\n",
            "📄 Pages 135-140: 3036 tokens\n",
            "📄 Pages 139-144: 2917 tokens\n",
            "📄 Pages 143-148: 2244 tokens\n",
            "📄 Pages 147-152: 3282 tokens\n",
            "📄 Pages 151-156: 3219 tokens\n",
            "📄 Pages 155-160: 2949 tokens\n",
            "📄 Pages 159-164: 2429 tokens\n",
            "📄 Pages 163-168: 2870 tokens\n",
            "📄 Pages 167-172: 3049 tokens\n",
            "📄 Pages 171-176: 3189 tokens\n",
            "📄 Pages 175-180: 2201 tokens\n",
            "📄 Pages 179-184: 1909 tokens\n",
            "📄 Pages 183-188: 2903 tokens\n",
            "📄 Pages 187-192: 2931 tokens\n",
            "📄 Pages 191-196: 2413 tokens\n",
            "📄 Pages 195-200: 2953 tokens\n",
            "📄 Pages 199-204: 3078 tokens\n",
            "📄 Pages 203-208: 3269 tokens\n",
            "📄 Pages 207-212: 3608 tokens\n",
            "📄 Pages 211-216: 3404 tokens\n",
            "📄 Pages 215-220: 3203 tokens\n",
            "📄 Pages 219-225: 2619 tokens\n",
            "📄 Pages 224-229: 3393 tokens\n",
            "📄 Pages 228-233: 3304 tokens\n",
            "📄 Pages 232-237: 3380 tokens\n",
            "📄 Pages 236-241: 3130 tokens\n",
            "📄 Pages 240-245: 3019 tokens\n",
            "📄 Pages 244-250: 2359 tokens\n",
            "📄 Pages 249-254: 2974 tokens\n",
            "📄 Pages 253-258: 2966 tokens\n",
            "📄 Pages 257-262: 2692 tokens\n",
            "📄 Pages 261-266: 3200 tokens\n",
            "📄 Pages 265-270: 3301 tokens\n",
            "📄 Pages 269-275: 2751 tokens\n",
            "📄 Pages 274-279: 3101 tokens\n",
            "📄 Pages 278-284: 2885 tokens\n",
            "📄 Pages 283-288: 3114 tokens\n",
            "📄 Pages 287-292: 3223 tokens\n",
            "📄 Pages 291-296: 2809 tokens\n",
            "📄 Pages 295-300: 2199 tokens\n",
            "📄 Pages 299-304: 2388 tokens\n",
            "📄 Pages 303-308: 2147 tokens\n",
            "📄 Pages 307-312: 2612 tokens\n",
            "📄 Pages 311-317: 2688 tokens\n",
            "📄 Pages 316-321: 2425 tokens\n",
            "📄 Pages 320-325: 2072 tokens\n",
            "📄 Pages 324-330: 2418 tokens\n",
            "📄 Pages 329-334: 2684 tokens\n",
            "📄 Pages 333-339: 2383 tokens\n",
            "📄 Pages 337-343: 1983 tokens\n",
            "📄 Pages 342-347: 1688 tokens\n",
            "📄 Pages 346-351: 2424 tokens\n",
            "📄 Pages 350-355: 2162 tokens\n",
            "📄 Pages 354-359: 2411 tokens\n",
            "📄 Pages 358-363: 2697 tokens\n",
            "📄 Pages 362-367: 2850 tokens\n",
            "📄 Pages 366-371: 2133 tokens\n",
            "📄 Pages 370-375: 2322 tokens\n",
            "📄 Pages 374-380: 2626 tokens\n",
            "📄 Pages 379-384: 2928 tokens\n",
            "📄 Pages 383-388: 4096 tokens\n",
            "📄 Pages 387-392: 4114 tokens\n",
            "📄 Pages 391-396: 3794 tokens\n",
            "📄 Pages 395-396: 960 tokens\n",
            "\n",
            "⏱️ Total time: 36.98 seconds\n",
            "📊 Total tokens: 272035\n",
            "💰 Estimated embedding cost: $0.006529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib, re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Normalize text to avoid minor whitespace/case changes causing new IDs.\"\"\"\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower()\n",
        "\n",
        "def make_chunk_id(course_id: str, chunk_text: str) -> str:\n",
        "    \"\"\"Deterministic ID based on course_id + chunk_text.\"\"\"\n",
        "    norm = normalize_text(chunk_text)\n",
        "    raw = f\"{course_id}|{norm}\"\n",
        "    return hashlib.blake2b(raw.encode(\"utf-8\"), digest_size=16).hexdigest()\n"
      ],
      "metadata": {
        "id": "rUwMCTl4sSMI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opensearchpy.helpers import parallel_bulk\n",
        "\n",
        "index_name = \"test-auqa\"\n",
        "\n",
        "def doc_to_action(doc):\n",
        "    # ✅ Keep original ID logic: course_id + chunk_text\n",
        "    doc_id = make_chunk_id(doc[\"course_id\"], doc[\"chunk_text\"])\n",
        "\n",
        "    return {\n",
        "        \"_op_type\": \"index\",    # overwrite if exists\n",
        "        \"_index\": index_name,\n",
        "        \"_id\": doc_id,\n",
        "        \"_source\": {\n",
        "            \"chunk_text\": doc[\"chunk_text\"],\n",
        "            \"course_id\": doc[\"course_id\"],\n",
        "            \"filename\": doc[\"filename\"],\n",
        "            # Add both page_range (string) and pages (list)\n",
        "            \"page_range\": doc.get(\"page_range\"),\n",
        "            \"pages\": doc.get(\"pages\", [doc.get(\"page_no\")]),\n",
        "            \"vector_field\": doc[\"vector_field\"]   # Titan embedding (1024 floats)\n",
        "        }\n",
        "    }\n",
        "\n",
        "# build actions generator\n",
        "actions = (doc_to_action(d) for d in all_embeddings)\n",
        "\n",
        "# bulk insert\n",
        "for ok, result in parallel_bulk(client, actions, thread_count=1, chunk_size=50):\n",
        "    if not ok:\n",
        "        print(\"❌ Failed:\", result)\n",
        "\n",
        "print(\"✅ Bulk upsert finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwKHeDGFsVgo",
        "outputId": "058d1539-c475-4192-b4db-0dfedf006e10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bulk upsert finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "# Titan embedding model ID\n",
        "MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "REGION = \"us-east-1\"  # Change if needed\n",
        "\n",
        "# Create Bedrock runtime client\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "def get_titan_embedding(text: str):\n",
        "    \"\"\"\n",
        "    Generates an embedding vector for the given text using Amazon Titan model.\n",
        "    \"\"\"\n",
        "    # Convert the input into JSON format required by Bedrock\n",
        "    body = json.dumps({\"inputText\": text})\n",
        "\n",
        "    # Invoke the Bedrock model\n",
        "    resp = bedrock_rt.invoke_model(modelId=MODEL_ID, body=body)\n",
        "\n",
        "    # Parse the response\n",
        "    result = json.loads(resp[\"body\"].read())\n",
        "\n",
        "    # Return the embedding and token count\n",
        "    return result[\"embedding\"], result.get(\"inputTextTokenCount\", None)\n",
        "\n",
        "# ✅ Take query text as user input\n",
        "query_text = input(\"Enter your search query: \")\n",
        "\n",
        "# Get Titan embeddings for the user-provided query\n",
        "query_vector, token_count = get_titan_embedding(query_text)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\n✅ Query embedded successfully!\")\n",
        "print(f\"Query: {query_text}\")\n",
        "print(f\"Vector size: {len(query_vector)} dimensions\")\n",
        "print(f\"Token count: {token_count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWNe5hit_6U",
        "outputId": "489e2d84-5be5-41be-af60-514fd5b7e63c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your search query: Project Evaluation\n",
            "\n",
            "✅ Query embedded successfully!\n",
            "Query: Project Evaluation\n",
            "Vector size: 1024 dimensions\n",
            "Token count: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 5\n",
        "BM25_WEIGHT = 0.3\n",
        "VECTOR_WEIGHT = 0.7\n",
        "\n",
        "# --- Run BM25 ---\n",
        "bm25_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\"size\": TOP_N, \"query\": {\"match\": {\"chunk_text\": query_text}}}\n",
        ")\n",
        "bm25_hits = bm25_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Run Vector ---\n",
        "vector_resp = client.search(\n",
        "    index=index_name,\n",
        "    body={\n",
        "        \"size\": TOP_N,\n",
        "        \"query\": {\n",
        "            \"knn\": {\"vector_field\": {\"vector\": query_vector, \"k\": TOP_N}}\n",
        "        }\n",
        "    }\n",
        ")\n",
        "vector_hits = vector_resp[\"hits\"][\"hits\"]\n",
        "\n",
        "# --- Normalize scores ---\n",
        "def normalize_scores(hits):\n",
        "    scores = [h[\"_score\"] for h in hits]\n",
        "    if not scores:\n",
        "        return {}\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if min_s == max_s:\n",
        "        return {h[\"_id\"]: 1.0 for h in hits}\n",
        "    return {h[\"_id\"]: (h[\"_score\"] - min_s) / (max_s - min_s) for h in hits}\n",
        "\n",
        "bm25_norm = normalize_scores(bm25_hits)\n",
        "vector_norm = normalize_scores(vector_hits)\n",
        "\n",
        "# --- Combine ---\n",
        "combined = {}\n",
        "for h in bm25_hits + vector_hits:\n",
        "    _id = h[\"_id\"]\n",
        "    src = h[\"_source\"]\n",
        "    bm25_s = bm25_norm.get(_id, 0.0)\n",
        "    vec_s = vector_norm.get(_id, 0.0)\n",
        "    hybrid_score = BM25_WEIGHT * bm25_s + VECTOR_WEIGHT * vec_s\n",
        "    combined[_id] = {\n",
        "        \"hybrid_score\": hybrid_score,\n",
        "        \"bm25_score\": bm25_s,\n",
        "        \"vector_score\": vec_s,\n",
        "        \"source\": src\n",
        "    }\n",
        "\n",
        "results = sorted(combined.values(), key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
        "\n",
        "# --- Print top results ---\n",
        "print(\"\\n🔎 Manual Hybrid Results:\")\n",
        "for r in results[:TOP_N]:   # ✅ only top N\n",
        "    src = r[\"source\"]\n",
        "    # prefer page_range if exists\n",
        "    page_info = src.get(\"page_range\", src.get(\"page_no\", \"N/A\"))\n",
        "    print(f\"Hybrid={r['hybrid_score']:.3f} | BM25={r['bm25_score']:.3f} | Vec={r['vector_score']:.3f}\")\n",
        "    print(f\"Pages={page_info} | Course={src.get('course_id','N/A')} | File={src.get('filename','N/A')}\")\n",
        "    print(src[\"chunk_text\"][:5], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hecRPJZwHnp",
        "outputId": "1503af17-3cbf-4f7f-8489-4d9a7da3e56b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Manual Hybrid Results:\n",
            "Hybrid=1.000 | BM25=1.000 | Vec=1.000\n",
            "Pages=49-54 | Course=CS6022 | File=SPM.pdf\n",
            "Chapt ...\n",
            "\n",
            "Hybrid=0.259 | BM25=0.170 | Vec=0.297\n",
            "Pages=61-66 | Course=CS6022 | File=SPM.pdf\n",
            "3.6 C ...\n",
            "\n",
            "Hybrid=0.197 | BM25=0.000 | Vec=0.281\n",
            "Pages=45-50 | Course=CS6022 | File=SPM.pdf\n",
            "2.9 S ...\n",
            "\n",
            "Hybrid=0.131 | BM25=0.000 | Vec=0.188\n",
            "Pages=65-71 | Course=CS6022 | File=SPM.pdf\n",
            "3.7 R ...\n",
            "\n",
            "Hybrid=0.055 | BM25=0.184 | Vec=0.000\n",
            "Pages=143-148 | Course=CS6022 | File=SPM.pdf\n",
            "6.19  ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect top 5 page chunks into one string\n",
        "context_text = \"\\n\\n\".join([r[\"source\"][\"chunk_text\"] for r in results[:]])\n",
        "ques_query = query_text   # the query you used\n",
        "no = 20\n"
      ],
      "metadata": {
        "id": "PiJhquiAyjO2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_inputs():\n",
        "    # Get number of questions\n",
        "    while True:\n",
        "        try:\n",
        "            no = int(input(\"Enter the number of questions you want to generate: \"))\n",
        "            if no > 0:\n",
        "                break\n",
        "            else:\n",
        "                print(\"❌ Please enter a positive number.\")\n",
        "        except ValueError:\n",
        "            print(\"❌ Invalid input! Please enter a number.\")\n",
        "\n",
        "    # Get difficulty ratios (easy, medium, hard) -> sum must be 1\n",
        "    while True:\n",
        "        try:\n",
        "            easy = float(input(\"Enter difficulty ratio for EASY questions (0 to 1): \"))\n",
        "            medium = float(input(\"Enter difficulty ratio for MEDIUM questions (0 to 1): \"))\n",
        "            hard = float(input(\"Enter difficulty ratio for HARD questions (0 to 1): \"))\n",
        "\n",
        "            total = easy + medium + hard\n",
        "            if abs(total - 1.0) < 1e-6:  # Allowing floating-point precision errors\n",
        "                break\n",
        "            else:\n",
        "                print(f\"❌ Invalid ratios! The sum must be 1, but you entered {total}. Please try again.\")\n",
        "        except ValueError:\n",
        "            print(\"❌ Invalid input! Please enter decimal values.\")\n",
        "\n",
        "    return no, easy, medium, hard\n",
        "\n",
        "# Example usage\n",
        "no, easy_ratio, medium_ratio, hard_ratio = get_user_inputs()\n",
        "print(f\"\\n✅ Number of Questions: {no}\")\n",
        "print(f\"✅ Difficulty Ratios → Easy: {easy_ratio}, Medium: {medium_ratio}, Hard: {hard_ratio}\")\n"
      ],
      "metadata": {
        "id": "QwxATKx02VY6",
        "outputId": "c50dd23b-a9e3-4403-8230-ef2f81c6c77e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of questions you want to generate: 8\n",
            "Enter difficulty ratio for EASY questions (0 to 1): 0.3\n",
            "Enter difficulty ratio for MEDIUM questions (0 to 1): 0.4\n",
            "Enter difficulty ratio for HARD questions (0 to 1): 0.3\n",
            "\n",
            "✅ Number of Questions: 8\n",
            "✅ Difficulty Ratios → Easy: 0.3, Medium: 0.4, Hard: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an **AI assistant** specialized in **automatic question generation**.\n",
        "Your task is to create insightful and meaningful questions based on the provided **context** and **user query**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Goals**\n",
        "1. Generate **{no} questions** in total, distributed based on the given difficulty ratio:\n",
        "   - **Easy**: {easy_ratio * 100:.0f}%\n",
        "   - **Medium**: {medium_ratio * 100:.0f}%\n",
        "   - **Hard**: {hard_ratio * 100:.0f}%\n",
        "2. Ensure the ratio is **strictly followed**. If the sum of ratios is not exactly 1.0, adjust proportionally.\n",
        "3. Classify each question by:\n",
        "   - **Difficulty Level** → Easy / Medium / Hard\n",
        "   - **Bloom's Taxonomy Level** → Choose one of:\n",
        "        * Remember\n",
        "        * Understand\n",
        "        * Apply\n",
        "        * Analyze\n",
        "        * Evaluate\n",
        "        * Create\n",
        "\n",
        "---\n",
        "\n",
        "### **Input**\n",
        "**Context**:\n",
        "{context_text}\n",
        "\n",
        "**User Query (Focus Topic)**:\n",
        "{ques_query}\n",
        "\n",
        "**User Requirements**:\n",
        "- Total Questions: {no}\n",
        "- Difficulty Ratio: Easy={easy_ratio}, Medium={medium_ratio}, Hard={hard_ratio}\n",
        "\n",
        "---\n",
        "\n",
        "### **Few-shot Examples**\n",
        "\n",
        "#### Example 1:\n",
        "**Input Context:** \"Basics of Data Structures\"\n",
        "**User Query:** \"Arrays and Linked Lists\"\n",
        "**Output:**\n",
        "[\n",
        "  {{\n",
        "    \"question\": \"What is the primary difference between arrays and linked lists in terms of memory allocation?\",\n",
        "    \"difficulty\": \"Medium\",\n",
        "    \"blooms_level\": \"Understand\"\n",
        "  }},\n",
        "  {{\n",
        "    \"question\": \"Explain how linked lists handle dynamic memory better than arrays.\",\n",
        "    \"difficulty\": \"Hard\",\n",
        "    \"blooms_level\": \"Analyze\"\n",
        "  }},\n",
        "  {{\n",
        "    \"question\": \"Define an array and give one real-life example of its usage.\",\n",
        "    \"difficulty\": \"Easy\",\n",
        "    \"blooms_level\": \"Remember\"\n",
        "  }}\n",
        "]\n",
        "\n",
        "#### Example 2:\n",
        "**Input Context:** \"Cloud Computing\"\n",
        "**User Query:** \"AWS EC2 and Lambda\"\n",
        "**Output:**\n",
        "[\n",
        "  {{\n",
        "    \"question\": \"What is the main purpose of AWS EC2 instances?\",\n",
        "    \"difficulty\": \"Easy\",\n",
        "    \"blooms_level\": \"Remember\"\n",
        "  }},\n",
        "  {{\n",
        "    \"question\": \"Compare AWS Lambda and EC2 in terms of scalability and cost-efficiency.\",\n",
        "    \"difficulty\": \"Medium\",\n",
        "    \"blooms_level\": \"Analyze\"\n",
        "  }},\n",
        "  {{\n",
        "    \"question\": \"Design a serverless architecture using AWS Lambda for a chat application.\",\n",
        "    \"difficulty\": \"Hard\",\n",
        "    \"blooms_level\": \"Create\"\n",
        "  }}\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output**\n",
        "Return the results **strictly in valid JSON** format:\n",
        "[\n",
        "  {{\n",
        "    \"question\": \"...\",\n",
        "    \"difficulty\": \"...\",\n",
        "    \"blooms_level\": \"...\"\n",
        "  }}\n",
        "]\n",
        "\n",
        "### **Important Rules**\n",
        "- Do **NOT** copy sentences directly from the context.\n",
        "- Make the questions **broad, meaningful, and conceptual**.\n",
        "- Follow the exact JSON format without any extra text.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8M7L4Eiryjio"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "REGION = \"ap-south-1\"\n",
        "PROFILE_ARN = \"arn:aws:bedrock:ap-south-1:850146468080:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        "\n",
        "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
        "\n",
        "body = {\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 4000,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "start = time.time()\n",
        "resp = bedrock_rt.invoke_model(modelId=PROFILE_ARN, body=json.dumps(body))\n",
        "elapsed = time.time() - start\n",
        "\n",
        "output = json.loads(resp[\"body\"].read())\n",
        "\n",
        "reply = output[\"content\"][0][\"text\"]\n",
        "\n",
        "# Usage metadata (if present)\n",
        "usage = output.get(\"usage\", {})\n",
        "input_tokens = usage.get(\"input_tokens\", \"N/A\")\n",
        "output_tokens = usage.get(\"output_tokens\", \"N/A\")\n",
        "\n",
        "print(\"✅ Generated Output:\\n\")\n",
        "print(reply)\n",
        "\n",
        "print(\"\\n📊 Stats:\")\n",
        "print(\"  Input tokens :\", input_tokens)\n",
        "print(\"  Output tokens:\", output_tokens)\n",
        "print(f\"  Time taken   : {elapsed:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PimWoLJNyjlt",
        "outputId": "21c82e9f-abd8-49d4-e5e4-0c63c347b5a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generated Output:\n",
            "\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What are the main elements considered in a strategic assessment of a software project according to the chapter?\",\n",
            "    \"difficulty\": \"Easy\",\n",
            "    \"blooms_level\": \"Remember\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"How does the concept of a 'programme' influence individual project evaluation and management?\",\n",
            "    \"difficulty\": \"Easy\",\n",
            "    \"blooms_level\": \"Understand\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Compare and contrast the effectiveness of NPV and IRR as project evaluation techniques.\",\n",
            "    \"difficulty\": \"Medium\",\n",
            "    \"blooms_level\": \"Analyze\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"In what ways can risk premiums be incorporated into project evaluation methods to account for uncertainty?\",\n",
            "    \"difficulty\": \"Medium\",\n",
            "    \"blooms_level\": \"Apply\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"How would you evaluate whether a project's cash flow forecasts are reliable enough for decision-making?\",\n",
            "    \"difficulty\": \"Medium\",\n",
            "    \"blooms_level\": \"Evaluate\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What factors should be considered when choosing between competing projects that all show positive NPVs?\",\n",
            "    \"difficulty\": \"Hard\",\n",
            "    \"blooms_level\": \"Evaluate\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Design a comprehensive framework for evaluating software projects that combines strategic, technical, and economic criteria.\",\n",
            "    \"difficulty\": \"Hard\",\n",
            "    \"blooms_level\": \"Create\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"How might changes in market conditions affect the validity of initial project evaluation decisions, and how can this be managed?\",\n",
            "    \"difficulty\": \"Hard\",\n",
            "    \"blooms_level\": \"Analyze\"\n",
            "  }\n",
            "]\n",
            "\n",
            "📊 Stats:\n",
            "  Input tokens : 21221\n",
            "  Output tokens: 414\n",
            "  Time taken   : 10.82 seconds\n"
          ]
        }
      ]
    }
  ]
}